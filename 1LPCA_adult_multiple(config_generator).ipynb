{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Import the three supervised learning models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA    \n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest For Adult\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from random import *\n",
    "from subprocess import check_output\n",
    "def adult_rf(X,Y):\n",
    "    #Split data into training and test datasets (training will be based on 70% of data)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0,shuffle=True) \n",
    "    #test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\n",
    "    print('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "    \n",
    "    #Scaling data\n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=64, random_state=0)\n",
    "    \n",
    "    print(Y_train.dtypes)\n",
    "    Y_train=Y_train.astype('int')\n",
    "    print(Y_train.dtypes)\n",
    "    \n",
    "    print(Y_test.dtypes)\n",
    "    Y_test=Y_test.astype('int')\n",
    "    print(Y_test.dtypes)\n",
    "    \n",
    "    \n",
    "    rf.fit(X_train, Y_train)\n",
    "    print('The accuracy of the RF classifier on training data is {:.2f}'.format(rf.score(X_train, Y_train)))\n",
    "    print('The accuracy of the RF classifier on test data is {:.2f}'.format(rf.score(X_test, Y_test)))\n",
    "    print('####Train prediction Label###############################################')\n",
    "    Y_train_pred=rf.predict(X_train)\n",
    "    #print(y_1)\n",
    "    Y_test_pred=rf.predict(X_test)\n",
    "\n",
    "    print('####Actual Train Label###############################################')\n",
    "\n",
    "\n",
    "    print('####Change to colors###############################################')\n",
    "        \n",
    "    e=rf.predict_proba(X_test)\n",
    "    print(e)\n",
    "    return X_test,Y_test_pred,Y_test,e\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(datax, y_test, y_test_pred,e): \n",
    "        \n",
    "    n=datax.shape[1]\n",
    "    s=datax.shape[0]    \n",
    "    data = np.zeros((s, n), dtype = int)\n",
    "    \n",
    "    r = np.zeros(n, dtype = int) \n",
    "    \n",
    "    for i in range(n):\n",
    "        if int(y_test.iloc[i])==1 :\n",
    "            r[i]=1\n",
    "        else :\n",
    "            r[i]= -1  \n",
    "    \n",
    "    r2 = np.zeros(n, dtype = int) \n",
    "    for i in range(n):\n",
    "        if int(y_test_pred[i])==1 :\n",
    "            r2[i]=1\n",
    "        else :\n",
    "            r2[i]= -1          \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        for i in range(n):\n",
    "                data[j][i]= datax.iloc[j,i]\n",
    "                if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r[i]==1:\n",
    "                         acc1=acc1+1 \n",
    "\n",
    "        print(\"ACTUAL----------total ,accepted, aceeptance rate:\")             \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    print(\"data DP\")\n",
    "    print(DP)\n",
    "    \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        prec=0\n",
    "        reca=0\n",
    "        accur=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "        TP=0\n",
    "        TN=0\n",
    "        for i in range(n):\n",
    "             if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r2[i]==1:\n",
    "                        acc1=acc1+1 \n",
    "                        if r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        else:\n",
    "                             FP=FP+1                \n",
    "                    else:\n",
    "                        if r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        else:\n",
    "                            TN=TN+1    \n",
    "        \n",
    "        print(\"prec reca accuracy for each sens\") \n",
    "        prec= float(TP/(TP+FP))\n",
    "        reca= float(TP/(TP+FN))\n",
    "        accur= float((TP+TN)/a)\n",
    "        print(prec,reca,accur)\n",
    "        \n",
    "        print(\"Random Forest---------total , accepted, aceeptance rate:\")             \n",
    "        \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "  \n",
    " \n",
    "    \n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    \n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "#COMMENT   : \"ar\" #############use this predicted RandomForet accptance rate(config) hardcoded or\n",
    "#can pass it through LPCA as lpca() \n",
    "#above ar: Random forest acceptance rate is use as beta_initial/beta (named as beata_initial in paper)\n",
    "######################################RF acceptance rate as beta_initial #############   \n",
    "    beta_initial=ar\n",
    "\n",
    "########################################################################################      \n",
    "    \n",
    "    print(\"data DP\")\n",
    "    print(DP) \n",
    "    \n",
    "    print(\"Random Forest accuracy--------------------------\")\n",
    "    prec=0\n",
    "    reca=0\n",
    "    accur=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    for i in range(n):\n",
    "            if r2[i]==1:\n",
    "                acc1=acc1+1 \n",
    "                if r[i]==1:\n",
    "                    TP=TP+1\n",
    "                else:\n",
    "                     FP=FP+1                \n",
    "            else:\n",
    "                if r[i]==1:\n",
    "                     FN=FN+1\n",
    "                else:\n",
    "                     TN=TN+1    \n",
    "\n",
    "        \n",
    "    prec= float(TP/(TP+FP))\n",
    "    reca= float(TP/(TP+FN))\n",
    "    accur= float((TP+TN)/n)\n",
    "    print(\"prec--reca--accur\")\n",
    "    print(prec,reca,accur)\n",
    "    ########################COMMENTS\n",
    "######################SET input parameters of users choice beta_ converge,  \n",
    "\n",
    "    \n",
    "#(beta_converge, alpha and epsilon are the parameters of LPCA ) \n",
    "# alpha=1 use initial predicted(beta configs) by classifier (RF), at alpha=0 converges to Least DDP config at beta_converge\n",
    "#Example for beta_converge = [.10,.1248,.1646,.18,.20,.24]\n",
    "    \n",
    "    beta_converge = [.20]\n",
    "    alpha = [1,.08,.06,.04,.02,0]\n",
    "    epsilon=[.005,.01]\n",
    "    \n",
    "    \n",
    "    \n",
    "#beta_initial by RF(in paper beta_initial rf)\n",
    "#  sub groups  's_male', 's_female'  ,'r_white', 'r_black', 'r_asian-pac-islander','r_amer-indian-eskimo, others '\n",
    "#Sizes of each  9211,       4356,      11678,    1220,          404,                   150,               115\n",
    "    \n",
    "    #gamma = [0.2576267506242536, 0.07575757575757576, 0.2111662955985614, 0.08934426229508197, 0.2698019801980198, 0.1, 0.034782608695652174]        \n",
    "    fi= np.zeros(n,dtype=int) \n",
    "    t=0\n",
    "    \n",
    "    #COMMENT\n",
    "    #beta_initial(passed in min_sum_lpca()) \n",
    "    #mentioned in min_sum_lpca():function cell below    \n",
    "    #beta_initial = [0.2576, 0.0757, 0.2111, 0.0893, 0.2698, 0.1, 0.0347]\n",
    "\n",
    "    for eps in epsilon:\n",
    "        for beta_avg in beta_converge:\n",
    "            print(\"----------------This is for covergence at beta = \",beta_avg, \" ----------------\")\n",
    "            for a in alpha:\n",
    "               \n",
    "            ######COMMENT min_sum_lpca()\n",
    "#beta_initial passed in min_sum_lpca() with name beta_initial i.e acc_rate of RF can be paased dynamically\n",
    "            ##parameters\n",
    "                  #data: sensitive sub groups in adult it is 7 dimensional\n",
    "                  #beta_avg (beta^): convergence point from beta_initial(at alpha=1)\n",
    "                  #alpha=0 for minimum DDP achievement\n",
    "                    #eps: to contain config within eps(eps=.005 means achieving DDP<2*eps means <.01 DDP )      \n",
    "\n",
    "                u1,u2 = min_sum_lpca(data,beta_initial,eps,e,beta_avg,a)\n",
    "                #######################Disp_impact#######################  \n",
    "                print(\"alpha, beta_avg\",a,beta_avg)\n",
    "                accu_all=[]\n",
    "                DP_all=[]\n",
    "                precision_all=[]\n",
    "                recall_all=[]\n",
    "                ar_all=[]\n",
    "                acceptance_rate=np.zeros((7,28),dtype=float)\n",
    "                count=0\n",
    "                print(\"<------epsilon-\",eps,\"-------------------------------->\")\n",
    "                t=t+1\n",
    "                print(\"iteration t\",t)\n",
    "       \n",
    "\n",
    "                for i in range(n):\n",
    "                     fi[i] = u1[i]\n",
    "\n",
    "\n",
    "                for j in range(s):\n",
    "                    print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "                    TP=0\n",
    "                    FP=0\n",
    "                    FN=0\n",
    "                    TN=0\n",
    "                    precision=0\n",
    "                    recall=0\n",
    "                    for i in range(n):\n",
    "                         if data[j][i]== 1 :                        \n",
    "                            if fi[i]==1 and r[i]==1:\n",
    "                                TP=TP+1\n",
    "                            if fi[i]==1 and r[i]==-1:\n",
    "                                FP=FP+1 \n",
    "                            if fi[i]==-1 and r[i]==1:\n",
    "                                FN=FN+1\n",
    "                            if fi[i]==-1 and r[i]==-1:\n",
    "                                TN=TN+1    \n",
    "                    if TP+FP !=0:\n",
    "                        precision=float(TP/(TP+FP))\n",
    "                    #print(\"precision\",precision)\n",
    "                    if TP+FN !=0:    \n",
    "                        recall=float(TP/(TP+FN))\n",
    "                   # print(\"recall\",recall)\n",
    "\n",
    "                    precision_all.append(precision)\n",
    "                    recall_all.append(recall)\n",
    "                    #print(\"TP,FP,TN,FN\")\n",
    "                    #print(TP,FP,TN,FN)\n",
    "\n",
    "                    a=0\n",
    "                    b=0\n",
    "                    acc1=0\n",
    "                    acc2=0\n",
    "                    for i in range(n):\n",
    "                            if data[j][i]== 1 :\n",
    "                                a=a+1\n",
    "                                if fi[i]==1:\n",
    "                                     acc1=acc1+1 \n",
    "\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                    a1=float(acc1/a)\n",
    "\n",
    "\n",
    "\n",
    "        #                         print(a)\n",
    "        #                         print(acc1)\n",
    "        #                         print(a1)\n",
    "                    ar_all.append(a1)\n",
    "\n",
    "                count = count+1\n",
    "                maxi=max(ar_all)\n",
    "                mini= min(ar_all)\n",
    "                DP=float(maxi-mini)\n",
    "                print(\"individual acceptance rates\")\n",
    "                print(ar_all)\n",
    "                print(\"individual precision\")\n",
    "                print(precision_all)\n",
    "                print(\"individual recall\")\n",
    "                print(recall_all)\n",
    "                print(\"DP all\")\n",
    "                print(DP)\n",
    "                f_acc=0\n",
    "                for i in range(n):\n",
    "                     if fi[i] == r[i]:\n",
    "                            f_acc=f_acc+1\n",
    "                f_acc_l=float((f_acc*100)/n) \n",
    "\n",
    "        #######################################################################33   \n",
    "\n",
    "        #                         print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                TP=0\n",
    "                FP=0\n",
    "                FN=0\n",
    "                TN=0\n",
    "                precision=0\n",
    "                recall=0\n",
    "                accu=0\n",
    "                for i in range(n):\n",
    "                        if fi[i]==1 and r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        if fi[i]==1 and r[i]==-1:\n",
    "                            FP=FP+1 \n",
    "                        if fi[i]==-1 and r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        if fi[i]==-1 and r[i]==-1:\n",
    "                            TN=TN+1    \n",
    "\n",
    "                if TP+FP!=0:\n",
    "                    precision=float(TP/(TP+FP))\n",
    "                print(\"precision all\",precision)\n",
    "                if TP+FN!=0:\n",
    "                    recall=float(TP/(TP+FN))\n",
    "\n",
    "\n",
    "                print(\"recall all\",recall)\n",
    "                accu=float((TP+TN)/(TP+FN+TN+FP))\n",
    "\n",
    "\n",
    "                print(\"accuracy all\",accu)\n",
    "\n",
    "\n",
    "\n",
    "                print(\"TP,FP,TN,FN\")\n",
    "                print(TP,FP,TN,FN)\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                a1=float(acc1/a)\n",
    "          \n",
    "\n",
    "    print(\"<--------------------------------------->\")\n",
    "    alpha_weight=np.arange(0,1.05,.05)        \n",
    "    return accu_all,DP_all,acceptance_rate,alpha_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "####################LPCA######CODE######to corverge in given beta_avg setting###########by varying alpha from 1 to 0\n",
    "\n",
    "#data1 is sensitive group in binary matrix sets \n",
    "import time\n",
    "import pulp as p \n",
    "def min_sum_lpca(data1,beta,eps,e,beta_avg,alpha):\n",
    "    import pulp as p \n",
    "    import math\n",
    "    \n",
    "    \n",
    "    m=data1.shape[0]\n",
    "    n=data1.shape[1]\n",
    "    print('dimension of data')\n",
    "    print(m,n)\n",
    "    \n",
    "    ################ sorted order(Ranking) of multy sensitive groups based on \"e\" confidence value from Random Forest.\n",
    "    h1=[]\n",
    "    h2=[]\n",
    "    h3=[]\n",
    "    h4=[]\n",
    "    h5=[]\n",
    "    h6=[]\n",
    "    h7=[]\n",
    "    key1=[]\n",
    "    key2=[]\n",
    "    key3=[]\n",
    "    key4=[]\n",
    "    key5=[]\n",
    "    key6=[]\n",
    "    key7=[]\n",
    "    cost=np.zeros(n,dtype=int)\n",
    "    data2=np.zeros((m,n),dtype=int)\n",
    "    for i in range(n):\n",
    "        if data1[0][i]==1:            \n",
    "\n",
    "            h1.append(e[i][1])\n",
    "            key1.append(i)\n",
    "\n",
    "        elif data1[1][i]==1:\n",
    "            h2.append(e[i][1])\n",
    "            key2.append(i)\n",
    "            \n",
    "        if data1[2][i]==1:\n",
    "            h3.append(e[i][1])\n",
    "            key3.append(i)\n",
    "            \n",
    "        elif data1[3][i]==1:\n",
    "            h4.append(e[i][1])\n",
    "            key4.append(i)\n",
    "        elif data1[4][i]==1:\n",
    "            h5.append(e[i][1])\n",
    "            key5.append(i)\n",
    "        elif data1[5][i]==1:\n",
    "            h6.append(e[i][1])\n",
    "            key6.append(i)\n",
    "        elif data1[6][i]==1:\n",
    "            h7.append(e[i][1])\n",
    "            key7.append(i)\n",
    "\n",
    "    \n",
    "    for i in range(1,len(h1)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h1[j-1]<h1[j]:\n",
    "                index=j\n",
    "                var=h1[j]\n",
    "                h1[j]=h1[j-1]\n",
    "                h1[j-1]=var\n",
    "\n",
    "                var2=key1[j]\n",
    "                key1[j]=key1[j-1]\n",
    "                key1[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "\n",
    "    for i in range(1,len(h2)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h2[j-1]<h2[j]:\n",
    "                index=j\n",
    "                var=h2[j]\n",
    "                h2[j]=h2[j-1]\n",
    "                h2[j-1]=var\n",
    "\n",
    "                var2=key2[j]\n",
    "                key2[j]=key2[j-1]\n",
    "                key2[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h3)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h3[j-1]<h3[j]:\n",
    "                index=j\n",
    "                var=h3[j]\n",
    "                h3[j]=h3[j-1]\n",
    "                h3[j-1]=var\n",
    "\n",
    "                var2=key3[j]\n",
    "                key3[j]=key3[j-1]\n",
    "                key3[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h4)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h4[j-1]<h4[j]:\n",
    "                index=j\n",
    "                var=h4[j]\n",
    "                h4[j]=h4[j-1]\n",
    "                h4[j-1]=var\n",
    "\n",
    "                var2=key4[j]\n",
    "                key4[j]=key4[j-1]\n",
    "                key4[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h5)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h5[j-1]<h5[j]:\n",
    "                index=j\n",
    "                var=h5[j]\n",
    "                h5[j]=h5[j-1]\n",
    "                h5[j-1]=var\n",
    "\n",
    "                var2=key5[j]\n",
    "                key5[j]=key5[j-1]\n",
    "                key5[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "                \n",
    "                \n",
    "    for i in range(1,len(h6)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h6[j-1]<h6[j]:\n",
    "                index=j\n",
    "                var=h6[j]\n",
    "                h6[j]=h6[j-1]\n",
    "                h6[j-1]=var\n",
    "\n",
    "                var2=key6[j]\n",
    "                key6[j]=key6[j-1]\n",
    "                key6[j-1]=var2\n",
    "            else:\n",
    "                break        \n",
    "                \n",
    "\n",
    "    for i in range(1,len(h7)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h7[j-1]<h7[j]:\n",
    "                index=j\n",
    "                var=h7[j]\n",
    "                h7[j]=h7[j-1]\n",
    "                h7[j-1]=var\n",
    "\n",
    "                var2=key7[j]\n",
    "                key7[j]=key7[j-1]\n",
    "                key7[j-1]=var2\n",
    "            else:\n",
    "                break \n",
    "   \n",
    "    \n",
    "    #####alpha2 is just another weight influencing parameter for now its neutral with [\"ones\"] vector\n",
    "    alpha2=[1,1,1,1,1,1,1]\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "# ranking each sensitive subgroups as per confidence values(with or without weights)   \n",
    "\n",
    "####################################################################################\n",
    "\n",
    "######Setup1################LPCA with ranked sensitive groups without equalized weight \n",
    "    \n",
    "    # ->comment the setup1 below and try setup2\n",
    "    for j in range(len(key1)):    \n",
    "        data2[0][key1[j]]=(j+1)\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)\n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)\n",
    "                        \n",
    "        \n",
    "    for j in range(len(key4)):\n",
    "        data2[3][key4[j]]=(j+1)\n",
    "        \n",
    "                             \n",
    "    for j in range(len(key5)):\n",
    "        data2[4][key5[j]]=(j+1)\n",
    "        \n",
    "    for j in range(len(key6)):\n",
    "        data2[5][key6[j]]=(j+1)\n",
    "    for j in range(len(key7)):\n",
    "        data2[6][key7[j]]=(j+1)\n",
    "     \n",
    " #####Setup2#################LPCA with ranked sensitive groups with weight equalization\n",
    "    '''\n",
    "#             's_male', 's_female'  ,'r_white', 'r_black', 'r_asian-pac-islander','r_amer-indian-eskimo, others '\n",
    "#Sizes of each  9211,       4356,      11678,    1220,          404,                   150,               115\n",
    "#subgroup\n",
    "#Let the weight of ranks within remains of equal weight with respect to higher size subgroups\n",
    "    #####gender####\n",
    "    for j in range(len(key1)):    \n",
    "        data2[0][key1[j]]=(j+1)*alpha2[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*((beta[0]*len(key1))/(beta[1]*len(key2)))*alpha2[1]\n",
    "   \n",
    "\n",
    "    #####race####\n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)*alpha2[2]\n",
    "                         \n",
    "        \n",
    "    for j in range(len(key4)):                       \n",
    "        data2[3][key4[j]]=(j+1)*((beta[2]*len(key3))/(beta[3]*len(key4)))*alpha2[3]                           \n",
    "    for j in range(len(key5)):\n",
    "        data2[4][key5[j]]=(j+1)*((beta[2]*len(key3))/(beta[4]*len(key5)))*alpha2[4]      \n",
    "    for j in range(len(key6)):                 \n",
    "        data2[5][key6[j]]=(j+1)*((beta[2]*len(key3))/(beta[5]*len(key6)))*alpha2[5]  \n",
    "    for j in range(len(key7)):                 \n",
    "        data2[6][key7[j]]=(j+1)*((beta[2]*len(key3))/(beta[6]*len(key7)))*alpha2[6]         \n",
    "    '''    \n",
    "\n",
    "\n",
    "########################################################################    \n",
    "#sum up the weighted subgroup rank in cost \n",
    "    for j in range(n):\n",
    "        summ=0\n",
    "        for i in range(m):\n",
    "       \n",
    "            summ=summ+data2[i][j] \n",
    "        cost[j]=summ\n",
    "\n",
    "    Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
    "    \n",
    "###############################Optimization fuction for LPCA###################\n",
    "# beta_avg(convergence point to achieve least DP at alpha=0   \n",
    "# beta_actual is beta_initial (acceptance rate) config obtaind for each group from random forest prediction)   \n",
    "\n",
    "\n",
    "    X=np.zeros(n+m+1,dtype=p.LpVariable)\n",
    "    Y=np.zeros(m,dtype=p.LpVariable)\n",
    "    \n",
    "    sizes=np.zeros(m,dtype=int)\n",
    "    \n",
    "#sum up the subgroup ranks in cost   \n",
    "    max_size=0\n",
    "    for i in range(m):\n",
    "        count=0\n",
    "        for j in range(n):\n",
    "            if data1[i][j]==1:\n",
    "                count=count+1 \n",
    "        if count>max_size:\n",
    "            max_size=count\n",
    "        sizes[i]=count\n",
    "    print(sizes)        \n",
    "    \n",
    "    \n",
    "    \n",
    "    beta_initial=beta\n",
    "    #beta_initial = [0.2576267506242536, 0.07575757575757576, 0.2111662955985614, 0.08934426229508197, 0.2698019801980198, 0.1, 0.034782608695652174]        \n",
    "    \n",
    "    \n",
    "    select_sizes=np.zeros(m,dtype=int)\n",
    "   \n",
    "    size_final=np.zeros(m,dtype=int)\n",
    "\n",
    "    for i in range(m):\n",
    "        var1 = str(n+100+i)\n",
    "        Y[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Continuous')\n",
    "    \n",
    "    for i in range(n):\n",
    "        var1=str(i)       \n",
    "        X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "   \n",
    "    X[n]=p.LpVariable(str(n),lowBound=0,upBound=1,cat='Continuous')  \n",
    "\n",
    "    Lp_prob+= p.lpSum([(X[j])*cost[j] for j in range(n)])\n",
    "    \n",
    "\n",
    "    for i in range(2*m):\n",
    "        if i<m:\n",
    "\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) >= (Y[i]-eps)*sizes[i]\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) <= (Y[i]+eps)*sizes[i]\n",
    "    \n",
    "    '''\n",
    "# for minimum ddp at alpa=0 setup (like paper) change (1- alpha) to (alpha) & (alpha) to (1-alpha)    \n",
    "    for i in range(m):\n",
    "            if beta_initial[i] >= beta_avg:\n",
    "\n",
    "                Lp_prob += Y[i] >= (1-alpha)*beta_initial[i] +alpha*beta_avg\n",
    "                Lp_prob += Y[i] <= (1-alpha)*beta_initial[i] +alpha*beta_avg\n",
    "               \n",
    "            else:\n",
    "                Lp_prob += Y[i] >= (1-alpha)*beta_initial[i] + alpha*beta_avg\n",
    "                Lp_prob += Y[i] <= beta_avg   \n",
    "    '''      \n",
    "\n",
    "\n",
    "    for i in range(m):\n",
    "            if beta_initial[i] >= beta_avg:\n",
    "\n",
    "                Lp_prob += Y[i] >= (alpha)*beta_initial[i] +(1-alpha)*beta_avg\n",
    "                Lp_prob += Y[i] <= (alpha)*beta_initial[i] +(1-alpha)*beta_avg\n",
    "               \n",
    "            else:\n",
    "                Lp_prob += Y[i] >= (alpha)*beta_initial[i] + (1-alpha)*beta_avg\n",
    "                Lp_prob += Y[i] <= beta_avg               \n",
    "   \n",
    "    #####################################\n",
    "    status = Lp_prob.solve()   # Solver \n",
    "    print(p.LpStatus[status]) \n",
    "    print(\"objective is:\")        \n",
    "    #print(p.value(Lp_prob.objective))\n",
    "    print(\"discripency is:\") \n",
    "    print(p.value(X[n]))\n",
    "    x=np.zeros(n,dtype=float)\n",
    "    Synth1={}\n",
    "    Synth2={}\n",
    "    for i in range(n):\n",
    "        if(p.value(X[i])==1):\n",
    "            Synth1[i]=1 \n",
    "            Synth2[i]=-1\n",
    "\n",
    "        else:\n",
    "            Synth1[i]=-1\n",
    "            Synth2[i]=1\n",
    "    Synthu1=Synth1  \n",
    "    Synthu2=Synth2  \n",
    "    \n",
    "              \n",
    "    return Synthu1,Synthu2   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adult Data Preparation\n",
    "import time\n",
    "# import pulp as p \n",
    "# from random import *\n",
    "\n",
    "# Add column names to data set\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', \n",
    "           'relationship', 'race','sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "# Read in train data\n",
    "adult_train = pd.read_csv('data/adult_actual/adult_train_data.csv', header=None, names=columns, skipinitialspace=True)\n",
    "\n",
    "# Drop the fnlwgt column which is useless for later analysis\n",
    "adult_train = adult_train.drop('fnlwgt', axis=1)\n",
    "\n",
    "# Read in test data\n",
    "adult_test = pd.read_csv('data/adult_actual/adult_test_data.csv', header=None, skiprows=1, names=columns, skipinitialspace=True)\n",
    "\n",
    "# Drop the fnlwgt column which is useless for later analysis\n",
    "adult_test = adult_test.drop('fnlwgt', axis=1)\n",
    "\n",
    "# Remove '.' in income column\n",
    "adult_test['income'] = adult_test['income'].apply(lambda x: '>50k' if x=='>50k.'  else '<=50k')\n",
    "\n",
    "\n",
    "# Convert '?' to NaNs and remove the entries with NaN value\n",
    "# Check missing value code and convert to NaNs\n",
    "object_col = adult_train.select_dtypes(include=object).columns.tolist()\n",
    "for col in object_col:\n",
    "    adult_train.loc[adult_train[col]=='?', col] = np.nan\n",
    "    adult_test.loc[adult_test[col]=='?', col] = np.nan\n",
    "\n",
    "# Perform an mssing assessment in each column of the dataset.\n",
    "col_missing_pct = adult_train.isna().sum()/adult_train.shape[0]\n",
    "col_missing_pct.sort_values(ascending=False)\n",
    "\n",
    "# Remove data entries with missing value\n",
    "adult_train = adult_train.dropna(axis=0, how='any')\n",
    "adult_test = adult_test.dropna(axis=0, how='any')\n",
    "\n",
    "# Show the results of the split\n",
    "# print(\"After removing the missing value:\")\n",
    "# print(\"Training set has {} samples.\".format(adult_train.shape[0]))\n",
    "# print(\"Testing set has {} samples.\".format(adult_test.shape[0]))\n",
    "for col in object_col:\n",
    "    print(adult_train[col].value_counts(dropna=False)/adult_train.shape[0],'\\n')\n",
    "# print(adult_train.head())\n",
    "# print(adult_test.head())    \n",
    "\n",
    "adult_train.reset_index(drop=True, inplace=True)\n",
    "adult_test.reset_index(drop=True, inplace=True)\n",
    "p=adult_train.shape[0]\n",
    "q =adult_test.shape[0]\n",
    "# reducing dimensionality of some very sparse features\n",
    "for i in range(0,p):\n",
    "    if adult_train.loc[i,'native-country'] not in [\"united-states\"] :\n",
    "               adult_train.loc[i,\"native-country\"] = \"non-united-stated\"        \n",
    "    if adult_train.loc[i,\"education\"] in [\"Preschool\", \"1st-4th\", \"5th-6th\", \"7th-8th\"]:\n",
    "               adult_train.loc[i,\"education\"] = \"prim-middle-school\"\n",
    "    elif adult_train.loc[i,\"education\"] in [\"9th\", \"10th\", \"11th\", \"12th\"]:\n",
    "               adult_train.loc[i,\"education\"] = \"high-school\"   \n",
    "    if adult_train.loc[i,'income'] in [\">50k\"] :\n",
    "               adult_train.loc[i,\"income\"] = 1 \n",
    "    else: \n",
    "               adult_train.loc[i,\"income\"] = 0         \n",
    "#reducing dimensionality of some very sparse features\n",
    "for i in range(0,q):                \n",
    "    if adult_test.loc[i,'native-country'] not in [\"united-states\"]:\n",
    "               adult_test.loc[i,'native-country'] = \"non-united-stated\"\n",
    "    if adult_test.loc[i,'education'] in [\"Preschool\", \"1st-4th\", \"5th-6th\", \"7th-8th\"]:\n",
    "               adult_test.loc[i,'education'] = \"prim-middle-school\"\n",
    "    elif adult_test.loc[i,'education'] in [\"9th\", \"10th\", \"11th\", \"12th\"]:\n",
    "               adult_test.loc[i,'education'] = \"high-school\"   \n",
    "    if adult_test.loc[i,'income'] in [\">50k\",\">50k.\"] :\n",
    "               adult_test.loc[i,\"income\"] = 1 \n",
    "    else: \n",
    "               adult_test.loc[i,\"income\"] = 0            \n",
    "# print(adult_train.head())\n",
    "# print(adult_test.head())\n",
    "DATA=pd.concat([adult_train,adult_test],ignore_index=True)\n",
    "# print(DATA.tail())\n",
    "m=DATA.shape[1]\n",
    "\n",
    "dat=DATA.iloc[:,0:m-1]\n",
    "\n",
    "\n",
    "# Initialize a scaler, then apply it to the features\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "num_col = dat.dtypes[dat.dtypes != 'object'].index\n",
    "features_log_minmax_transform = pd.DataFrame(data = dat)\n",
    "features_log_minmax_transform[num_col] = scaler.fit_transform(features_log_minmax_transform[num_col])\n",
    "\n",
    "display(features_log_minmax_transform.head())\n",
    "\n",
    "# sens=DATA[['sex','race']]\n",
    "\n",
    "Data_c = pd.get_dummies(features_log_minmax_transform, columns=['sex','race','workclass','education','marital-status','occupation','relationship','native-country'], prefix =['s','r','work','edu','ms','occ','rls','nc'])\n",
    "r=DATA.iloc[:,m-1]\n",
    "print(Data_c)\n",
    "print(DATA['income'].value_counts())\n",
    "\n",
    "#######Random forest Confidence Values fetched #############\n",
    "X_test,Y_test_pred,Y_test,e = adult_rf(Data_c , r)\n",
    "\n",
    "\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "# Y_test_pred.reset_index()\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print(X_test)\n",
    "# print(Y_test_pred)\n",
    "# print(Y_test)\n",
    "\n",
    "##7 sensitive sub groupss in adult(independent group overlapping case with Gender and Race)\n",
    "sens=X_test[['s_male', 's_female'  ,'r_white', 'r_black', 'r_asian-pac-islander','r_amer-indian-eskimo','r_other']]\n",
    "\n",
    "####################\n",
    "sensitive = sens.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e)\n",
    "\n",
    "\n",
    "# Observe group wise acc_rate,precision, recall, DP\n",
    "#eps=.005 ensures DDP<.01\n",
    "#see the the convergence at alpha =0\n",
    "#SEE Optimal group wise acceptance rate/config (individual beta) with varying alpha\n",
    "\n",
    "#observe valid configurations only\n",
    "\n",
    "#optimal before iterations tells the valid configuration on parameters.\n",
    "#undefined ,means invalid configuration on given parameters\n",
    "       #if undefined increase eps from .005 to .01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''      #############################READ ME\n",
    "\n",
    "    adult_rf():\n",
    "1> Get confidence values of predicted labels\n",
    "      main():\n",
    "2> pass parameters or inputs (beta_avg, alpha=[0,1],within eps,)\n",
    "    #(In code use alpha=1  for beta_initial and alpha=0  for beta_avg)\n",
    "     (In paper: alpha=1 to alpha= 0)\n",
    "   -uses (sensitive groups with predicted labels) to fetch \n",
    "    RF predicted initial acceptance rate(named as: in code and beta_initial in paper)\n",
    "    -in other codes we have hardcoded beta_initial in case of adult in min_sum_lpca()\n",
    "   -beta_initial converges at beta_avg(name in paper: beta^)   \n",
    "   min_sum_lpca():\n",
    "3> use optimizer to converge at beta_avg with eps=.005(DDP<=.01), at alpha=0(for generating least DDP config)\n",
    "   \n",
    "'''\n",
    "\n",
    "\n",
    "'''  \n",
    "###############################COMMENT#######################\n",
    "Ranks weights can further be groupwise equalized ranks within each group for weighted prec and weighted recall\n",
    "#########uncomment the setup 2 and try again###############\n",
    "'''\n",
    "\n",
    "\n",
    "#COMMENT\n",
    "    #beta_actual(we hardcode can be passed in min_sum_lpca()) mentioned in min_sum_lpca():function cell below \n",
    "    #( In paper its mentioned beta_initial )    \n",
    "    #beta_initial = [0.2576, 0.0757, 0.2111, 0.0893, 0.2698, 0.1, 0.0347]\n",
    "    \n",
    " ######COMMENT min_sum_lpca()\n",
    "#beta_initial hardcode in min_sum_lpca() with name beta_actual i.e acc_rate of RF can be paased dynamically\n",
    "            ##parameters\n",
    "                  #data: sensitive features\n",
    "                  #beta_avg (beta^): convergence point from beta_initial()\n",
    "                  #alpha=0 for minimum DDP achievement\n",
    "                    #eps: to contain config within eps(eps=.005 means achieving DDP<2*eps means <.01 DDP )      \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMENT:\n",
    "#RESULT\n",
    "#BELOW is the method for weighted precision/recall by observing the (configs groupwise)\n",
    "# Precision when subgroups configs go downward and Recall when subgroup configs goes upward#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta_avg at .20, find weighted precision and recall groupwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rate=[\n",
    "[0.2526327217457388, 0.0711662075298439, 0.20619969172803562, 0.08442622950819673, 0.26485148514851486, 0.1, 0.034782608695652174]\n",
    ",    \n",
    "[0.24112474215611768, 0.09618916437098256, 0.20397328309642063, 0.10655737704918032, 0.2524752475247525, 0.12, 0.06956521739130435]\n",
    ",\n",
    "\n",
    "[0.22961676256649657, 0.12098255280073462, 0.20174687446480563, 0.12868852459016394, 0.2376237623762376, 0.14, 0.10434782608695652]\n",
    ",\n",
    "[0.21832591466724569, 0.1453168044077135, 0.1995204658331906, 0.15081967213114755, 0.22524752475247525, 0.16, 0.13043478260869565]\n",
    ",\n",
    "[0.2067093692324395, 0.17033976124885217, 0.1972940572015756, 0.17295081967213113, 0.2103960396039604, 0.18, 0.16521739130434782]\n",
    ",\n",
    "[0.19520138964281836, 0.19513314967860423, 0.19506764856996062, 0.19508196721311474, 0.19554455445544555, 0.2, 0.2]\n",
    "]\n",
    "prec=[\n",
    "[0.7739578856897292, 0.7806451612903226, 0.7736710963455149, 0.8155339805825242, 0.7850467289719626, 0.5333333333333333, 1.0]\n",
    ",\n",
    "[0.7847816298964431, 0.6992840095465394, 0.7779177162048698, 0.7, 0.7647058823529411, 0.5, 0.625]\n",
    ",\n",
    "[0.8014184397163121, 0.6129032258064516, 0.7809847198641766, 0.5923566878980892, 0.75, 0.3333333333333333, 0.5]\n",
    ",\n",
    "[0.8130283441074092, 0.5434439178515008, 0.778969957081545, 0.4945652173913043, 0.6813186813186813, 0.20833333333333334, 0.4]\n",
    ",\n",
    "[0.8256302521008403, 0.4919137466307278, 0.7769097222222222, 0.4028436018957346, 0.6, 0.18518518518518517, 0.3157894736842105]\n",
    ",\n",
    "[0.8320355951056729, 0.44588235294117645, 0.7664618086040387, 0.3403361344537815, 0.4810126582278481, 0.13333333333333333, 0.2608695652173913]\n",
    "]\n",
    "rec=[\n",
    "[0.644134477825465, 0.5170940170940171, 0.6323828920570265, 0.5185185185185185, 0.6666666666666666, 0.5333333333333333, 0.26666666666666666]\n",
    ",\n",
    "[0.6233905579399142, 0.6260683760683761, 0.6289884589273591, 0.5617283950617284, 0.6190476190476191, 0.6, 0.3333333333333333]\n",
    ",\n",
    "[0.6062231759656652, 0.6901709401709402, 0.6245756958587916, 0.5740740740740741, 0.5714285714285714, 0.4666666666666667, 0.4]\n",
    ",\n",
    "[0.5847639484978541, 0.7350427350427351, 0.6160896130346232, 0.5617283950617284, 0.49206349206349204, 0.3333333333333333, 0.4]\n",
    ",\n",
    "[0.5622317596566524, 0.7799145299145299, 0.6076035302104549, 0.5246913580246914, 0.40476190476190477, 0.3333333333333333, 0.4]\n",
    ",\n",
    "[0.5350500715307582, 0.8098290598290598, 0.5926680244399185, 0.5, 0.30158730158730157, 0.26666666666666666, 0.4]\n",
    "]\n",
    "dp=[0.2300,0.1829,0.1332,0.0948,0.04517,0.0049]\n",
    "accu=[ 0.8662,0.8649,0.8621,0.8562,0.8499,0.8406]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the final values\n",
    "import numpy as np\n",
    "\n",
    "#P R P R P R R \n",
    "#0.7251,0.8101 0.5470,0.7329 0.6311,0.7389 0.5308,0.6172 0.6764,0.4776  0.4666,0.5333  0.2666,0.7333\n",
    "#      \n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "p=[0,2,4]\n",
    "r=[1,3,5,6]\n",
    "# print(np.transpose(acc_rate))\n",
    "weight_prec=0\n",
    "weight_p=0\n",
    "weight_rec=0\n",
    "weight_r=0\n",
    "dp_list=[]\n",
    "\n",
    "\n",
    "sizes=[9211, 4356, 11678,1220, 404, 150,115]\n",
    "for i in range(6):\n",
    "    weight_prec=0\n",
    "    weight_p=0\n",
    "    weight_rec=0\n",
    "    weight_r=0\n",
    "    acc_list=[]\n",
    "    for j in range(7):\n",
    "        #print(j)\n",
    "        if j in p:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            #print(j)\n",
    "        if j in r:    \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "        #print(acc_rate[i][j])    \n",
    "        acc_list.append(acc_rate[i][j])\n",
    "    #print(acc_list)\n",
    "    dp=max(acc_list)-min(acc_list)   \n",
    "    dp_list.append(dp)     \n",
    "    wp=weight_prec/weight_p\n",
    "    wr=weight_rec/weight_r\n",
    "    weighted_precision.append(wp)\n",
    "    weighted_recall.append(wr)\n",
    "print(weighted_precision, weighted_recall,accu,dp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "weight_prec=[0.7740109907817448, 0.7806362588309396, 0.789236124867365, 0.7918502786654945, 0.7946288446067699, 0.7894120124604503]\n",
    "weight_rec=[0.5128780684336239, 0.6061968535289314, 0.6544692665219972, 0.6819302851945664, 0.7076581320146921, 0.723097994284435] \n",
    "weight_acc=[0.8662, 0.8649, 0.8621, 0.8562, 0.8499, 0.8406]\n",
    "dp=[0.2300688764528627,0.18291003013344814,  0.13327593628928108, 0.0948127421437796, 0.04517864829961257, 0.0049323514300393945]\n",
    "print(weight_prec[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####--->>beta at .20\n",
    "import pulp as p \n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "alpha=[1,.8,.6,.4,.2,0]\n",
    "\n",
    "\n",
    "a=[acc_rate[i][0]*100 for i in range(6)]  \n",
    "b=[acc_rate[i][1]*100 for i in range(6)]  \n",
    "c=[acc_rate[i][2]*100 for i in range(6)]  \n",
    "d=[acc_rate[i][3]*100 for i in range(6)]   \n",
    "e=[acc_rate[i][4]*100 for i in range(6)]  \n",
    "f=[acc_rate[i][5]*100 for i in range(6)]   \n",
    "g=[acc_rate[i][6]*100 for i in range(6)] \n",
    "\n",
    "\n",
    "  # 's_male', 's_female'  ,'r_white', 'r_black', 'r_asian-pac-islander','r_amer-indian-eskimo'\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(alpha,a,label='Male',color='red',linestyle='--') \n",
    "ax.plot(alpha,b,label='Female',color='brown',linestyle='--')  \n",
    "ax.plot(alpha,c,label='White',color='blue',linestyle='--') \n",
    "ax.plot(alpha,d,label='Black',color='yellow',linestyle='--')\n",
    "ax.plot(alpha,e,label='Asian',color='green',linestyle='--') \n",
    "ax.plot(alpha,f,label='Am-Ind',color='magenta',linestyle='--')\n",
    "ax.plot(alpha,g,label='Other',color='cyan',linestyle='--')\n",
    "\n",
    "   \n",
    "plt.title('')\n",
    "ax.set_xlabel('Fairness parameter (alpha)')\n",
    "ax.set_ylabel('% in +ve class (Acceptance Rate)') \n",
    "\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.30), shadow=True, ncol=10)\n",
    "plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the above results with DP variations\n",
    "#P R P R P R R \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "np.set_printoptions(precision=4)  # For compact display.\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "\n",
    "weight_prec=[0.7740109907817448, 0.7806362588309396, 0.789236124867365, 0.7918502786654945, 0.7946288446067699, 0.7894120124604503]\n",
    "weight_rec=[0.5128780684336239, 0.6061968535289314, 0.6544692665219972, 0.6819302851945664, 0.7076581320146921, 0.723097994284435] \n",
    "weight_acc=[0.8662, 0.8649, 0.8621, 0.8562, 0.8499, 0.8406]\n",
    "dp=[0.2300688764528627,0.18291003013344814,  0.13327593628928108, 0.0948127421437796, 0.04517864829961257, 0.0049323514300393945]\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "'''\n",
    "ax.plot(dp,y1,label='Weighted Precision',color='blue',marker='^',linestyle='-')  \n",
    "ax.plot(dp,y2,label='Weighted Recall',color='cyan',marker='^',linestyle='-')\n",
    "ax.plot(dp,y3,label=' Accuracy',color='red',marker='^',linestyle='-')\n",
    "'''\n",
    "ax.plot(dp[::-1],weight_prec[::-1],label='Weighted Precision',color='blue',marker='^',linestyle='--')  \n",
    "ax.plot(dp[::-1],weight_rec[::-1],label='Weighted Recall',color='cyan',marker='^',linestyle='--')\n",
    "ax.plot(dp[::-1],weight_acc[::-1],label=' Accuracy',color='red',marker='^',linestyle='--')\n",
    "\n",
    "   \n",
    "plt.title('')\n",
    "ax.set_xlabel('Demographic Disparity')\n",
    "ax.set_ylabel('Performance Metrics') \n",
    "# ax.set_ylabel('% in +ve class (Acceptance Rate)') \n",
    "\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.30), shadow=True, ncol=10)\n",
    "plt.show() \n",
    "fig.savefig('a2.png') \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
