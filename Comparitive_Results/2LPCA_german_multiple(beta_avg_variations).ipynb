{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Notes\n",
    "\n",
    "#1.Run cells in the code from top to down smoothly to see its working\n",
    "#2. Read The COMMENTS and change parameter accordinly\n",
    "#\n",
    "\n",
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Import the three supervised learning models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA    \n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################LPCA######CODE##### same # beta setting ########\n",
    "               ### fixing alpha to 0 and (varying beta_avg) #########\n",
    "    # In beta_avg config all sub groups have same acceptance rate \n",
    "\n",
    "#data1 is sensitive group in binary matrix sets \n",
    "import time\n",
    "import pulp as p \n",
    "def min_sum_lpca(data1,beta_initial,eps,e,beta,alpha):\n",
    "#here beta_avg=beta[0], same beta is used for ranked weighting\n",
    "    import pulp as p \n",
    "    import math\n",
    "    \n",
    "    m=data1.shape[0]\n",
    "    n=data1.shape[1]\n",
    "    print('dimension of data')\n",
    "    print(m,n)\n",
    "    \n",
    "    ################ sorted result\n",
    "    a1=0\n",
    "    a2=0\n",
    "    b1=0\n",
    "    b2=0\n",
    "    c1=0\n",
    "    c2=0\n",
    "    d1=0\n",
    "    d2=0\n",
    "    e1=0\n",
    "    e2=0\n",
    "    h1=[]\n",
    "    h2=[]\n",
    "    h3=[]\n",
    "    h4=[]\n",
    " \n",
    "    key1=[]\n",
    "    key2=[]\n",
    "    key3=[]\n",
    "    key4=[]\n",
    "\n",
    "    cost=np.zeros(n,dtype=int)\n",
    "    data2=np.zeros((m,n),dtype=int)\n",
    "    for i in range(n):\n",
    "        if data1[0][i]==1:            \n",
    "\n",
    "            h1.append(e[i][1])\n",
    "            key1.append(i)\n",
    "            if data1[2][i]==1:\n",
    "                a1=a1+1\n",
    "            elif data1[3][i]==1:\n",
    "                b1=b1+1\n",
    "           \n",
    "\n",
    "        elif data1[1][i]==1:\n",
    "            h2.append(e[i][1])\n",
    "            key2.append(i)\n",
    "            if data1[2][i]==1:\n",
    "                a2=a2+1\n",
    "            elif data1[3][i]==1:\n",
    "                b2=b2+1\n",
    "            \n",
    "            \n",
    "        if data1[2][i]==1:\n",
    "            h3.append(e[i][1])\n",
    "            key3.append(i)\n",
    "            \n",
    "        elif data1[3][i]==1:\n",
    "            h4.append(e[i][1])\n",
    "            key4.append(i)\n",
    "        \n",
    "\n",
    "    for i in range(1,len(h1)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h1[j-1]<h1[j]:\n",
    "                index=j\n",
    "                var=h1[j]\n",
    "                h1[j]=h1[j-1]\n",
    "                h1[j-1]=var\n",
    "\n",
    "                var2=key1[j]\n",
    "                key1[j]=key1[j-1]\n",
    "                key1[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "\n",
    "    for i in range(1,len(h2)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h2[j-1]<h2[j]:\n",
    "                index=j\n",
    "                var=h2[j]\n",
    "                h2[j]=h2[j-1]\n",
    "                h2[j-1]=var\n",
    "\n",
    "                var2=key2[j]\n",
    "                key2[j]=key2[j-1]\n",
    "                key2[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h3)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h3[j-1]<h3[j]:\n",
    "                index=j\n",
    "                var=h3[j]\n",
    "                h3[j]=h3[j-1]\n",
    "                h3[j-1]=var\n",
    "\n",
    "                var2=key3[j]\n",
    "                key3[j]=key3[j-1]\n",
    "                key3[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h4)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h4[j-1]<h4[j]:\n",
    "                index=j\n",
    "                var=h4[j]\n",
    "                h4[j]=h4[j-1]\n",
    "                h4[j-1]=var\n",
    "\n",
    "                var2=key4[j]\n",
    "                key4[j]=key4[j-1]\n",
    "                key4[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    #####alpha2 is just another weight influencing parameter for now its neutral with [\"ones\"] vector\n",
    "    alpha2=[1,1,1,1]\n",
    "    \n",
    "#####Setup2#################LPCA with ranked sensitive groups with weight equalization    \n",
    "    for j in range(len(key1)):    \n",
    "               \n",
    "        data2[0][key1[j]]=(j+1)*alpha2[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*((beta[0]*len(key1))/(beta[1]*len(key2)))*alpha2[1]\n",
    "    \n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)*alpha2[2]\n",
    "                         \n",
    "        \n",
    "    for j in range(len(key4)):                            \n",
    "        data2[3][key4[j]]=(j+1)*((beta[2]*len(key3))/(beta[3]*len(key4)))*alpha2[3]                         \n",
    " \n",
    " \n",
    "#sum up the weighted subgroup rank in cost        \n",
    "    for j in range(n):\n",
    "        summ=0\n",
    "        for i in range(m):\n",
    "       \n",
    "            summ=summ+data2[i][j] \n",
    "        cost[j]=summ\n",
    "        \n",
    "        \n",
    "    ################\n",
    "    \n",
    "    \n",
    "    Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
    "    ###############################Optimization fuction for LPCA###################\n",
    "# beta_avg(convergence point to achieve least DP at alpha=0   \n",
    "# beta_initial (acceptance rate) config obtaind for each group from random forest prediction)   \n",
    "\n",
    "\n",
    "    X=np.zeros(n+m+1,dtype=p.LpVariable)\n",
    "    Y=np.zeros(m,dtype=p.LpVariable)\n",
    "    \n",
    "    sizes=np.zeros(m,dtype=int)\n",
    "  \n",
    "    max_size=0\n",
    "    for i in range(m):\n",
    "        count=0\n",
    "        for j in range(n):\n",
    "            if data1[i][j]==1:\n",
    "                count=count+1 \n",
    "        if count>max_size:\n",
    "            max_size=count\n",
    "        sizes[i]=count\n",
    "    print(sizes)        \n",
    "    \n",
    " ################ As all beta[i]'s are same beta_avg=beta[0] #################   \n",
    "    \n",
    "    beta_avg=beta[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    select_sizes=np.zeros(m,dtype=int)\n",
    "   \n",
    "    size_final=np.zeros(m,dtype=int)\n",
    "\n",
    "    for i in range(m):\n",
    "        var1 = str(n+100+i)\n",
    "        Y[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Continuous')\n",
    "    \n",
    "    for i in range(n):\n",
    "        var1=str(i)       \n",
    "        X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "   \n",
    "    X[n]=p.LpVariable(str(n),lowBound=0,upBound=1,cat='Continuous')  \n",
    "#minimize Objective#\n",
    "    Lp_prob+= p.lpSum([(X[j])*cost[j] for j in range(n)])\n",
    "    \n",
    "\n",
    "    for i in range(2*m):\n",
    "        if i<m:\n",
    "\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) >= (Y[i]-eps)*sizes[i]\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) <= (Y[i]+eps)*sizes[i]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "# for minimum ddp at alpa=0 setup (like paper) change (1- alpha) to (alpha) & (alpha) to (1-alpha)    \n",
    "    for i in range(m):\n",
    "            if beta_initial[i] >= beta_avg:\n",
    "\n",
    "                Lp_prob += Y[i] >= (1-alpha)*beta_initial[i] +alpha*beta_avg\n",
    "                Lp_prob += Y[i] <= (1-alpha)*beta_initial[i] +alpha*beta_avg\n",
    "               \n",
    "            else:\n",
    "                Lp_prob += Y[i] >= (1-alpha)*beta_initial[i] + alpha*beta_avg\n",
    "                Lp_prob += Y[i] <= beta_avg   \n",
    "    '''      \n",
    "\n",
    "\n",
    "    for i in range(m):\n",
    "        Lp_prob += Y[i] >= (alpha)*beta_initial[i] +(1-alpha)*beta_avg\n",
    "        Lp_prob += Y[i] <= (alpha)*beta_initial[i] +(1-alpha)*beta_avg\n",
    "                         \n",
    "   \n",
    "    #####################################\n",
    "    status = Lp_prob.solve()   # Solver \n",
    "    print(p.LpStatus[status]) \n",
    "    print(\"objective is:\")        \n",
    "    print(p.value(Lp_prob.objective))\n",
    "    print(\"discripency is:\") \n",
    "    print(p.value(X[n]))\n",
    "    x=np.zeros(n,dtype=float)\n",
    "\n",
    "   # The solution status \n",
    "    Synth1={}\n",
    "    Synth2={}\n",
    "    # # Printing the final solution \n",
    "    for i in range(n):\n",
    "        if(p.value(X[i])==1):\n",
    "            Synth1[i]=1 \n",
    "            Synth2[i]=-1\n",
    "#             if(data1[2][i]==1):\n",
    "#                 print(\"no\")\n",
    "        else:\n",
    "            Synth1[i]=-1\n",
    "            Synth2[i]=1\n",
    "    Synthu1=Synth1  \n",
    "    Synthu2=Synth2  \n",
    "    \n",
    "              \n",
    "    return Synthu1,Synthu2   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from random import *\n",
    "from subprocess import check_output\n",
    "def german_rf(X,Y):\n",
    "    #Split data into training and test datasets (training will be based on 70% of data)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=10,shuffle=True)     #test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\n",
    "    print('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "    \n",
    "    #Scaling data\n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    #sc = StandardScaler(with_mean=False)\n",
    "    \n",
    "    \n",
    "    #sc.fit(X_train)\n",
    "    #X_train_std = sc.transform(X_train)\n",
    "    #X_test_std = sc.transform(X_test)\n",
    "\n",
    "    #X_train_std and X_test_std are the scaled datasets to be used in algorithms\n",
    "\n",
    "    #Applying SVC (Support Vector Classification)\n",
    "#     from sklearn.svm import SVC\n",
    "#     svm = SVC(kernel='rbf', random_state=0, gamma=.1, C=10.0,probability=True)\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=99, max_depth=None, min_samples_split=15, random_state=0)\n",
    "    \n",
    "    print(Y_train.dtypes)\n",
    "    Y_train=Y_train.astype('int')\n",
    "    print(Y_train.dtypes)\n",
    "    \n",
    "    print(Y_test.dtypes)\n",
    "    Y_test=Y_test.astype('int')\n",
    "    print(Y_test.dtypes)\n",
    "    \n",
    "    \n",
    "    rf.fit(X_train, Y_train)\n",
    "    print('The accuracy of the RF classifier on training data is {:.2f}'.format(rf.score(X_train, Y_train)))\n",
    "    print('The accuracy of the RF classifier on test data is {:.2f}'.format(rf.score(X_test, Y_test)))\n",
    "    print('####Train prediction Label###############################################')\n",
    "    Y_train_pred=rf.predict(X_train)\n",
    "    #print(y_1)\n",
    "    Y_test_pred=rf.predict(X_test)\n",
    "\n",
    "    print('####Actual Train Label###############################################')\n",
    "\n",
    "\n",
    "    print('####Change to colors###############################################')\n",
    "        \n",
    "    e=rf.predict_proba(X_test)\n",
    "    print(e)\n",
    "    return X_test,Y_test_pred,Y_test,e\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(datax, y_test, y_test_pred,e): \n",
    "        \n",
    "    n=datax.shape[1]\n",
    "    s=datax.shape[0]    \n",
    "    data = np.zeros((s, n), dtype = int)\n",
    "    \n",
    "    r = np.zeros(n, dtype = int) \n",
    "    \n",
    "    for i in range(n):\n",
    "        if int(y_test.iloc[i])==1 :\n",
    "            r[i]=1\n",
    "        else :\n",
    "            r[i]= -1  \n",
    "    \n",
    "    r2 = np.zeros(n, dtype = int) \n",
    "    for i in range(n):\n",
    "        if int(y_test_pred[i])==1 :\n",
    "            r2[i]=1\n",
    "        else :\n",
    "            r2[i]= -1          \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        for i in range(n):\n",
    "                data[j][i]= datax.iloc[j,i]\n",
    "                if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r[i]==1:\n",
    "                         acc1=acc1+1 \n",
    "\n",
    "        print(\"ACTUAL----------total ,accepted, aceeptance rate:\")             \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    print(\"data DP\")\n",
    "    print(DP)\n",
    "    \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        prec=0\n",
    "        reca=0\n",
    "        accur=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "        TP=0\n",
    "        TN=0\n",
    "        for i in range(n):\n",
    "             if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r2[i]==1:\n",
    "                        acc1=acc1+1 \n",
    "                        if r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        else:\n",
    "                             FP=FP+1                \n",
    "                    else:\n",
    "                        if r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        else:\n",
    "                            TN=TN+1    \n",
    "        \n",
    "        print(\"prec reca accuracy for each sens\") \n",
    "        prec= float(TP/(TP+FP))\n",
    "        reca= float(TP/(TP+FN))\n",
    "        accur= float((TP+TN)/a)\n",
    "        print(prec,reca,accur)\n",
    "        \n",
    "        print(\"Random Forest---------total , accepted, aceeptance rate:\")             \n",
    "        \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "#COMMENT   : \"ar\" #############use this predicted RandomForet accptance rate(config) hardcoded or\n",
    "#can pass it through LPCA as lpca() \n",
    "#above ar: Random forest acceptance rate is use as beta_initial/beta (named as beata_initial in paper)\n",
    "######################################RF acceptance rate as beta_initial #############   \n",
    "    beta_initial=ar\n",
    "\n",
    "########################################################################################    \n",
    " \n",
    "    \n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    \n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    \n",
    "    print(\"data DP\")\n",
    "    print(DP) \n",
    "    \n",
    "    print(\"Random Forest accuracy--------------------------\")\n",
    "    prec=0\n",
    "    reca=0\n",
    "    accur=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    for i in range(n):\n",
    "            if r2[i]==1:\n",
    "                acc1=acc1+1 \n",
    "                if r[i]==1:\n",
    "                    TP=TP+1\n",
    "                else:\n",
    "                     FP=FP+1                \n",
    "            else:\n",
    "                if r[i]==1:\n",
    "                     FN=FN+1\n",
    "                else:\n",
    "                     TN=TN+1    \n",
    "\n",
    "        \n",
    "    prec= float(TP/(TP+FP))\n",
    "    reca= float(TP/(TP+FN))\n",
    "    accur= float((TP+TN)/n)\n",
    "    print(prec,reca,accur)\n",
    "    \n",
    "    \n",
    "    \n",
    "    fi= np.zeros(n,dtype=int) \n",
    "#(beta_converge, alpha and epsilon are the parameters of LPCA ) \n",
    "# alpha=1 initial predicted(beta configs) by classifier (RF), at alpha=0 Least DDP config at beta_converge\n",
    "\n",
    "#Example for beta_converge where at all cases alpha=0\n",
    "   \n",
    "    \n",
    "\n",
    "    beta_converge=[.68,.7,.75,.8,.84,.87,.89 ,.91]\n",
    "    \n",
    "    gamma=np.zeros((8,4),dtype=float)\n",
    "    for i in range(4):\n",
    "        for j in range(8):\n",
    "            gamma[j][i]=beta_converge[j]\n",
    "    alpha=[0]\n",
    "    epsilon=[.01,.005]\n",
    "    t=0\n",
    "    new=0\n",
    "    \n",
    "    \n",
    "    t=0\n",
    "    a=0\n",
    "    \n",
    "    for eps in epsilon:\n",
    "        for new in range(8):\n",
    "        \n",
    "            for a in alpha:\n",
    "                k=0\n",
    "                u1,u2=min_sum_lpca(data,beta_initial,eps,e,gamma[new],a)\n",
    "                #######################Disp_impact#######################  \n",
    "                print(\"gamma-epsilon-delta\",gamma[new],eps)\n",
    "                accu_all=[]\n",
    "                DP_all=[]\n",
    "                precision_all=[]\n",
    "                recall_all=[]\n",
    "                ar_all=[]\n",
    "                acceptance_rate=np.zeros((7,28),dtype=float)\n",
    "                count=0\n",
    "                print(\"<--------------------------------------->\")\n",
    "                print(\"iteration t\",k)\n",
    "                k=k+1\n",
    "\n",
    "\n",
    "                for i in range(n):\n",
    "                     fi[i] = u1[i]\n",
    "\n",
    "\n",
    "                for j in range(s):\n",
    "                    print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "                    TP=0\n",
    "                    FP=0\n",
    "                    FN=0\n",
    "                    TN=0\n",
    "                    precision=0\n",
    "                    recall=0\n",
    "                    for i in range(n):\n",
    "                         if data[j][i]== 1 :                        \n",
    "                            if fi[i]==1 and r[i]==1:\n",
    "                                TP=TP+1\n",
    "                            if fi[i]==1 and r[i]==-1:\n",
    "                                FP=FP+1 \n",
    "                            if fi[i]==-1 and r[i]==1:\n",
    "                                FN=FN+1\n",
    "                            if fi[i]==-1 and r[i]==-1:\n",
    "                                TN=TN+1    \n",
    "                    if TP+FP !=0:\n",
    "                        precision=float(TP/(TP+FP))\n",
    "                    #print(\"precision\",precision)\n",
    "                    if TP+FN !=0:    \n",
    "                        recall=float(TP/(TP+FN))\n",
    "                    #print(\"recall\",recall)\n",
    "\n",
    "                    precision_all.append(precision)\n",
    "                    recall_all.append(recall)\n",
    "                    #print(\"TP,FP,TN,FN\")\n",
    "                    #print(TP,FP,TN,FN)\n",
    "\n",
    "                    a=0\n",
    "                    b=0\n",
    "                    acc1=0\n",
    "                    acc2=0\n",
    "                    for i in range(n):\n",
    "                            if data[j][i]== 1 :\n",
    "                                a=a+1\n",
    "                                if fi[i]==1:\n",
    "                                     acc1=acc1+1 \n",
    "\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                    a1=float(acc1/a)\n",
    "\n",
    "\n",
    "\n",
    "        #                         print(a)\n",
    "        #                         print(acc1)\n",
    "        #                         print(a1)\n",
    "                    ar_all.append(a1)\n",
    "\n",
    "                count = count+1\n",
    "                maxi=max(ar_all)\n",
    "                mini= min(ar_all)\n",
    "                DP=float(maxi-mini)\n",
    "                print(\"individual acceptance rates\")\n",
    "                print(ar_all)\n",
    "                print(\"individual precision\")\n",
    "                print(precision_all)\n",
    "                print(\"individual recall\")\n",
    "                print(recall_all)\n",
    "                print(\"DP all\")\n",
    "                print(DP)\n",
    "                f_acc=0\n",
    "                for i in range(n):\n",
    "                     if fi[i] == r[i]:\n",
    "                            f_acc=f_acc+1\n",
    "                f_acc_l=float((f_acc*100)/n) \n",
    "\n",
    "        #######################################################################33   \n",
    "\n",
    "        #                         print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                TP=0\n",
    "                FP=0\n",
    "                FN=0\n",
    "                TN=0\n",
    "                precision=0\n",
    "                recall=0\n",
    "                accu=0\n",
    "                for i in range(n):\n",
    "                        if fi[i]==1 and r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        if fi[i]==1 and r[i]==-1:\n",
    "                            FP=FP+1 \n",
    "                        if fi[i]==-1 and r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        if fi[i]==-1 and r[i]==-1:\n",
    "                            TN=TN+1    \n",
    "\n",
    "                if TP+FP!=0:\n",
    "                    precision=float(TP/(TP+FP))\n",
    "                print(\"precision all\",precision)\n",
    "                if TP+FN!=0:\n",
    "                    recall=float(TP/(TP+FN))\n",
    "\n",
    "\n",
    "                print(\"recall all\",recall)\n",
    "                accu=float((TP+TN)/(TP+FN+TN+FP))\n",
    "\n",
    "\n",
    "                print(\"accuracy all\",accu)\n",
    "\n",
    "\n",
    "\n",
    "                print(\"TP,FP,TN,FN\")\n",
    "                print(TP,FP,TN,FN)\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                a1=float(acc1/a)\n",
    "\n",
    "\n",
    "    print(\"<--------------------------------------->\")\n",
    "    alpha_weight=np.arange(0,1.05,.05)        \n",
    "    return accu_all,DP_all,acceptance_rate,alpha_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "# import pulp as p \n",
    "# from random import *\n",
    "from sklearn import preprocessing\n",
    "# Add column names to data set\n",
    "\n",
    "data= pd.read_csv('data/german.csv' , skipinitialspace=True)\n",
    "print(data.head())\n",
    "print(data.shape[0],data.shape[1])\n",
    "\n",
    "#sensitive columns name '12'='age','8'='gender/personal_status'  '19'- foreign workers 20'=1(good)/2(bad))\n",
    "\n",
    "# print(sens)\n",
    "r=data[['20']]\n",
    "# print(r)\n",
    "p=data.shape[0]\n",
    "for i in range(0,p):  \n",
    "    if data.loc[i,\"12\"]>25 :\n",
    "               data.loc[i,\"12\"] = 1 \n",
    "    else :\n",
    "               data.loc[i,\"12\"] = 2\n",
    "    if r.loc[i,'20'] == 1 :\n",
    "               r.loc[i,\"20\"] = 1 \n",
    "    else: \n",
    "               r.loc[i,\"20\"] = 0  \n",
    "            \n",
    "print(data['20'].value_counts())            \n",
    "\n",
    "print(data['8'].value_counts())\n",
    "print(data['12'].value_counts())\n",
    "print(data['19'].value_counts())\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "# Initialize a scaler, then apply it to the features\n",
    "'''\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "num_col = dat.dtypes[dat.dtypes != 'object'].index\n",
    "features_log_minmax_transform = pd.DataFrame(data = dat)\n",
    "features_log_minmax_transform[num_col] = scaler.fit_transform(features_log_minmax_transform[num_col])\n",
    "\n",
    "\n",
    "display(features_log_minmax_transform.head())\n",
    "'''\n",
    "\n",
    "# sens=DATA[['sex','race']]\n",
    "#Data_c = pd.get_dummies(features_log_minmax_transform, columns=['sex','race','workclass','education','marital-status','occupation','relationship','native-country'], prefix =['s','r','work','edu','ms','occ','rls','nc'])\n",
    "m=data.shape[1]\n",
    "data_c1=data.iloc[:,0:m-1]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data_c2 = min_max_scaler.fit_transform(data_c1)\n",
    "data_c = pd.DataFrame(data_c2,columns=data_c1.columns)\n",
    "print(data_c)\n",
    "\n",
    "\n",
    "\n",
    "X_test,Y_test_pred,Y_test,e = german_rf(data_c , r)\n",
    "#print(X_test.iloc[:,:])\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "# Y_test_pred.reset_index()\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print(X_test)\n",
    "# print(Y_test_pred)\n",
    "# print(Y_test)\n",
    "sens=X_test[['8', '12' ]]\n",
    "# print(sens)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "     \n",
    "# for i in range(0,p):  \n",
    "#     if r.loc[i,'y'] == 1 :\n",
    "#                r.loc[i,\"y\"] = 1 \n",
    "#     else: \n",
    "#                r.loc[i,\"y\"] = 0 \n",
    "print(sens['8'].value_counts())            \n",
    "print(sens['12'].value_counts())\n",
    "#print(sens['19'].value_counts())\n",
    "sens1=pd.get_dummies(sens, columns=['8','12'], prefix =['8','12'])\n",
    "sensitive=sens1.T\n",
    "print(sensitive) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "'''\n",
    "\n",
    "'''\n",
    "# beta=.70\n",
    "# .0075, .7533\n",
    "\n",
    "# beta=.72\n",
    "# .0048, .76\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################Run and observe the results at varius parameters\n",
    "# Run for getting the Final output\n",
    "\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )\n",
    "\n",
    "# Observe group wise acc_rate,precision, recall, DP\n",
    "#eps=.005 ensures DDP<.01  , configuration within [beta_avg+eps ,beta_avg-eps] \n",
    "#see the the outputs at alpha =0 beta_avg varies\n",
    "#SEE Optimal group wise acceptance rate/config (individual beta)\n",
    "\n",
    "#optimal before iterations tells the valid configuration on parameters.\n",
    "#undefined ,means invalid configuration on given parameters\n",
    "       #if undefined increase eps from .005 to .01 \n",
    "#gamma is beta_avg in vector form    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta same from LPCA\n",
    "\n",
    "acc_rate=[\n",
    "[0.6910994764397905, 0.6972477064220184, 0.6938775510204082, 0.6909090909090909],\n",
    "[0.743455497382199, 0.7431192660550459, 0.7428571428571429, 0.7454545454545455],\n",
    "[0.7905759162303665, 0.7981651376146789, 0.7918367346938775, 0.8],\n",
    "[0.8324607329842932, 0.8348623853211009, 0.8326530612244898, 0.8363636363636363],\n",
    "    [0.8638743455497382, 0.8623853211009175, 0.8612244897959184, 0.8727272727272727],\n",
    "\n",
    "[0.900523560209424, 0.908256880733945, 0.9020408163265307, 0.9090909090909091]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "prec=[\n",
    "    \n",
    "[0.8409090909090909, 0.7368421052631579, 0.8352941176470589, 0.6578947368421053],\n",
    "[0.8380281690140845, 0.7283950617283951, 0.8241758241758241, 0.6829268292682927],\n",
    "[0.8211920529801324, 0.6896551724137931, 0.8041237113402062, 0.6363636363636364],\n",
    "[0.8113207547169812, 0.6703296703296703, 0.7892156862745098, 0.6304347826086957],\n",
    "  [0.806060606060606, 0.648936170212766, 0.7819905213270142, 0.6041666666666666],\n",
    "\n",
    "[0.7965116279069767, 0.6262626262626263, 0.7647058823529411, 0.6]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "rec=[\n",
    "    \n",
    "[0.7928571428571428, 0.8888888888888888, 0.8208092485549133, 0.8333333333333334],\n",
    "[0.85, 0.9365079365079365, 0.8670520231213873, 0.9333333333333333],\n",
    "[0.8857142857142857, 0.9523809523809523, 0.9017341040462428, 0.9333333333333333],\n",
    "[0.9214285714285714, 0.9682539682539683, 0.930635838150289, 0.9666666666666667],\n",
    "   [0.95, 0.9682539682539683, 0.953757225433526, 0.9666666666666667],\n",
    "\n",
    "[0.9785714285714285, 0.9841269841269841, 0.976878612716763, 1.0]\n",
    "]\n",
    "\n",
    "accu= [0.7433,0.76666,0.75666,0.7566,0.7533,0.7466]\n",
    "\n",
    "beta=[.7,.75,.8,.84,.87,.91]\n",
    "\n",
    "beta_p=[]\n",
    "beta_r=[]\n",
    "beta_check=[0.8795811518324608, 0.7614678899082569, 0.8571428571428571, 0.7454545454545455]\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "   \n",
    "   \n",
    "    \n",
    "#P R P R P R R \n",
    "#0.7251,0.8101 0.5470,0.7329 0.6311,0.7389 0.5308,0.6172 0.6764,0.4776  0.4666,0.5333  0.2666,0.7333\n",
    "#      \n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "#p=[]\n",
    "#r=[0,2,4,1,3,5,6]\n",
    "\n",
    "dp_list=[]\n",
    "sizes=[191,109,245,55]\n",
    "for i in range(6):\n",
    "    weight_prec=0\n",
    "    weight_p=0\n",
    "    weight_rec=0\n",
    "    weight_r=0\n",
    "    cnt1=0\n",
    "    cnt2=0\n",
    "    for j in range(4):\n",
    "        #print(j)\n",
    "              \n",
    "        \n",
    "        if beta[i] <=beta_check[j]:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            cnt1=1\n",
    "        else:  \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "            cnt2=1\n",
    "    if cnt1==1:\n",
    "        wp=weight_prec/weight_p\n",
    "        weighted_precision.append(wp)\n",
    "        beta_p.append(beta[i])\n",
    "\n",
    "    if cnt2==1: \n",
    "        wr=weight_rec/weight_r\n",
    "        weighted_recall.append(wr) \n",
    "        beta_r.append(beta[i])\n",
    "            \n",
    "   \n",
    "    \n",
    "    \n",
    "len1=(len(weighted_precision)) \n",
    "len2=(len(weighted_recall)) \n",
    "    \n",
    "print(weighted_precision, weighted_recall,beta_p,beta_r,len1,len2)\n",
    "'''\n",
    "[0.9520125938773091, 0.9403591343418822, 0.8608764055619338],0.8561212064210523, 0.8128034718440105, 0.782912341254576, 0.7358009995069338\n",
    "0.4610404591572201, 0.5551300667433751, 0.5983458776930215, 0.6580824258162613,0.6476895465902177, 0.7070185074558867, 0.754950250161771\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "np.set_printoptions(precision=4)  # For compact display.\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "\n",
    "'''\n",
    "y1=savgol_filter(a, 6, 2)\n",
    "y2=savgol_filter(b, 6, 2)\n",
    "y3=savgol_filter(c, 6, 2)\n",
    "y4=savgol_filter(d, 6, 2)\n",
    "y5=savgol_filter(e, 6, 2)\n",
    "y6=savgol_filter(f, 6, 2)\n",
    "y7=savgol_filter(g, 6, 2)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "# weighted_precision=[0.8319821845141085, 0.8289612688936687, 0.8158017367577965, 0.8190048909330003, 0.8046384596813133, 0.7804878048780488] \n",
    "# weighted_recall= [0.7666666666666667, 0.8666666666666668, 0.8708865660085173, 0.9248935346496321, 0.927902973395931, 0.95113420956051]\n",
    "beta1=beta_p\n",
    "beta2=beta_r\n",
    " \n",
    "# accu=[0.6533,0.7033,0.7466, 0.7366,0.7433,0.74,0.7333]\n",
    "'''\n",
    "weighted_precision=[0.9520125938773091, 0.9403591343418822, 0.8608764055619338,0.8561212064210523, 0.8128034718440105, 0.782912341254576, 0.7358009995069338]\n",
    "weighted_recall=[0.4610404591572201, 0.5551300667433751, 0.5983458776930215, 0.6580824258162613,0.6476895465902177, 0.7070185074558867, 0.754950250161771]\n",
    "beta1=[.02,.025,.05,.1,.14,.16,.2]\n",
    "beta2=[.1,.14,.16,.2,.25,.3,.35]\n",
    "\n",
    "accu=[ 0.7774,0.7813,0.7954,0.8157,0.82442,0.8245,0.8216,0.8146,0.7954,0.7699]\n",
    "'''\n",
    "ax.plot(beta1,weighted_precision,label='Weighted Precision',color='blue',marker='^',linestyle='--')  \n",
    "ax.plot(beta2,weighted_recall,label='Weighted Recall',color='cyan',marker='^',linestyle='--')\n",
    "ax.plot(beta,accu,label=' Accuracy',color='red',marker='^',linestyle='--')\n",
    "#ax.vlines(y=[.1992], ymin=[0], ymax=[1], colors='purple', linestyles='--', lw=2, label='PRedict avg. acc.')\n",
    "#plt.axvline(.1992, color='green', linestyle='--')\n",
    "#plt.axvline(.10, color='orange', linestyle='--')\n",
    "#plt.axvline(.20, color='orange', linestyle='--')\n",
    "plt.axvline(0.8795, color='orange', linestyle='--')\n",
    "plt.axvline(0.7614, color='orange', linestyle='--')\n",
    "plt.axvline(0.8571, color='orange', linestyle='--')\n",
    "plt.axvline(0.7454, color='orange', linestyle='--')\n",
    "\n",
    "#  [0.8795811518324608, 0.7614678899082569, 0.8571428571428571, 0.7454545454545455]\n",
    "\n",
    "plt.title('')\n",
    "ax.set_xlabel('Acceptance Rate')\n",
    "ax.set_ylabel('Performance Metrics') \n",
    "# ax.set_ylabel('% in +ve class (Acceptance Rate)') \n",
    "\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.30), shadow=True, ncol=10)\n",
    "plt.show() \n",
    "fig.savefig('a2.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred ,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.2576267506242536, 0.07575757575757576, 0.2111662955985614, 0.08934426229508197, 0.2698019801980198, 0.1, 0.034782608695652174]\n",
    "[0.27979274611398963, 0.0718288334182374, 0.07343212490076739, 0.10702734489855925, 0.06472727272727273, 0.0]\n",
    "[0.26282051282051283, 0.58125, 0.5182539682539683, 0.21296296296296297]\n",
    "[0.8795811518324608, 0.7614678899082569, 0.8571428571428571, 0.7454545454545455]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "np.set_printoptions(precision=4)  # For compact display.\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "\n",
    "'''\n",
    "y1=savgol_filter(a, 6, 2)\n",
    "y2=savgol_filter(b, 6, 2)\n",
    "y3=savgol_filter(c, 6, 2)\n",
    "y4=savgol_filter(d, 6, 2)\n",
    "y5=savgol_filter(e, 6, 2)\n",
    "y6=savgol_filter(f, 6, 2)\n",
    "y7=savgol_filter(g, 6, 2)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "weighted_precision=[0.8319821845141085, 0.8289612688936687, 0.8158017367577965, 0.8190048909330003, 0.8046384596813133, 0.7804878048780488] \n",
    "weighted_recall= [0.7666666666666667, 0.8666666666666668, 0.8708865660085173, 0.9248935346496321, 0.927902973395931, 0.95113420956051]\n",
    "beta1=[.5,.6,.7,.75,.8,.84]\n",
    "beta2=[.6,.7,.75,.8,.84,.88]\n",
    " \n",
    "accu=[0.6533,0.7033,0.7466, 0.7366,0.7433,0.74,0.7333]\n",
    "'''\n",
    "weighted_precision=[0.9520125938773091, 0.9403591343418822, 0.8608764055619338,0.8561212064210523, 0.8128034718440105, 0.782912341254576, 0.7358009995069338]\n",
    "weighted_recall=[0.4610404591572201, 0.5551300667433751, 0.5983458776930215, 0.6580824258162613,0.6476895465902177, 0.7070185074558867, 0.754950250161771]\n",
    "beta1=[.02,.025,.05,.1,.14,.16,.2]\n",
    "beta2=[.1,.14,.16,.2,.25,.3,.35]\n",
    "\n",
    "accu=[ 0.7774,0.7813,0.7954,0.8157,0.82442,0.8245,0.8216,0.8146,0.7954,0.7699]\n",
    "'''\n",
    "ax.plot(beta1,weighted_precision,label='Weighted Precision',color='blue',marker='^',linestyle='--')  \n",
    "ax.plot(beta2,weighted_recall,label='Weighted Recall',color='cyan',marker='^',linestyle='--')\n",
    "ax.plot(beta,accu,label=' Accuracy',color='red',marker='^',linestyle='--')\n",
    "#ax.vlines(y=[.1992], ymin=[0], ymax=[1], colors='purple', linestyles='--', lw=2, label='PRedict avg. acc.')\n",
    "#plt.axvline(.1992, color='green', linestyle='--')\n",
    "#plt.axvline(.10, color='orange', linestyle='--')\n",
    "#plt.axvline(.20, color='orange', linestyle='--')\n",
    "plt.axvline(0.8795, color='orange', linestyle='--')\n",
    "plt.axvline(0.7614, color='orange', linestyle='--')\n",
    "plt.axvline(0.8571, color='orange', linestyle='--')\n",
    "plt.axvline(0.7454, color='orange', linestyle='--')\n",
    "\n",
    " \n",
    "plt.title('')\n",
    "ax.set_xlabel('Acceptance Rate')\n",
    "ax.set_ylabel('Performance Metrics') \n",
    "# ax.set_ylabel('% in +ve class (Acceptance Rate)') \n",
    "\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.30), shadow=True, ncol=10)\n",
    "plt.show() \n",
    "fig.savefig('a2.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec=[[0.8269230769230769, 0.7160493827160493, 0.803921568627451, 0.696969696969697],\n",
    "[0.8176100628930818, 0.7037037037037037, 0.8029556650246306, 0.6486486486486487],\n",
    "[0.8098159509202454, 0.7108433734939759, 0.81, 0.6304347826086957],\n",
    "[0.8098159509202454, 0.6976744186046512, 0.801980198019802, 0.6382978723404256]]\n",
    "\n",
    "rec=[[0.9214285714285714, 0.9206349206349206, 0.9479768786127167, 0.7666666666666667],\n",
    "[0.9285714285714286, 0.9047619047619048, 0.9421965317919075, 0.8],\n",
    "[0.9428571428571428, 0.9365079365079365, 0.9364161849710982, 0.9666666666666667],\n",
    "[0.9428571428571428, 0.9523809523809523, 0.9364161849710982, 1.0]]\n",
    "\n",
    "accu=[0.78,0.77, 0.7766, 0.7733]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yang ours\n",
    "\n",
    "gamma=[[0.8196, 0.7512, 0.8424, 0.6222],\n",
    "[0.8324, 0.7431, 0.8285, 0.6727],\n",
    "[0.8534, 0.7614, 0.8163, 0.8363],\n",
    "[0.8534, 0.7889, 0.8244, 0.8545]]\n",
    "\n",
    "prec=[[0.8269230769230769, 0.7160493827160493, 0.803921568627451, 0.696969696969697],\n",
    "[0.8176100628930818, 0.7037037037037037, 0.8029556650246306, 0.6486486486486487],\n",
    "[0.8098159509202454, 0.7108433734939759, 0.81, 0.6304347826086957],\n",
    "[0.8098159509202454, 0.6976744186046512, 0.801980198019802, 0.6382978723404256]]\n",
    "\n",
    "rec=[[0.9214285714285714, 0.9206349206349206, 0.9479768786127167, 0.7666666666666667],\n",
    "[0.9285714285714286, 0.9047619047619048, 0.9421965317919075, 0.8],\n",
    "[0.9428571428571428, 0.9365079365079365, 0.9364161849710982, 0.9666666666666667],\n",
    "[0.9428571428571428, 0.9523809523809523, 0.9364161849710982, 1.0]]\n",
    "\n",
    "accu=[0.78,0.77, 0.7766, 0.7733]\n",
    "\n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "p=[2]\n",
    "r=[0,1,3]\n",
    "#print(np.transpose(acc_rate))\n",
    "weight_prec=0\n",
    "weight_p=0\n",
    "weight_rec=0\n",
    "weight_r=0\n",
    "sizes=[191,109,245,55]\n",
    "dp_list=[]\n",
    "\n",
    "for i in range(4):\n",
    "    weight_prec=0\n",
    "    weight_rec=0\n",
    "    weight_p=0\n",
    "    weight_r=0\n",
    "    acc_list=[]\n",
    "    for j in range(4):\n",
    "        #print(j)\n",
    "        if j in p:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            #print(j)\n",
    "        if j in r:    \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "        #print(acc_rate[i][j])    \n",
    "        #acc_list.append(acc_rate[i][j])\n",
    "    #print(acc_list)\n",
    "    #dp=max(acc_list)-min(acc_list)   \n",
    "    #dp_list.append(dp)     \n",
    "    wp=weight_prec/weight_p\n",
    "    wr=weight_rec/weight_r\n",
    "    weighted_precision.append(wp)\n",
    "    weighted_recall.append(wr)\n",
    "print( weighted_precision,weighted_recall,accu)\n",
    "weight_prec1=weighted_precision\n",
    "weight_rec1=weighted_recall\n",
    "accu1=accu\n",
    "\n",
    "#yang actual\n",
    "\n",
    "gamma=[[0.8196, 0.7512, 0.8424, 0.6222],\n",
    "[0.8324, 0.7431, 0.8285, 0.6727],\n",
    "[0.8534, 0.7614, 0.8163, 0.8363],\n",
    "[0.8534, 0.7889, 0.8244, 0.8545]]\n",
    "\n",
    "prec=[[0.7995, 0.8145, 0.8151, 0.7380],\n",
    "[0.8113, 0.6913, 0.7783, 0.7297],\n",
    "[0.7914, 0.6746, 0.785, 0.6086],\n",
    "[0.7914, 0.6627, 0.7772, 0.6170]]\n",
    "\n",
    "rec=[[0.9108, 0.8913, 0.9304, 0.775],\n",
    "[0.9214, 0.8888, 0.9132, 0.9],\n",
    "[0.9214, 0.8888, 0.9075, 0.9333],\n",
    "[0.9214, 0.9047, 0.9075, 0.9666]]\n",
    "\n",
    "\n",
    "\n",
    "accu=[ 0.7757,0.7566,0.7366,0.7333]\n",
    "\n",
    "dp=[0.2202,0.1597,0.0919,0.0655]\n",
    "\n",
    "\n",
    "\n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "p=[2]\n",
    "r=[0,1,3]\n",
    "#print(np.transpose(acc_rate))\n",
    "weight_prec=0\n",
    "weight_p=0\n",
    "weight_rec=0\n",
    "weight_r=0\n",
    "sizes=[191,109,245,55]\n",
    "dp_list=[]\n",
    "\n",
    "for i in range(4):\n",
    "    weight_prec=0\n",
    "    weight_rec=0\n",
    "    weight_p=0\n",
    "    weight_r=0\n",
    "    acc_list=[]\n",
    "    for j in range(4):\n",
    "        #print(j)\n",
    "        if j in p:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            #print(j)\n",
    "        if j in r:    \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "        #print(acc_rate[i][j])    \n",
    "        #acc_list.append(acc_rate[i][j])\n",
    "    #print(acc_list)\n",
    "    #dp=max(acc_list)-min(acc_list)   \n",
    "    #dp_list.append(dp)     \n",
    "    wp=weight_prec/weight_p\n",
    "    wr=weight_rec/weight_r\n",
    "    weighted_precision.append(wp)\n",
    "    weighted_recall.append(wr)\n",
    "print( weighted_precision,weighted_recall,accu)\n",
    "weight_prec2=weighted_precision\n",
    "weight_rec2=weighted_recall\n",
    "accu2=accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[]\n",
    "r=[]\n",
    "a=[]\n",
    "for i in range(4):\n",
    "    p.append(weight_prec2[i])\n",
    "    p.append(weight_prec1[i])\n",
    "    r.append(weight_rec2[i])\n",
    "    r.append(weight_rec1[i])\n",
    "    a.append(accu2[i])\n",
    "    a.append(accu1[i])\n",
    "\n",
    "print(p)\n",
    "print(r)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.8151, 0.803921568627451, 0.7783, 0.8029556650246306, 0.785, 0.81, 0.7772, 0.801980198019802]\n",
    "[0.8837732394366198, 0.8972076905879722, 0.9080749295774648, 0.9013413816230718, 0.9132340845070422, 0.9445964676950592, 0.9232752112676056, 0.9546344735077129]\n",
    "[0.7757, 0.78, 0.7566, 0.77, 0.7366, 0.7766, 0.7333, 0.7733]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,Y_test_pred,Y_test,e = law_school_svm(data_c , r)\n",
    "#print(X_test.iloc[:,:])\n",
    "\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "# Y_test_pred.reset_index()\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print(X_test)\n",
    "# print(Y_test_pred)\n",
    "# print(Y_test)\n",
    "sens=X_test[['sex', 'race' ]]\n",
    "# print(sens)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "     \n",
    "# for i in range(0,p):  \n",
    "#     if r.loc[i,'y'] == 1 :\n",
    "#                r.loc[i,\"y\"] = 1 \n",
    "#     else: \n",
    "#                r.loc[i,\"y\"] = 0 \n",
    "print(sens['sex'].value_counts())\n",
    "print(sens['race'].value_counts()) \n",
    "\n",
    "#print(sens['19'].value_counts())\n",
    "sens1=pd.get_dummies(sens, columns=['sex','race'], prefix =['s','r'])\n",
    "sensitive=sens1.T\n",
    "print(sensitive) \n",
    "\n",
    "\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Beta equals\n",
    "beta=[[0.4973821989528796, 0.4954128440366973, 0.49795918367346936, 0.4909090909090909],\n",
    "[0.5968586387434555, 0.6055045871559633, 0.6, 0.6],\n",
    "[0.6963350785340314, 0.6972477064220184, 0.6979591836734694, 0.6909090909090909],\n",
    "[0.7486910994764397, 0.7431192660550459, 0.746938775510204, 0.7454545454545455],\n",
    "[0.7958115183246073, 0.8073394495412844, 0.8, 0.8],\n",
    "[0.837696335078534, 0.8348623853211009, 0.8367346938775511, 0.8363636363636363],\n",
    "[0.8795811518324608, 0.8715596330275229, 0.8775510204081632, 0.8727272727272727]]\n",
    "precision=[0.8842105263157894, 0.7407407407407407, 0.860655737704918, 0.7037037037037037],\n",
    "[0.8596491228070176, 0.7424242424242424, 0.8435374149659864, 0.696969696969697],\n",
    "[0.8571428571428571, 0.7105263157894737, 0.8304093567251462, 0.6842105263157895],\n",
    "[0.8321678321678322, 0.6790123456790124, 0.8087431693989071, 0.6341463414634146],\n",
    "[0.8223684210526315, 0.6590909090909091, 0.7908163265306123, 0.6363636363636364],\n",
    "[0.80625, 0.6483516483516484, 0.7804878048780488, 0.6086956521739131],\n",
    "[0.7916666666666666, 0.631578947368421, 0.7627906976744186, 0.6041666666666666]]\n",
    "\n",
    "recall=[[0.6, 0.6349206349206349, 0.6069364161849711, 0.6333333333333333],\n",
    "[0.7, 0.7777777777777778, 0.7167630057803468, 0.7666666666666667],\n",
    "[0.8142857142857143, 0.8571428571428571, 0.8208092485549133, 0.8666666666666667],\n",
    "[0.85, 0.873015873015873, 0.8554913294797688, 0.8666666666666667],\n",
    "[0.8928571428571429, 0.9206349206349206, 0.8959537572254336, 0.9333333333333333],\n",
    "[0.9214285714285714, 0.9365079365079365, 0.9248554913294798, 0.9333333333333333],\n",
    "[0.95, 0.9523809523809523, 0.9479768786127167, 0.9666666666666667]]\n",
    "\n",
    "accu[0.6533,0.7033,0.7466, 0.7366,0.7433,0.74,0.7333]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bilal\n",
    "'''\n",
    "gamma=[ [0.8324 ,0.688  ,0.8204 ,0.6   ],\n",
    "       [0.8429 ,0.7339 ,0.7918 ,0.8545]\n",
    " [0.8376 ,0.7339 ,0.8163 ,0.7272],\n",
    " [0.8272 ,0.7431 ,0.7959 ,0.8   ] ]\n",
    "prec=[[0.8050,0.7333,0.7860,0.7575],\n",
    "[0.7888,0.6875,0.7886,0.6170],\n",
    "[0.8062,0.7125,0.79,0.7],\n",
    "[0.7974,0.6913,0.7897, 0.6363]]\n",
    "\n",
    "rec=[[0.9142,0.8730,0.9132,0.8333],\n",
    "[0.9071,0.8730,0.8843,0.9666],\n",
    "[0.9214,0.9047,0.9132,0.9333],\n",
    "[0.9,0.8888,0.8901,0.9333]]\n",
    "\n",
    "dp=[  0.2324,0.1206, 0.1104 ,0.0841]\n",
    "begin\n",
    "[0.7633,0.7333, 0.7633, 0.74]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yang 1>\n",
    "'''\n",
    "------------------Results  start----------------------------\n",
    "prec=[[0.7995, 0.8145, 0.8151, 0.7380],\n",
    "[0.8113, 0.6913, 0.7783, 0.7297],\n",
    "[0.7914, 0.6746, 0.785, 0.6086],\n",
    "[0.7914, 0.6627, 0.7772, 0.6170]]\n",
    "\n",
    "rec=[[0.9108, 0.8913, 0.9304, 0.775],\n",
    "[0.9214, 0.8888, 0.9132, 0.9],\n",
    "[0.9214, 0.8888, 0.9075, 0.9333],\n",
    "[0.9214, 0.9047, 0.9075, 0.9666]]\n",
    "\n",
    "beta=[[0.8196, 0.7512, 0.8424, 0.6222],\n",
    "[0.8324, 0.7431, 0.8285, 0.6727],\n",
    "[0.8534, 0.7614, 0.8163, 0.8363],\n",
    "[0.8534, 0.7889, 0.8244, 0.8545]]\n",
    "\n",
    "acc=[ 0.7757,0.7566,0.7366,0.7333]\n",
    "\n",
    "dp=[0.2202,0.1597,0.0919,0.0655]\n",
    "'''\n",
    "prec=[[0.8152, 0.6867, 0.7766, 0.7352],\n",
    "[0.8164, 0.7, 0.7871, 0.7222],\n",
    "[0.7962, 0.6829, 0.7889, 0.6222],\n",
    "[0.7962, 0.6705, 0.7910, 0.5869]]\n",
    "rec=[[0.9142, 0.9047, 0.9248, 0.8333],\n",
    "[0.9214, 0.8888, 0.9190, 0.8666],\n",
    "[0.9214, 0.8888, 0.9075, 0.9333],\n",
    "[0.9214, 0.9047, 0.9190, 0.9]]\n",
    "\n",
    "acc`=[0.7566,0.7633,0.7433,0.74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agarwal\n",
    "'''\n",
    "gamma= [\n",
    "[0.82199,0.715596, 0.808163,0.672727],\n",
    "[0.795812,0.761468,0.804082,0.690909],\n",
    "    [0.816754,0.779817,0.820408,0.727273]\n",
    "[0.816754,0.752294,0.808163,0.727273]]\n",
    "acc=[0.7466,0.74,0.74,0.75]\n",
    "\n",
    "prec= [\n",
    "    [0.80254, 0.70512, 0.79292, 0.64864],\n",
    "    [0.81578,0.67469,0.79187,0.63157] ,\n",
    "[0.80769, 0.67058, 0.78606,0.625],\n",
    "[0.81410,0.68292,0.78787,0.675]]\n",
    "\n",
    "rec= [[0.9,0.87301,0.90751,0.8],\n",
    "      [0.88571,0.88888,0.90173,0.8],\n",
    "      [0.9,0.90476,0.91329,0.83333]\n",
    "[0.90714,0.88888,0.90173,0.9]]\n",
    "dp=[0.1492,0.1131,0.0932 ,0.0895  ]    \n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padala\n",
    "'''\n",
    "\n",
    "acc=[0.7166,0.7333,0.7266,0.69]\n",
    "dp= [0.1573,0.1153,0.0867,0.0396]\n",
    "\n",
    "prec=[[ 0.8275,0.6455, 0.7842, 0.6470],\n",
    "[0.8051,0.6883,  0.7823, 0.6842],\n",
    " [0.8157, 0.6582, 0.7864, 0.6410],\n",
    "[0.8201, 0.64, 0.7897, 0.6052]]\n",
    "\n",
    "rec=[[0.8571, 0.8095, 0.8612, 0.7333],\n",
    "[0.8857, 0.8412, 0.8728, 0.8666],\n",
    "[0.8857, 0.8253, 0.8728, 0.8333],\n",
    " [0.8142, 0.7619, 0.8034, 0.7666]]\n",
    "\n",
    "gamma=[[0.7591, 0.7247, 0.7755, 0.6181],\n",
    "[0.8062, 0.7064, 0.7877, 0.6909],\n",
    "[0.7958, 0.7247, 0.7836, 0.7090],\n",
    "[0.7277, 0.6880, 0.7183, 0.6909]]\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "individual acceptance rates\n",
    "prec=[[0.8472, 0.7179, 0.8095, 0.7575],\n",
    "[0.8169, 0.7236, 0.8020, 0.7027],\n",
    "[0.8211, 0.7051, 0.8010, 0.6842],\n",
    "[0.8478, 0.7162, 0.8228, 0.7027]]\n",
    "rec=[[0.8714, 0.8888, 0.8843, 0.8333],\n",
    "[0.8928, 0.8730, 0.8901, 0.8666],\n",
    "[0.8857, 0.8730, 0.8843, 0.8666],\n",
    "[0.8357, 0.8412, 0.8323, 0.8666]]\n",
    "\n",
    "dp=[0.1714,0.1283,0.0996,0.0497]\n",
    "\n",
    "acc=[0.77,0.76 0.7533,0.75]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
