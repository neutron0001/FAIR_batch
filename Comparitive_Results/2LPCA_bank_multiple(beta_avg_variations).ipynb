{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Notes\n",
    "\n",
    "#1.Run cells in the code from top to down smoothly to see its working\n",
    "#2. Read The COMMENTS and change parameter accordinly\n",
    "#\n",
    "\n",
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Import the three supervised learning models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA    \n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from random import *\n",
    "from subprocess import check_output\n",
    "def Bank_rf(X,Y):\n",
    "    #Split data into training and test datasets (training will be based on 70% of data)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0,shuffle=True) \n",
    "    #test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\n",
    "    print('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "    \n",
    "    #Scaling data\n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    #sc = StandardScaler(with_mean=False)\n",
    "    \n",
    "    \n",
    "    #sc.fit(X_train)\n",
    "    #X_train_std = sc.transform(X_train)\n",
    "    #X_test_std = sc.transform(X_test)\n",
    "\n",
    "    #X_train_std and X_test_std are the scaled datasets to be used in algorithms\n",
    "\n",
    "    #Applying SVC (Support Vector Classification)\n",
    "#     from sklearn.svm import SVC\n",
    "#     svm = SVC(kernel='rbf', random_state=0, gamma=.1, C=10.0,probability=True)\n",
    "    \n",
    "    #rf = RandomForestClassifier(n_estimators=150, max_depth=None, min_samples_split=30, random_state=0)\n",
    "    rf = RandomForestClassifier(n_estimators=180, max_depth=None, min_samples_split=30, random_state=0)\n",
    "    \n",
    "    print(Y_train.dtypes)\n",
    "    Y_train=Y_train.astype('int')\n",
    "    print(Y_train.dtypes)\n",
    "    \n",
    "    print(Y_test.dtypes)\n",
    "    Y_test=Y_test.astype('int')\n",
    "    print(Y_test.dtypes)\n",
    "    \n",
    "    \n",
    "    rf.fit(X_train, Y_train)\n",
    "    print('The accuracy of the RF classifier on training data is {:.2f}'.format(rf.score(X_train, Y_train)))\n",
    "    print('The accuracy of the RF classifier on test data is {:.2f}'.format(rf.score(X_test, Y_test)))\n",
    "    print('####Train prediction Label###############################################')\n",
    "    Y_train_pred=rf.predict(X_train)\n",
    "    #print(y_1)\n",
    "    Y_test_pred=rf.predict(X_test)\n",
    "\n",
    "    print('####Actual Train Label###############################################')\n",
    "\n",
    "\n",
    "    print('####Change to colors###############################################')\n",
    "        \n",
    "    e=rf.predict_proba(X_test)\n",
    "    print(e)\n",
    "    return X_test,Y_test_pred,Y_test,e\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(datax, y_test, y_test_pred,e): \n",
    "        \n",
    "    n=datax.shape[1]\n",
    "    s=datax.shape[0]    \n",
    "    data = np.zeros((s, n), dtype = int)\n",
    "    \n",
    "    r = np.zeros(n, dtype = int) \n",
    "    \n",
    "    for i in range(n):\n",
    "        if int(y_test.iloc[i])==1 :\n",
    "            r[i]=1\n",
    "        else :\n",
    "            r[i]= -1  \n",
    "    \n",
    "    r2 = np.zeros(n, dtype = int) \n",
    "    for i in range(n):\n",
    "        if int(y_test_pred[i])==1 :\n",
    "            r2[i]=1\n",
    "        else :\n",
    "            r2[i]= -1          \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        for i in range(n):\n",
    "                data[j][i]= datax.iloc[j,i]\n",
    "                if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r[i]==1:\n",
    "                         acc1=acc1+1 \n",
    "\n",
    "        print(\"ACTUAL----------total ,accepted, aceeptance rate:\")             \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    print(\"data DP\")\n",
    "    print(DP)\n",
    "    \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        prec=0\n",
    "        reca=0\n",
    "        accur=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "        TP=0\n",
    "        TN=0\n",
    "        for i in range(n):\n",
    "             if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r2[i]==1:\n",
    "                        acc1=acc1+1 \n",
    "                        if r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        else:\n",
    "                             FP=FP+1                \n",
    "                    else:\n",
    "                        if r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        else:\n",
    "                            TN=TN+1    \n",
    "        \n",
    "        print(\"prec reca accuracy for each sens\") \n",
    "        prec= float(TP/(TP+FP))\n",
    "        reca= float(TP/(TP+FN))\n",
    "        accur= float((TP+TN)/a)\n",
    "        print(prec,reca,accur)\n",
    "        \n",
    "        print(\"Random Forest---------total , accepted, aceeptance rate:\")             \n",
    "        \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "#COMMENT   : \"ar\" #############use this predicted RandomForet accptance rate(config) hardcoded or\n",
    "#can pass it through LPCA as lpca() \n",
    "#above ar: Random forest acceptance rate is use as beta_initial/beta (named as beata_initial in paper)\n",
    "######################################RF acceptance rate as beta_initial #############   \n",
    "    beta_initial=ar\n",
    "\n",
    "########################################################################################    \n",
    " \n",
    "    \n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    \n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    \n",
    "    print(\"data DP\")\n",
    "    print(DP) \n",
    "    \n",
    "    print(\"Random Forest accuracy--------------------------\")\n",
    "    prec=0\n",
    "    reca=0\n",
    "    accur=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    for i in range(n):\n",
    "            if r2[i]==1:\n",
    "                acc1=acc1+1 \n",
    "                if r[i]==1:\n",
    "                    TP=TP+1\n",
    "                else:\n",
    "                     FP=FP+1                \n",
    "            else:\n",
    "                if r[i]==1:\n",
    "                     FN=FN+1\n",
    "                else:\n",
    "                     TN=TN+1    \n",
    "\n",
    "        \n",
    "    prec= float(TP/(TP+FP))\n",
    "    reca= float(TP/(TP+FN))\n",
    "    accur= float((TP+TN)/n)\n",
    "    print(prec,reca,accur)\n",
    "    \n",
    "    \n",
    "    \n",
    "    fi= np.zeros(n,dtype=int) \n",
    "#(beta_converge, alpha and epsilon are the parameters of LPCA ) \n",
    "# alpha=1 initial predicted(beta configs) by classifier (RF), at alpha=0 Least DDP config at beta_converge\n",
    "\n",
    "#Example for beta_converge where at all cases alpha=0\n",
    "    \n",
    "    beta_average=[0.04, 0.048,.085,.13,.17,.26,.305]\n",
    "    alpha=[0]\n",
    "\n",
    "    \n",
    "    gamma=np.zeros((7,6),dtype=float)\n",
    "    for i in range(6):\n",
    "        for j in range(7):\n",
    "            gamma[j][i]=beta_average[j]\n",
    "    epsilon=[0.005,.01]        \n",
    "    print(gamma)\n",
    "    \n",
    "    t=0\n",
    "    a=0\n",
    "    #for t in range(gamma.shape[0]):\n",
    "    #for t in range(28):\n",
    "    for eps in epsilon:\n",
    "        for new in range(7):\n",
    "        \n",
    "            for a in alpha:\n",
    "                k=0\n",
    "                u1,u2=min_sum_lpca(data,beta_initial,eps,e,gamma[new],a)\n",
    "                #######################Disp_impact#######################  \n",
    "                print(\"gamma-epsilon-delta\",gamma[new],eps)\n",
    "                accu_all=[]\n",
    "                DP_all=[]\n",
    "                precision_all=[]\n",
    "                recall_all=[]\n",
    "                ar_all=[]\n",
    "                acceptance_rate=np.zeros((7,28),dtype=float)\n",
    "                count=0\n",
    "                print(\"<--------------------------------------->\")\n",
    "                print(\"iteration t\",k)\n",
    "                k=k+1\n",
    "\n",
    "\n",
    "                for i in range(n):\n",
    "                     fi[i] = u1[i]\n",
    "\n",
    "\n",
    "                for j in range(s):\n",
    "                    print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "                    TP=0\n",
    "                    FP=0\n",
    "                    FN=0\n",
    "                    TN=0\n",
    "                    precision=0\n",
    "                    recall=0\n",
    "                    for i in range(n):\n",
    "                         if data[j][i]== 1 :                        \n",
    "                            if fi[i]==1 and r[i]==1:\n",
    "                                TP=TP+1\n",
    "                            if fi[i]==1 and r[i]==-1:\n",
    "                                FP=FP+1 \n",
    "                            if fi[i]==-1 and r[i]==1:\n",
    "                                FN=FN+1\n",
    "                            if fi[i]==-1 and r[i]==-1:\n",
    "                                TN=TN+1    \n",
    "                    if TP+FP !=0:\n",
    "                        precision=float(TP/(TP+FP))\n",
    "                    #print(\"precision\",precision)\n",
    "                    if TP+FN !=0:    \n",
    "                        recall=float(TP/(TP+FN))\n",
    "                    #print(\"recall\",recall)\n",
    "\n",
    "                    precision_all.append(precision)\n",
    "                    recall_all.append(recall)\n",
    "                    #print(\"TP,FP,TN,FN\")\n",
    "                    #print(TP,FP,TN,FN)\n",
    "\n",
    "                    a=0\n",
    "                    b=0\n",
    "                    acc1=0\n",
    "                    acc2=0\n",
    "                    for i in range(n):\n",
    "                            if data[j][i]== 1 :\n",
    "                                a=a+1\n",
    "                                if fi[i]==1:\n",
    "                                     acc1=acc1+1 \n",
    "\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                    a1=float(acc1/a)\n",
    "\n",
    "\n",
    "\n",
    "        #                         print(a)\n",
    "        #                         print(acc1)\n",
    "        #                         print(a1)\n",
    "                    ar_all.append(a1)\n",
    "\n",
    "                count = count+1\n",
    "                maxi=max(ar_all)\n",
    "                mini= min(ar_all)\n",
    "                DP=float(maxi-mini)\n",
    "                print(\"individual acceptance rates\")\n",
    "                print(ar_all)\n",
    "                print(\"individual precision\")\n",
    "                print(precision_all)\n",
    "                print(\"individual recall\")\n",
    "                print(recall_all)\n",
    "                print(\"DP all\")\n",
    "                print(DP)\n",
    "                f_acc=0\n",
    "                for i in range(n):\n",
    "                     if fi[i] == r[i]:\n",
    "                            f_acc=f_acc+1\n",
    "                f_acc_l=float((f_acc*100)/n) \n",
    "\n",
    "        #######################################################################33   \n",
    "\n",
    "        #                         print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                TP=0\n",
    "                FP=0\n",
    "                FN=0\n",
    "                TN=0\n",
    "                precision=0\n",
    "                recall=0\n",
    "                accu=0\n",
    "                for i in range(n):\n",
    "                        if fi[i]==1 and r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        if fi[i]==1 and r[i]==-1:\n",
    "                            FP=FP+1 \n",
    "                        if fi[i]==-1 and r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        if fi[i]==-1 and r[i]==-1:\n",
    "                            TN=TN+1    \n",
    "\n",
    "                if TP+FP!=0:\n",
    "                    precision=float(TP/(TP+FP))\n",
    "                print(\"precision all\",precision)\n",
    "                if TP+FN!=0:\n",
    "                    recall=float(TP/(TP+FN))\n",
    "\n",
    "\n",
    "                print(\"recall all\",recall)\n",
    "                accu=float((TP+TN)/(TP+FN+TN+FP))\n",
    "\n",
    "\n",
    "                print(\"accuracy all\",accu)\n",
    "\n",
    "\n",
    "\n",
    "                print(\"TP,FP,TN,FN\")\n",
    "                print(TP,FP,TN,FN)\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                a1=float(acc1/a)\n",
    "\n",
    "\n",
    "    print(\"<--------------------------------------->\")\n",
    "    alpha_weight=np.arange(0,1.05,.05)        \n",
    "    return accu_all,DP_all,acceptance_rate,alpha_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################LPCA######CODE##### same # beta setting ########\n",
    "               ### fixing alpha to 0 and (varying beta_avg) #########\n",
    "    # In beta_avg config all sub groups have same acceptance rate \n",
    "\n",
    "#data1 is sensitive group in binary matrix sets \n",
    "import time\n",
    "import pulp as p \n",
    "def min_sum_lpca(data1,beta_initial,eps,e,beta,alpha):\n",
    "#here beta_avg=beta[0], same beta is used for ranked weighting\n",
    "    import pulp as p \n",
    "    import math\n",
    "    \n",
    "    m=data1.shape[0]\n",
    "    n=data1.shape[1]\n",
    "    print('dimension of data')\n",
    "    print(m,n)\n",
    "    \n",
    "    ################ sorted result\n",
    "    h1=[]\n",
    "    h2=[]\n",
    "    h3=[]\n",
    "    h4=[]\n",
    "    h5=[]\n",
    "    h6=[]\n",
    "   \n",
    "    key1=[]\n",
    "    key2=[]\n",
    "    key3=[]\n",
    "    key4=[]\n",
    "    key5=[]\n",
    "    key6=[]\n",
    "   \n",
    "    cost=np.zeros(n,dtype=int)\n",
    "    data2=np.zeros((m,n),dtype=int)\n",
    "    for i in range(n):\n",
    "        if data1[0][i]==1:            \n",
    "\n",
    "            h1.append(e[i][1])\n",
    "            key1.append(i)\n",
    "\n",
    "        elif data1[1][i]==1:\n",
    "            h2.append(e[i][1])\n",
    "            key2.append(i)\n",
    "            \n",
    "        if data1[2][i]==1:\n",
    "            h3.append(e[i][1])\n",
    "            key3.append(i)\n",
    "            \n",
    "        elif data1[3][i]==1:\n",
    "            h4.append(e[i][1])\n",
    "            key4.append(i)\n",
    "        elif data1[4][i]==1:\n",
    "            h5.append(e[i][1])\n",
    "            key5.append(i)\n",
    "        elif data1[5][i]==1:\n",
    "            h6.append(e[i][1])\n",
    "            key6.append(i)\n",
    "        \n",
    "\n",
    "    \n",
    "    for i in range(1,len(h1)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h1[j-1]<h1[j]:\n",
    "                index=j\n",
    "                var=h1[j]\n",
    "                h1[j]=h1[j-1]\n",
    "                h1[j-1]=var\n",
    "\n",
    "                var2=key1[j]\n",
    "                key1[j]=key1[j-1]\n",
    "                key1[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "\n",
    "    for i in range(1,len(h2)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h2[j-1]<h2[j]:\n",
    "                index=j\n",
    "                var=h2[j]\n",
    "                h2[j]=h2[j-1]\n",
    "                h2[j-1]=var\n",
    "\n",
    "                var2=key2[j]\n",
    "                key2[j]=key2[j-1]\n",
    "                key2[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h3)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h3[j-1]<h3[j]:\n",
    "                index=j\n",
    "                var=h3[j]\n",
    "                h3[j]=h3[j-1]\n",
    "                h3[j-1]=var\n",
    "\n",
    "                var2=key3[j]\n",
    "                key3[j]=key3[j-1]\n",
    "                key3[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h4)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h4[j-1]<h4[j]:\n",
    "                index=j\n",
    "                var=h4[j]\n",
    "                h4[j]=h4[j-1]\n",
    "                h4[j-1]=var\n",
    "\n",
    "                var2=key4[j]\n",
    "                key4[j]=key4[j-1]\n",
    "                key4[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h5)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h5[j-1]<h5[j]:\n",
    "                index=j\n",
    "                var=h5[j]\n",
    "                h5[j]=h5[j-1]\n",
    "                h5[j-1]=var\n",
    "\n",
    "                var2=key5[j]\n",
    "                key5[j]=key5[j-1]\n",
    "                key5[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "                \n",
    "                \n",
    "    for i in range(1,len(h6)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h6[j-1]<h6[j]:\n",
    "                index=j\n",
    "                var=h6[j]\n",
    "                h6[j]=h6[j-1]\n",
    "                h6[j-1]=var\n",
    "\n",
    "                var2=key6[j]\n",
    "                key6[j]=key6[j-1]\n",
    "                key6[j-1]=var2\n",
    "            else:\n",
    "                break     \n",
    "    #####alpha2 is just another weight influencing parameter for now its neutral with [\"ones\"] vector\n",
    "    alpha2=[1,1,1,1,1,1]            \n",
    "                \n",
    "#####Setup2#################LPCA with ranked sensitive groups with weight equalization         \n",
    "    #LPCA with weighted param\n",
    "    for j in range(len(key1)):    \n",
    "        #data2[0][key1[j]]=((j+1)/((beta[0]*len(key1))*((beta[0]*len(key1))+1)/2)*alpha[0])\n",
    "         \n",
    "        data2[0][key1[j]]=(j+1)*((beta[1]*len(key2))*(beta[0]*len(key1)))*alpha2[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*alpha2[1]\n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)*alpha2[2]              \n",
    "        \n",
    "    for j in range(len(key4)):\n",
    "        data2[3][key4[j]]=(j+1)*((beta[2]*len(key3))*(beta[3]*len(key4)))*alpha2[3]\n",
    "        \n",
    "                             \n",
    "    for j in range(len(key5)):               \n",
    "        data2[4][key5[j]]=(j+1)*((beta[2]*len(key3))*(beta[4]*len(key5)))*alpha2[4]\n",
    "       \n",
    "    for j in range(len(key6)):\n",
    "        data2[5][key6[j]]=(j+1)*((beta[2]*len(key3))*(beta[5]*len(key6)))*alpha2[5]\n",
    "    \n",
    "    #basic2\n",
    "            \n",
    "    '''\n",
    "     \n",
    "    \n",
    "    #LPCA  ranked without weighted param\n",
    "    for j in range(len(key1)):    \n",
    "        #data2[0][key1[j]]=((j+1)/((beta[0]*len(key1))*((beta[0]*len(key1))+1)/2)*alpha[0])\n",
    "         \n",
    "        data2[0][key1[j]]=(j+1)*((beta[0]*len(key1))/(beta[1]*len(key2)))*alpha[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*alpha[1]\n",
    "    \n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)*alpha[2]\n",
    "                         \n",
    "        \n",
    "    for j in range(len(key4)):           \n",
    "        #data2[3][key4[j]]=(j+1)*((beta[2]*len(key3))/(beta[3]*len(key4)))*alpha[3]          \n",
    "        data2[3][key4[j]]=(j+1)*((beta[3]*len(key4))/(beta[2]*len(key3)))*alpha[3]\n",
    "      \n",
    "                             \n",
    "    for j in range(len(key5)):\n",
    "        data2[4][key5[j]]=(j+1)*((beta[4]*len(key5))/(beta[2]*len(key3)))*alpha[4]\n",
    "       \n",
    "           \n",
    "    for j in range(len(key6)):                 \n",
    "        data2[5][key6[j]]=(j+1)*((beta[5]*len(key6))/(beta[2]*len(key3)))*alpha[5]\n",
    "    \n",
    "    '''   \n",
    "#sum up the weighted subgroup rank in cost        \n",
    "    for j in range(n):\n",
    "        summ=0\n",
    "        for i in range(m):\n",
    "       \n",
    "            summ=summ+data2[i][j] \n",
    "        cost[j]=summ\n",
    "        \n",
    "        \n",
    "    ################\n",
    "    \n",
    "    \n",
    "    Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
    "    ###############################Optimization fuction for LPCA###################\n",
    "# beta_avg(convergence point to achieve least DP at alpha=0   \n",
    "# beta_initial (acceptance rate) config obtaind for each group from random forest prediction)   \n",
    "\n",
    "\n",
    "    X=np.zeros(n+m+1,dtype=p.LpVariable)\n",
    "    Y=np.zeros(m,dtype=p.LpVariable)\n",
    "    \n",
    "    sizes=np.zeros(m,dtype=int)\n",
    "  \n",
    "    max_size=0\n",
    "    for i in range(m):\n",
    "        count=0\n",
    "        for j in range(n):\n",
    "            if data1[i][j]==1:\n",
    "                count=count+1 \n",
    "        if count>max_size:\n",
    "            max_size=count\n",
    "        sizes[i]=count\n",
    "    print(sizes)        \n",
    "    \n",
    " ################ As all beta[i]'s are same beta_avg=beta[0] #################   \n",
    "    \n",
    "    beta_avg=beta[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    select_sizes=np.zeros(m,dtype=int)\n",
    "   \n",
    "    size_final=np.zeros(m,dtype=int)\n",
    "\n",
    "    for i in range(m):\n",
    "        var1 = str(n+100+i)\n",
    "        Y[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Continuous')\n",
    "    \n",
    "    for i in range(n):\n",
    "        var1=str(i)       \n",
    "        X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "   \n",
    "    X[n]=p.LpVariable(str(n),lowBound=0,upBound=1,cat='Continuous')  \n",
    "#minimize Objective#\n",
    "    Lp_prob+= p.lpSum([(X[j])*cost[j] for j in range(n)])\n",
    "    \n",
    "\n",
    "    for i in range(2*m):\n",
    "        if i<m:\n",
    "\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) >= (Y[i]-eps)*sizes[i]\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) <= (Y[i]+eps)*sizes[i]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "# for minimum ddp at alpa=0 setup (like paper) change (1- alpha) to (alpha) & (alpha) to (1-alpha)    \n",
    "    for i in range(m):\n",
    "            if beta_initial[i] >= beta_avg:\n",
    "\n",
    "                Lp_prob += Y[i] >= (1-alpha)*beta_initial[i] +alpha*beta_avg\n",
    "                Lp_prob += Y[i] <= (1-alpha)*beta_initial[i] +alpha*beta_avg\n",
    "               \n",
    "            else:\n",
    "                Lp_prob += Y[i] >= (1-alpha)*beta_initial[i] + alpha*beta_avg\n",
    "                Lp_prob += Y[i] <= beta_avg   \n",
    "    '''      \n",
    "\n",
    "\n",
    "    for i in range(m):\n",
    "        Lp_prob += Y[i] >= (alpha)*beta_initial[i] +(1-alpha)*beta_avg\n",
    "        Lp_prob += Y[i] <= (alpha)*beta_initial[i] +(1-alpha)*beta_avg\n",
    "                         \n",
    "   \n",
    "    #####################################\n",
    "    status = Lp_prob.solve()   # Solver \n",
    "    print(p.LpStatus[status]) \n",
    "    print(\"objective is:\")        \n",
    "    print(p.value(Lp_prob.objective))\n",
    "    print(\"discripency is:\") \n",
    "    print(p.value(X[n]))\n",
    "    x=np.zeros(n,dtype=float)\n",
    "\n",
    "   # The solution status \n",
    "    Synth1={}\n",
    "    Synth2={}\n",
    "    # # Printing the final solution \n",
    "    for i in range(n):\n",
    "        if(p.value(X[i])==1):\n",
    "            Synth1[i]=1 \n",
    "            Synth2[i]=-1\n",
    "#             if(data1[2][i]==1):\n",
    "#                 print(\"no\")\n",
    "        else:\n",
    "            Synth1[i]=-1\n",
    "            Synth2[i]=1\n",
    "    Synthu1=Synth1  \n",
    "    Synthu2=Synth2  \n",
    "    \n",
    "              \n",
    "    return Synthu1,Synthu2   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# without accuracy\n",
    "import time\n",
    "# import pulp as p \n",
    "# from random import *\n",
    "data= pd.read_csv('data/bank_train.csv',skipinitialspace=True)\n",
    "\n",
    "print(data['marital'].value_counts())\n",
    "#marital\n",
    "#U=80, M=24928, S=11568, D=4612\n",
    "# m_3, m_0, m_1, m_2\n",
    "#age\n",
    "#>60 and <25= a_1\n",
    "#>=25and <=60 =a_2\n",
    "# print(data.head())\n",
    "# print(data.shape[0],data.shape[1])\n",
    "\n",
    "#sensitive columns name 0='age',2='marital'\n",
    "\n",
    "data_c = data.drop(columns=['age_group','y'])\n",
    "# print(sens)\n",
    "r=data[['y']]\n",
    "\n",
    "X_test,Y_test_pred,Y_test,e = Bank_rf(data_c , r)\n",
    "\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "# Y_test_pred.reset_index()\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "print(X_test)\n",
    "print(Y_test_pred)\n",
    "print(Y_test)\n",
    "sens=X_test[['age','marital']]\n",
    "print(sens)\n",
    "p=sens.shape[0]\n",
    "\n",
    "# for i in range(0,p):  \n",
    "#     if r.loc[i,'y'] == 1 :\n",
    "#                r.loc[i,\"y\"] = 1 \n",
    "#     else: \n",
    "#                r.loc[i,\"y\"] = 0 \n",
    "            \n",
    "for i in range(0,p):\n",
    "    if sens.loc[i,'age'] > 60 or sens.loc[i,'age'] < 25 :\n",
    "               sens.loc[i,'age'] = 1 \n",
    "    else :\n",
    "               sens.loc[i,'age'] = 2  \n",
    "            \n",
    "sens1 = pd.get_dummies(sens, columns=['age','marital'], prefix =['a','m'])\n",
    "print(sens1.head())\n",
    "sensitive = sens1.T\n",
    "\n",
    "print(sensitive)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################################Run and observe the results at varius parameters\n",
    "# Run for getting the Final output\n",
    "\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )\n",
    "\n",
    "# Observe group wise acc_rate,precision, recall, DP\n",
    "#eps=.005 ensures DDP<.01  , configuration within [beta_avg+eps ,beta_avg-eps] \n",
    "#see the the outputs at alpha =0 beta_avg varies\n",
    "#SEE Optimal group wise acceptance rate/config (individual beta)\n",
    "\n",
    "#optimal before iterations tells the valid configuration on parameters.\n",
    "#undefined ,means invalid configuration on given parameters\n",
    "       #if undefined increase eps from .005 to .01 \n",
    "#gamma is beta_avg in vector form    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec=[\n",
    "[0.8846153846153846, 0.7263922518159807, 0.7323420074349443, 0.7416666666666667, 0.7551020408163265, 0.0],\n",
    "[0.8666666666666667, 0.7120315581854043, 0.7051671732522796, 0.7414965986394558, 0.7666666666666667, 0.0],\n",
    "\n",
    "[0.8269230769230769, 0.6638388123011665, 0.6442622950819672, 0.7216117216117216, 0.7181818181818181, 0.0],\n",
    "\n",
    "[0.782051282051282, 0.5756958587915818, 0.5431578947368421, 0.6737089201877934, 0.6104651162790697, 0.3333333333333333],\n",
    "\n",
    "[0.7029702970297029, 0.49382716049382713, 0.4528753993610224, 0.6174377224199288, 0.5110132158590308, 0.25],\n",
    "\n",
    "[0.5882352941176471, 0.36950732356857524, 0.3245341614906832, 0.5092165898617511, 0.36752136752136755, 0.3333333333333333],\n",
    "[0.5642458100558659, 0.322863610639502, 0.2794894366197183, 0.4632713026444662, 0.3196125907990315, 0.2857142857142857]\n",
    "]\n",
    "rec=[\n",
    "[0.107981220657277, 0.2553191489361702, 0.26336898395721925, 0.1790744466800805, 0.2642857142857143, 0.0],\n",
    "[0.12206572769953052, 0.3072340425531915, 0.31016042780748665, 0.2193158953722334, 0.32857142857142857, 0.333],\n",
    "[0.20187793427230047, 0.5327659574468085, 0.5254010695187166, 0.3963782696177062, 0.5642857142857143, 0.333],\n",
    "[0.2863849765258216, 0.7217021276595744, 0.6898395721925134, 0.5774647887323944, 0.75, 0.3333333333333333],\n",
    "[0.3333333333333333, 0.8170212765957446, 0.7580213903743316, 0.6981891348088531, 0.8285714285714286, 0.3333333333333333],\n",
    "[0.4225352112676056, 0.9446808510638298, 0.8382352941176471, 0.8893360160965795, 0.9214285714285714, 0.6666666666666666],\n",
    "[0.47417840375586856, 0.971063829787234, 0.8489304812834224, 0.9517102615694165, 0.9428571428571428, 0.6666666666666666]\n",
    "]\n",
    "\n",
    "\n",
    "accu=[0.9044,0.9068,0.9154,0.9092,0.8890,0.82641, 0.7882]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beta=[0.04, 0.048,.085,.13,.17,.26,.305]\n",
    "\n",
    "beta_p=[]\n",
    "beta_r=[]\n",
    "beta_check=[0.27979274611398963, 0.0718288334182374, 0.07343212490076739, 0.10702734489855925, 0.06472727272727273, 0.043]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#P R P R P R R \n",
    "#0.7251,0.8101 0.5470,0.7329 0.6311,0.7389 0.5308,0.6172 0.6764,0.4776  0.4666,0.5333  0.2666,0.7333\n",
    "#      \n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "#p=[]\n",
    "#r=[0,2,4,1,3,5,6]\n",
    "\n",
    "dp_list=[]\n",
    "sizes=[579, 11778, 7558, 3401,1375,23]\n",
    "for i in range(7):\n",
    "    weight_prec=0\n",
    "    weight_p=0\n",
    "    weight_rec=0\n",
    "    weight_r=0\n",
    "    cnt1=0\n",
    "    cnt2=0\n",
    "    for j in range(6):\n",
    "        #print(j)\n",
    "              \n",
    "        \n",
    "        if beta[i] <=beta_check[j]:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            cnt1=1\n",
    "        else:  \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "            cnt2=1\n",
    "    if cnt1==1:\n",
    "        wp=weight_prec/weight_p\n",
    "        weighted_precision.append(wp)\n",
    "        beta_p.append(beta[i])\n",
    "\n",
    "    if cnt2==1: \n",
    "        wr=weight_rec/weight_r\n",
    "        weighted_recall.append(wr) \n",
    "        beta_r.append(beta[i])\n",
    "            \n",
    "   \n",
    "    \n",
    "    \n",
    "len1=(len(weighted_precision)) \n",
    "len2=(len(weighted_recall)) \n",
    "    \n",
    "print(weighted_precision, weighted_recall,beta_p,beta_r,len1,len2)\n",
    "'''\n",
    "[0.9520125938773091, 0.9403591343418822, 0.8608764055619338],0.8561212064210523, 0.8128034718440105, 0.782912341254576, 0.7358009995069338\n",
    "0.4610404591572201, 0.5551300667433751, 0.5983458776930215, 0.6580824258162613,0.6476895465902177, 0.7070185074558867, 0.754950250161771\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "np.set_printoptions(precision=4)  # For compact display.\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "\n",
    "'''\n",
    "y1=savgol_filter(a, 6, 2)\n",
    "y2=savgol_filter(b, 6, 2)\n",
    "y3=savgol_filter(c, 6, 2)\n",
    "y4=savgol_filter(d, 6, 2)\n",
    "y5=savgol_filter(e, 6, 2)\n",
    "y6=savgol_filter(f, 6, 2)\n",
    "y7=savgol_filter(g, 6, 2)\n",
    "'''\n",
    "\n",
    "\n",
    "# weighted_precision=[0.830695184472319, 0.7765088634340674, 0.726584109782975, 0.6960740636536334, 0.6921310404756075, 0.6867123925160906, 0.6992683564149464, 0.7062073789123974, 0.6534644405653471, 0.6531048465897986, 0.6302838057538701, 0.6095003833375926, 0.3, 0.2727272727272727] \n",
    "#weighted_recall=[0.4428571428571429, 0.4585581756170134, 0.561328585190352, 0.7626389133798003, 0.7821640114752402, 0.8615867507139823, 0.8950323721572476, 0.906607234460588, 0.9287836428087861, 0.9299842102801053, 0.9367495159330902] \n",
    "beta1=beta_p\n",
    "beta2=beta_r\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "'''\n",
    "accu=[0.8979,0.9018,0.9083,0.91284,0.9130,0.9154,0.9034,0.8979,0.8754,0.8532,0.8458,0.8259,0.8138,0.7953]\n",
    "\n",
    "\n",
    "weighted_precision=[0.9520125938773091, 0.9403591343418822, 0.8608764055619338,0.8561212064210523, 0.8128034718440105, 0.782912341254576, 0.7358009995069338]\n",
    "weighted_recall=[0.4610404591572201, 0.5551300667433751, 0.5983458776930215, 0.6580824258162613,0.6476895465902177, 0.7070185074558867, 0.754950250161771]\n",
    "beta1=[.02,.025,.05,.1,.14,.16,.2]\n",
    "beta2=[.1,.14,.16,.2,.25,.3,.35]\n",
    "\n",
    "accu=[ 0.7774,0.7813,0.7954,0.8157,0.82442,0.8245,0.8216,0.8146,0.7954,0.7699]\n",
    "'''\n",
    "ax.plot(beta1,weighted_precision,label='Weighted Precision',color='blue',marker='^',linestyle='--')  \n",
    "ax.plot(beta2,weighted_recall,label='Weighted Recall',color='cyan',marker='^',linestyle='--')\n",
    "ax.plot(beta,accu,label=' Accuracy',color='red',marker='^',linestyle='--')\n",
    "#ax.vlines(y=[.1992], ymin=[0], ymax=[1], colors='purple', linestyles='--', lw=2, label='PRedict avg. acc.')\n",
    "#plt.axvline(.1992, color='green', linestyle='--')\n",
    "#plt.axvline(.10, color='orange', linestyle='--')\n",
    "#plt.axvline(.20, color='orange', linestyle='--')\n",
    "plt.axvline(0.043, color='orange', linestyle='--')\n",
    "plt.axvline(0.06472, color='orange', linestyle='--')\n",
    "plt.axvline(0.07182, color='orange', linestyle='--')\n",
    "plt.axvline(0.07343, color='orange', linestyle='--')\n",
    "\n",
    "plt.axvline( 0.1070, color='orange', linestyle='--')\n",
    "plt.axvline(0.2797, color='orange', linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('')\n",
    "ax.set_xlabel('Acceptance Rate')\n",
    "ax.set_ylabel('Performance Metrics') \n",
    "# ax.set_ylabel('% in +ve class (Acceptance Rate)') \n",
    "\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.30), shadow=True, ncol=10)\n",
    "plt.show() \n",
    "fig.savefig('a2.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu=[0.8979,0.9018,0.9083,0.91284,0.9130,0.9154,0.9034,0.8979,0.8754,0.8532,0.8458,0.8259,0.8138,0.7953]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.2576267506242536, 0.07575757575757576, 0.2111662955985614, 0.08934426229508197, 0.2698019801980198, 0.1, 0.034782608695652174]\n",
    "# [0.27979274611398963, 0.0718288334182374, 0.07343212490076739, 0.10702734489855925, 0.06472727272727273, 0.0]\n",
    "# [0.26282051282051283, 0.58125, 0.5182539682539683, 0.21296296296296297]\n",
    "# [0.8795811518324608, 0.7614678899082569, 0.8571428571428571, 0.7454545454545455]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec=[[0.9285714285714286, 0.8248587570621468, 0.8508771929824561, 0.8035714285714286, 0.8095238095238095, 0],\n",
    "[0.9, 0.7694915254237288, 0.8095238095238095, 0.7252747252747253, 0.7428571428571429, 0],\n",
    "[0.8387096774193549, 0.7212806026365348, 0.7272727272727273, 0.7295597484276729, 0.7258064516129032, 0],\n",
    "[0.8095238095238095, 0.6917900403768507, 0.689727463312369, 0.7104072398190046, 0.7126436781609196, 0],\n",
    "[0.7954545454545454, 0.6822784810126582, 0.6804733727810651, 0.7051282051282052, 0.6881720430107527, 0],\n",
    "[0.7454545454545455, 0.6576846307385229, 0.6547433903576982, 0.6813559322033899, 0.6666666666666666, 0.0],\n",
    "[0.7191011235955056, 0.5456674473067916, 0.541058394160584, 0.6048387096774194, 0.51, 0.2],\n",
    "[0.7263157894736842, 0.523001095290252, 0.5179180887372014, 0.590566037735849, 0.48130841121495327, 0.2],\n",
    "[0.6694915254237288, 0.4584240313452329, 0.4497964721845319, 0.5391566265060241, 0.4052044609665427, 0.25],\n",
    "[0.6691176470588235, 0.4111655978875896, 0.4062316284538507, 0.4895833333333333, 0.36129032258064514, 0.25],\n",
    "[0.6453900709219859, 0.39920520231213874, 0.39335959482273497, 0.47875, 0.345679012345679, 0.25],\n",
    "[0.6217948717948718, 0.3692458374142997, 0.3652085452695829, 0.4463276836158192, 0.3128491620111732, 0.3],\n",
    "[0.6121212121212121, 0.35350416795307193, 0.3496873496873497, 0.43162393162393164, 0.2955145118733509, 0.3],\n",
    "[0.5795454545454546, 0.33179856115107914, 0.32869955156950675, 0.4043824701195219, 0.27832512315270935, 0.2727272727272727]]\n",
    "rec=[[0.06103286384976526, 0.12425531914893617, 0.12967914438502673, 0.09054325955734406, 0.12142857142857143, 0.0],\n",
    "[0.08450704225352113, 0.19319148936170213, 0.20454545454545456, 0.13279678068410464, 0.18571428571428572, 0.0],\n",
    "[0.12206572769953052, 0.32595744680851063, 0.3315508021390374, 0.23340040241448692, 0.32142857142857145, 0.0],\n",
    "[0.1596244131455399, 0.4374468085106383, 0.43983957219251335, 0.3158953722334004, 0.44285714285714284, 0.0],\n",
    "[0.1643192488262911, 0.45872340425531916, 0.4612299465240642, 0.3319919517102616, 0.45714285714285713, 0.0],\n",
    "[0.19248826291079812, 0.5608510638297872, 0.5628342245989305, 0.4044265593561368, 0.5571428571428572, 0.0],\n",
    "[0.3004694835680751, 0.7931914893617021, 0.7927807486631016, 0.6036217303822937, 0.7285714285714285, 0.3333333333333333],\n",
    "[0.323943661971831, 0.8127659574468085, 0.8114973262032086, 0.6297786720321932, 0.7357142857142858, 0.3333333333333333],\n",
    "[0.37089201877934275, 0.8961702127659574, 0.8863636363636364, 0.7203219315895373, 0.7785714285714286, 0.6666666666666666],\n",
    "[0.4272300469483568, 0.9276595744680851, 0.9237967914438503, 0.7565392354124748, 0.8, 0.6666666666666666],\n",
    "[0.4272300469483568, 0.9404255319148936, 0.9344919786096256, 0.7706237424547284, 0.8, 0.6666666666666666],\n",
    "[0.45539906103286387, 0.9625531914893617, 0.9598930481283422, 0.7947686116700201, 0.8, 1.0],\n",
    "[0.47417840375586856, 0.9744680851063829, 0.9719251336898396, 0.8128772635814889, 0.8, 1.0],\n",
    "[0.4788732394366197, 0.9812765957446808, 0.9799465240641712, 0.8169014084507042, 0.8071428571428572, 1.0]]\n",
    "accu=[0.8979,0.9018,0.9083,0.91284,0.9130,0.9154,0.9034,0.8979,0.8754,0.8532,0.8458,0.8259,0.8138,0.7953]\n",
    "\n",
    "beta=[ .02,.03,0.05, .068,.072, 0.09,  0.15, 0.16, 0.2, 0.23,0.24,.265, 0.28,.30]\n",
    "\n",
    "beta_p=[]\n",
    "beta_r=[]\n",
    "beta_check=[0.27979274611398963, 0.0718288334182374, 0.07343212490076739, 0.10702734489855925, 0.06472727272727273, 0.43]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#P R P R P R R \n",
    "#0.7251,0.8101 0.5470,0.7329 0.6311,0.7389 0.5308,0.6172 0.6764,0.4776  0.4666,0.5333  0.2666,0.7333\n",
    "#      \n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "#p=[]\n",
    "#r=[0,2,4,1,3,5,6]\n",
    "\n",
    "dp_list=[]\n",
    "sizes=[579, 11778, 7558, 3401,1375,23]\n",
    "for i in range(14):\n",
    "    weight_prec=0\n",
    "    weight_p=0\n",
    "    weight_rec=0\n",
    "    weight_r=0\n",
    "    cnt1=0\n",
    "    cnt2=0\n",
    "    for j in range(6):\n",
    "        #print(j)\n",
    "              \n",
    "        \n",
    "        if beta[i] <=beta_check[j]:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            cnt1=1\n",
    "        else:  \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "            cnt2=1\n",
    "    if cnt1==1:\n",
    "        wp=weight_prec/weight_p\n",
    "        weighted_precision.append(wp)\n",
    "        beta_p.append(beta[i])\n",
    "\n",
    "    if cnt2==1: \n",
    "        wr=weight_rec/weight_r\n",
    "        weighted_recall.append(wr) \n",
    "        beta_r.append(beta[i])\n",
    "            \n",
    "   \n",
    "    \n",
    "    \n",
    "len1=(len(weighted_precision)) \n",
    "len2=(len(weighted_recall)) \n",
    "    \n",
    "print(weighted_precision, weighted_recall,beta_p,beta_r,len1,len2)\n",
    "'''\n",
    "[0.9520125938773091, 0.9403591343418822, 0.8608764055619338],0.8561212064210523, 0.8128034718440105, 0.782912341254576, 0.7358009995069338\n",
    "0.4610404591572201, 0.5551300667433751, 0.5983458776930215, 0.6580824258162613,0.6476895465902177, 0.7070185074558867, 0.754950250161771\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "np.set_printoptions(precision=4)  # For compact display.\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "\n",
    "'''\n",
    "y1=savgol_filter(a, 6, 2)\n",
    "y2=savgol_filter(b, 6, 2)\n",
    "y3=savgol_filter(c, 6, 2)\n",
    "y4=savgol_filter(d, 6, 2)\n",
    "y5=savgol_filter(e, 6, 2)\n",
    "y6=savgol_filter(f, 6, 2)\n",
    "y7=savgol_filter(g, 6, 2)\n",
    "'''\n",
    "\n",
    "\n",
    "# weighted_precision=[0.830695184472319, 0.7765088634340674, 0.726584109782975, 0.6960740636536334, 0.6921310404756075, 0.6867123925160906, 0.6992683564149464, 0.7062073789123974, 0.6534644405653471, 0.6531048465897986, 0.6302838057538701, 0.6095003833375926, 0.3, 0.2727272727272727] \n",
    "#weighted_recall=[0.4428571428571429, 0.4585581756170134, 0.561328585190352, 0.7626389133798003, 0.7821640114752402, 0.8615867507139823, 0.8950323721572476, 0.906607234460588, 0.9287836428087861, 0.9299842102801053, 0.9367495159330902] \n",
    "beta1=beta_p\n",
    "beta2=beta_r\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "#accu=[0.8979,0.9018,0.9083,0.91284,0.9130,0.9154,0.9034,0.8979,0.8754,0.8532,0.8458,0.8259,0.8138,0.7953]\n",
    "\n",
    "'''\n",
    "weighted_precision=[0.9520125938773091, 0.9403591343418822, 0.8608764055619338,0.8561212064210523, 0.8128034718440105, 0.782912341254576, 0.7358009995069338]\n",
    "weighted_recall=[0.4610404591572201, 0.5551300667433751, 0.5983458776930215, 0.6580824258162613,0.6476895465902177, 0.7070185074558867, 0.754950250161771]\n",
    "beta1=[.02,.025,.05,.1,.14,.16,.2]\n",
    "beta2=[.1,.14,.16,.2,.25,.3,.35]\n",
    "\n",
    "accu=[ 0.7774,0.7813,0.7954,0.8157,0.82442,0.8245,0.8216,0.8146,0.7954,0.7699]\n",
    "'''\n",
    "ax.plot(beta1,weighted_precision,label='Weighted Precision',color='blue',marker='^',linestyle='--')  \n",
    "ax.plot(beta2,weighted_recall,label='Weighted Recall',color='cyan',marker='^',linestyle='--')\n",
    "ax.plot(beta,accu,label=' Accuracy',color='red',marker='^',linestyle='--')\n",
    "#ax.vlines(y=[.1992], ymin=[0], ymax=[1], colors='purple', linestyles='--', lw=2, label='PRedict avg. acc.')\n",
    "#plt.axvline(.1992, color='green', linestyle='--')\n",
    "#plt.axvline(.10, color='orange', linestyle='--')\n",
    "#plt.axvline(.20, color='orange', linestyle='--')\n",
    "plt.axvline(0.043, color='orange', linestyle='--')\n",
    "plt.axvline(0.06472, color='orange', linestyle='--')\n",
    "plt.axvline(0.07182, color='orange', linestyle='--')\n",
    "plt.axvline(0.07343, color='orange', linestyle='--')\n",
    "\n",
    "plt.axvline( 0.1070, color='orange', linestyle='--')\n",
    "plt.axvline(0.2797, color='orange', linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('')\n",
    "ax.set_xlabel('Acceptance Rate')\n",
    "ax.set_ylabel('Performance Metrics') \n",
    "# ax.set_ylabel('% in +ve class (Acceptance Rate)') \n",
    "\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.30), shadow=True, ncol=10)\n",
    "plt.show() \n",
    "fig.savefig('a2.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec=[[0.9285714285714286, 0.8248587570621468, 0.8508771929824561, 0.8035714285714286, 0.8095238095238095, 0],\n",
    "[0.9, 0.7694915254237288, 0.8095238095238095, 0.7252747252747253, 0.7428571428571429, 0],\n",
    "[0.8387096774193549, 0.7212806026365348, 0.7272727272727273, 0.7295597484276729, 0.7258064516129032, 0],\n",
    "[0.8095238095238095, 0.6917900403768507, 0.689727463312369, 0.7104072398190046, 0.7126436781609196, 0],\n",
    "[0.7954545454545454, 0.6822784810126582, 0.6804733727810651, 0.7051282051282052, 0.6881720430107527, 0],\n",
    "[0.7454545454545455, 0.6576846307385229, 0.6547433903576982, 0.6813559322033899, 0.6666666666666666, 0.0],\n",
    "[0.7191011235955056, 0.5456674473067916, 0.541058394160584, 0.6048387096774194, 0.51, 0.2],\n",
    "[0.7263157894736842, 0.523001095290252, 0.5179180887372014, 0.590566037735849, 0.48130841121495327, 0.2],\n",
    "[0.6694915254237288, 0.4584240313452329, 0.4497964721845319, 0.5391566265060241, 0.4052044609665427, 0.25],\n",
    "[0.6691176470588235, 0.4111655978875896, 0.4062316284538507, 0.4895833333333333, 0.36129032258064514, 0.25],\n",
    "[0.6453900709219859, 0.39920520231213874, 0.39335959482273497, 0.47875, 0.345679012345679, 0.25],\n",
    "[0.6217948717948718, 0.3692458374142997, 0.3652085452695829, 0.4463276836158192, 0.3128491620111732, 0.3],\n",
    "[0.6121212121212121, 0.35350416795307193, 0.3496873496873497, 0.43162393162393164, 0.2955145118733509, 0.3],\n",
    "[0.5795454545454546, 0.33179856115107914, 0.32869955156950675, 0.4043824701195219, 0.27832512315270935, 0.2727272727272727]]\n",
    "rec=[[0.06103286384976526, 0.12425531914893617, 0.12967914438502673, 0.09054325955734406, 0.12142857142857143, 0.0],\n",
    "[0.08450704225352113, 0.19319148936170213, 0.20454545454545456, 0.13279678068410464, 0.18571428571428572, 0.0],\n",
    "[0.12206572769953052, 0.32595744680851063, 0.3315508021390374, 0.23340040241448692, 0.32142857142857145, 0.0],\n",
    "[0.1596244131455399, 0.4374468085106383, 0.43983957219251335, 0.3158953722334004, 0.44285714285714284, 0.0],\n",
    "[0.1643192488262911, 0.45872340425531916, 0.4612299465240642, 0.3319919517102616, 0.45714285714285713, 0.0],\n",
    "[0.19248826291079812, 0.5608510638297872, 0.5628342245989305, 0.4044265593561368, 0.5571428571428572, 0.0],\n",
    "[0.3004694835680751, 0.7931914893617021, 0.7927807486631016, 0.6036217303822937, 0.7285714285714285, 0.3333333333333333],\n",
    "[0.323943661971831, 0.8127659574468085, 0.8114973262032086, 0.6297786720321932, 0.7357142857142858, 0.3333333333333333],\n",
    "[0.37089201877934275, 0.8961702127659574, 0.8863636363636364, 0.7203219315895373, 0.7785714285714286, 0.6666666666666666],\n",
    "[0.4272300469483568, 0.9276595744680851, 0.9237967914438503, 0.7565392354124748, 0.8, 0.6666666666666666],\n",
    "[0.4272300469483568, 0.9404255319148936, 0.9344919786096256, 0.7706237424547284, 0.8, 0.6666666666666666],\n",
    "[0.45539906103286387, 0.9625531914893617, 0.9598930481283422, 0.7947686116700201, 0.8, 1.0],\n",
    "[0.47417840375586856, 0.9744680851063829, 0.9719251336898396, 0.8128772635814889, 0.8, 1.0],\n",
    "[0.4788732394366197, 0.9812765957446808, 0.9799465240641712, 0.8169014084507042, 0.8071428571428572, 1.0]]\n",
    "accu=[0.8979,0.9018,0.9083,0.91284,0.9130,0.9154,0.9034,0.8979,0.8754,0.8532,0.8458,0.8259,0.8138,0.7953]\n",
    "\n",
    "beta=[ .02,.03,0.05, .068,.072, 0.09,  0.15, 0.16, 0.2, 0.23,0.24,.265, 0.28,.30]\n",
    "\n",
    "beta_p=[]\n",
    "beta_r=[]\n",
    "beta_check=[0.27979274611398963, 0.0718288334182374, 0.07343212490076739, 0.10702734489855925, 0.06472727272727273, 0.43]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#P R P R P R R \n",
    "#0.7251,0.8101 0.5470,0.7329 0.6311,0.7389 0.5308,0.6172 0.6764,0.4776  0.4666,0.5333  0.2666,0.7333\n",
    "#      \n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "#p=[]\n",
    "#r=[0,2,4,1,3,5,6]\n",
    "\n",
    "dp_list=[]\n",
    "sizes=[579, 11778, 7558, 3401,1375,23]\n",
    "for i in range(14):\n",
    "    weight_prec=0\n",
    "    weight_p=0\n",
    "    weight_rec=0\n",
    "    weight_r=0\n",
    "    cnt1=0\n",
    "    cnt2=0\n",
    "    for j in range(6):\n",
    "        #print(j)\n",
    "              \n",
    "        \n",
    "        if beta[i] <=beta_check[j]:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            cnt1=1\n",
    "        else:  \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "            cnt2=1\n",
    "    if cnt1==1:\n",
    "        wp=weight_prec/weight_p\n",
    "        weighted_precision.append(wp)\n",
    "        beta_p.append(beta[i])\n",
    "\n",
    "    if cnt2==1: \n",
    "        wr=weight_rec/weight_r\n",
    "        weighted_recall.append(wr) \n",
    "        beta_r.append(beta[i])\n",
    "            \n",
    "   \n",
    "    \n",
    "    \n",
    "len1=(len(weighted_precision)) \n",
    "len2=(len(weighted_recall)) \n",
    "    \n",
    "print(weighted_precision, weighted_recall,beta_p,beta_r,len1,len2)\n",
    "'''\n",
    "[0.9520125938773091, 0.9403591343418822, 0.8608764055619338],0.8561212064210523, 0.8128034718440105, 0.782912341254576, 0.7358009995069338\n",
    "0.4610404591572201, 0.5551300667433751, 0.5983458776930215, 0.6580824258162613,0.6476895465902177, 0.7070185074558867, 0.754950250161771\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "np.set_printoptions(precision=4)  # For compact display.\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "\n",
    "'''\n",
    "y1=savgol_filter(a, 6, 2)\n",
    "y2=savgol_filter(b, 6, 2)\n",
    "y3=savgol_filter(c, 6, 2)\n",
    "y4=savgol_filter(d, 6, 2)\n",
    "y5=savgol_filter(e, 6, 2)\n",
    "y6=savgol_filter(f, 6, 2)\n",
    "y7=savgol_filter(g, 6, 2)\n",
    "'''\n",
    "# weighted_precision, weighted_recall,beta_p,beta_r\n",
    "# weighted_precision=[0.7765088634340674, 0.726584109782975, 0.6960740636536334, 0.6921310404756075, 0.6867123925160906, 0.6992683564149464, 0.7062073789123974, 0.6534644405653471, 0.6531048465897986, 0.6302838057538701, 0.6095003833375926, 0.3, 0.2727272727272727] \n",
    "# weighted_recall=[0.4428571428571429, 0.4585581756170134, 0.561328585190352, 0.7626389133798003, 0.7821640114752402, 0.8615867507139823, 0.8950323721572476, 0.906607234460588, 0.9287836428087861, 0.9299842102801053, 0.9367495159330902] \n",
    "# beta1=[0.04, 0.05, 0.068, 0.072, 0.09, 0.15, 0.16, 0.2, 0.23, 0.24, 0.265, 0.28, 0.3] \n",
    "# beta2=[0.068, 0.072, 0.09, 0.15, 0.16, 0.2, 0.23, 0.24, 0.265, 0.28, 0.3]\n",
    "beta1=beta_p\n",
    "beta2=beta_r\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# accu=[.9018,0.9083,0.91284,0.9130,0.9154,0.9034,0.8979,0.8754,0.8532,0.8458,0.8259,0.8138,0.7953]\n",
    "\n",
    "'''\n",
    "weighted_precision=[0.9520125938773091, 0.9403591343418822, 0.8608764055619338,0.8561212064210523, 0.8128034718440105, 0.782912341254576, 0.7358009995069338]\n",
    "weighted_recall=[0.4610404591572201, 0.5551300667433751, 0.5983458776930215, 0.6580824258162613,0.6476895465902177, 0.7070185074558867, 0.754950250161771]\n",
    "beta1=[.02,.025,.05,.1,.14,.16,.2]\n",
    "beta2=[.1,.14,.16,.2,.25,.3,.35]\n",
    "\n",
    "accu=[ 0.7774,0.7813,0.7954,0.8157,0.82442,0.8245,0.8216,0.8146,0.7954,0.7699]\n",
    "'''\n",
    "ax.plot(beta1,weighted_precision,label='Weighted Precision',color='blue',marker='^',linestyle='--')  \n",
    "ax.plot(beta2,weighted_recall,label='Weighted Recall',color='cyan',marker='^',linestyle='--')\n",
    "ax.plot(beta,accu,label=' Accuracy',color='red',marker='^',linestyle='--')\n",
    "#ax.vlines(y=[.1992], ymin=[0], ymax=[1], colors='purple', linestyles='--', lw=2, label='PRedict avg. acc.')\n",
    "#plt.axvline(.1992, color='green', linestyle='--')\n",
    "#plt.axvline(.10, color='orange', linestyle='--')\n",
    "#plt.axvline(.20, color='orange', linestyle='--')\n",
    "plt.axvline(0.043, color='orange', linestyle='--')\n",
    "plt.axvline(0.06472, color='orange', linestyle='--')\n",
    "plt.axvline(0.07182, color='orange', linestyle='--')\n",
    "plt.axvline(0.07343, color='orange', linestyle='--')\n",
    "plt.axvline( 0.1070, color='orange', linestyle='--')\n",
    "plt.axvline(0.2797, color='orange', linestyle='--')\n",
    "\n",
    "\n",
    "plt.title('')\n",
    "ax.set_xlabel('Acceptance Rate')\n",
    "ax.set_ylabel('Performance Metrics') \n",
    "# ax.set_ylabel('% in +ve class (Acceptance Rate)') \n",
    "\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.30), shadow=True, ncol=10)\n",
    "plt.show() \n",
    "fig.savefig('a2.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with cv values(.01,.02)\n",
    "\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with cv values(.005)\n",
    "\n",
    "# 0.9003,.0094,.075\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with cv values(.01,.02)(+eps)\n",
    "\n",
    "# 0.9003,.0094,.075\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec=[[0.7570093457943925, 0.7107583774250441, 0.7154046997389034, 0.720164609053498, 0.7446808510638298, 0.0],\n",
    "      [0.8363636363636363, 0.7391304347826086, 0.7570422535211268, 0.7456647398843931, 0.7142857142857143, 0],\n",
    "      [0.7857142857142857, 0.7058823529411765, 0.7054263565891473, 0.7135678391959799, 0.7450980392156863, 0],\n",
    "      [0.9411764705882353, 0.8275862068965517, 0.8598130841121495, 0.796875, 0.85, 0]]\n",
    "      \n",
    "accu=[0.9114,0.9075,0.9094,0.8981]\n",
    "\n",
    "acc_rate=[ [0.18825561312607944, 0.05306503650874512, 0.055570256681661816, 0.0764481034989709, 0.03854545454545454, 0.043478260869565216],\n",
    "[0.09844559585492228, 0.04202750891492613, 0.04247155332098439, 0.055865921787709494, 0.029818181818181817, 0.0],\n",
    "[0.0690846286701209, 0.0554423501443369, 0.05609949722148717, 0.06233460746839165, 0.04145454545454545, 0.0],\n",
    "[0.025906735751295335, 0.019697741552046188, 0.01905265943371262, 0.022640399882387533, 0.01890909090909091, 0.0]]\n",
    "\n",
    "\n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "p=[0,1,2,3,4]\n",
    "r=[]\n",
    "print(np.transpose(acc_rate))\n",
    "weight_prec=0\n",
    "weight_p=0\n",
    "weight_rec=0\n",
    "weight_r=0\n",
    "dp_list=[]\n",
    "     \n",
    "sizes=[579, 11778, 7558, 3401,1375]\n",
    "for i in range(4):\n",
    "    acc_list=[]\n",
    "    for j in range(5):\n",
    "        #print(j)\n",
    "        if j in p:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            #print(j)\n",
    "        if j in r:    \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "        #print(acc_rate[i][j])    \n",
    "        acc_list.append(acc_rate[i][j])\n",
    "    #print(acc_list)\n",
    "    dp=max(acc_list)-min(acc_list)   \n",
    "    dp_list.append(dp)     \n",
    "    wp=weight_prec/weight_p\n",
    "#     wr=weight_rec/weight_r\n",
    "    weighted_precision.append(wp)\n",
    "#     weighted_recall.append(wr)\n",
    "print(weighted_precision, weighted_recall,accu,dp_list)  \n",
    "[0.7164499334477124, 0.7314299158131289, 0.7245723739443731, 0.7527124270098351] \n",
    "[0.9114, 0.9075, 0.9094, 0.8981]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "acc_rate=[[0.2970639032815199, 0.06707420614705382, 0.07105054247155332, 0.10202881505439576, 0.056, 0.043478260869565216],\n",
    "[0.25734024179620035, 0.06911190354898965, 0.0723736438211167, 0.09732431637753602, 0.06036363636363636, 0.08695652173913043],\n",
    "[0.21243523316062177, 0.07123450500933945, 0.07369674517068008, 0.09232578653337253, 0.06472727272727273, 0.08695652173913043],\n",
    "[0.16753022452504318, 0.07327220241127526, 0.07488753638528711, 0.08732725668920906, 0.06909090909090909, 0.08695652173913043],\n",
    "[0.12435233160621761, 0.07539480387162506, 0.07621063773485048, 0.0826227580123493, 0.07345454545454545, 0.08695652173913043],\n",
    "[0.08635578583765112, 0.07751740533197486, 0.07753373908441387, 0.07880035283740076, 0.07781818181818181, 0.08695652173913043]]\n",
    "\n",
    "prec=[[0.7034883720930233, 0.6620253164556962, 0.6741154562383612, 0.6512968299711815, 0.7272727272727273, 0.0],\n",
    "[0.7181208053691275, 0.6511056511056511, 0.6672760511882998, 0.6525679758308157, 0.6746987951807228, 0.0],\n",
    "[0.7154471544715447, 0.6328963051251489, 0.6409335727109515, 0.6560509554140127, 0.6292134831460674, 0.0],\n",
    "[0.6804123711340206, 0.6164542294322132, 0.6201413427561837, 0.6430976430976431, 0.5894736842105263, 0.0],\n",
    "[0.6666666666666666, 0.5968468468468469, 0.5954861111111112, 0.6370106761565836, 0.5544554455445545, 0.0],\n",
    "[0.7058823529411765, 0.5756578947368421, 0.5665529010238908, 0.6417910447761194, 0.5327102803738317, 0.0]]\n",
    "\n",
    "rec=[[0.568075117370892, 0.4451063829787234, 0.4839572192513369, 0.45472837022132795, 0.4, 0.0],\n",
    "[0.5023474178403756, 0.451063829787234, 0.4879679144385027, 0.4346076458752515, 0.4, 0.0],\n",
    "[0.4131455399061033, 0.45191489361702125, 0.4772727272727273, 0.41448692152917505, 0.4, 0.0],\n",
    "[0.30985915492957744, 0.45276595744680853, 0.4692513368983957, 0.3843058350100604, 0.4, 0.0],\n",
    "[0.22535211267605634, 0.451063829787234, 0.4585561497326203, 0.36016096579476864, 0.4, 0.0],\n",
    "[0.16901408450704225, 0.44680851063829785, 0.44385026737967914, 0.3460764587525151, 0.40714285714285714, 0.0]]\n",
    "\n",
    "accu=[0.9140,0.9128,0.9100,0.9067, 0.9035, 0.9005]\n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "p=[0,3]\n",
    "r=[1,2,4]\n",
    "print(np.transpose(acc_rate))\n",
    "weight_prec=0\n",
    "weight_p=0\n",
    "weight_rec=0\n",
    "weight_r=0\n",
    "dp_list=[]\n",
    "     \n",
    "sizes=[579, 11778, 7558, 3401,1375]\n",
    "for i in range(6):\n",
    "    acc_list=[]\n",
    "    for j in range(5):\n",
    "        #print(j)\n",
    "        if j in p:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            #print(j)\n",
    "        if j in r:    \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "        #print(acc_rate[i][j])    \n",
    "        acc_list.append(acc_rate[i][j])\n",
    "    #print(acc_list)\n",
    "    dp=max(acc_list)-min(acc_list)   \n",
    "    dp_list.append(dp)     \n",
    "    wp=weight_prec/weight_p\n",
    "    wr=weight_rec/weight_r\n",
    "    weighted_precision.append(wp)\n",
    "    weighted_recall.append(wr)\n",
    "print(weighted_precision, weighted_recall,accu,dp_list)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#without cv values--part 2\n",
    "\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bilal final( 3 iteration for each->)\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agarwal final\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bilal basic2\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agarwal basic2\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
