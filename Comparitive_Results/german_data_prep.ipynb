{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Import the three supervised learning models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA    \n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from random import *\n",
    "from subprocess import check_output\n",
    "def german_prep(X,Y):\n",
    "    #Split data into training and test datasets (training will be based on 70% of data)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=10,shuffle=True) \n",
    "    #test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\n",
    "    print('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))\n",
    "     \n",
    "        \n",
    "    X1=pd.concat([X_train,Y_train],ignore_index=True,axis=1)\n",
    "    X2=pd.concat([X_test,Y_test],ignore_index=True,axis=1)\n",
    "    df=pd.concat([X1,X2],ignore_index=True)\n",
    "    \n",
    "    print(X_train)\n",
    "    print(df)\n",
    "    compression_opts = dict(method='zip', archive_name='german.csv')  \n",
    "    df.to_csv('german_daata_prep.zip', index=False, compression=compression_opts)     \n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "# import pulp as p \n",
    "# from random import *\n",
    "from sklearn import preprocessing\n",
    "# Add column names to data set\n",
    "\n",
    "data= pd.read_csv('data/german.csv' , skipinitialspace=True)\n",
    "print(data.head())\n",
    "print(data.shape[0],data.shape[1])\n",
    "\n",
    "#sensitive columns name '12'='age','8'='gender/personal_status'  '19'- foreign workers 20'=1(good)/2(bad))\n",
    "\n",
    "# print(sens)\n",
    "r=data[['20']]\n",
    "# print(r)\n",
    "p=data.shape[0]\n",
    "for i in range(0,p):  \n",
    "    if data.loc[i,\"12\"]>25 :\n",
    "               data.loc[i,\"12\"] = 1 \n",
    "    else :\n",
    "               data.loc[i,\"12\"] = 2\n",
    "    if r.loc[i,'20'] == 1 :\n",
    "               r.loc[i,\"20\"] = 1 \n",
    "    else: \n",
    "               r.loc[i,\"20\"] = 0  \n",
    "            \n",
    "print(data['20'].value_counts())            \n",
    "\n",
    "print(data['8'].value_counts())\n",
    "print(data['12'].value_counts())\n",
    "print(data['19'].value_counts())\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "# Initialize a scaler, then apply it to the features\n",
    "'''\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "num_col = dat.dtypes[dat.dtypes != 'object'].index\n",
    "features_log_minmax_transform = pd.DataFrame(data = dat)\n",
    "features_log_minmax_transform[num_col] = scaler.fit_transform(features_log_minmax_transform[num_col])\n",
    "\n",
    "\n",
    "display(features_log_minmax_transform.head())\n",
    "'''\n",
    "\n",
    "# sens=DATA[['sex','race']]\n",
    "#Data_c = pd.get_dummies(features_log_minmax_transform, columns=['sex','race','workclass','education','marital-status','occupation','relationship','native-country'], prefix =['s','r','work','edu','ms','occ','rls','nc'])\n",
    "m=data.shape[1]\n",
    "data_c1=data.iloc[:,0:m-1]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data_c2 = min_max_scaler.fit_transform(data_c1)\n",
    "data_c = pd.DataFrame(data_c2,columns=data_c1.columns)\n",
    "print(data_c)\n",
    "\n",
    "\n",
    "\n",
    "german_prep(data_c , r)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
