{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Notes\n",
    "\n",
    "#1.Run cells in the code from top to down smoothly to see its working\n",
    "#2. Read The COMMENTS and change parameter accordinly\n",
    "#\n",
    "\n",
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Import the three supervised learning models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA    \n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from random import *\n",
    "from subprocess import check_output\n",
    "def Propublica_rf(X,Y):\n",
    "    #Split data into training and test datasets (training will be based on 70% of data)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0,shuffle=True) \n",
    "    #test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\n",
    "    print('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "    \n",
    "    #Scaling data\n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    #sc = StandardScaler(with_mean=False)\n",
    "    \n",
    "    \n",
    "    #sc.fit(X_train)\n",
    "    #X_train_std = sc.transform(X_train)\n",
    "    #X_test_std = sc.transform(X_test)\n",
    "\n",
    "    #X_train_std and X_test_std are the scaled datasets to be used in algorithms\n",
    "\n",
    "    #Applying SVC (Support Vector Classification)\n",
    "#     from sklearn.svm import SVC\n",
    "#     svm = SVC(kernel='rbf', random_state=0, gamma=.1, C=10.0,probability=True)\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=251, max_depth=None, min_samples_split=50, random_state=0)\n",
    "    \n",
    "    print(Y_train.dtypes)\n",
    "    Y_train=Y_train.astype('int')\n",
    "    print(Y_train.dtypes)\n",
    "    \n",
    "    print(Y_test.dtypes)\n",
    "    Y_test=Y_test.astype('int')\n",
    "    print(Y_test.dtypes)\n",
    "    \n",
    "    \n",
    "    rf.fit(X_train, Y_train)\n",
    "    print('The accuracy of the RF classifier on training data is {:.2f}'.format(rf.score(X_train, Y_train)))\n",
    "    print('The accuracy of the RF classifier on test data is {:.2f}'.format(rf.score(X_test, Y_test)))\n",
    "    print('####Train prediction Label###############################################')\n",
    "    Y_train_pred=rf.predict(X_train)\n",
    "    #print(y_1)\n",
    "    Y_test_pred=rf.predict(X_test)\n",
    "\n",
    "    print('####Actual Train Label###############################################')\n",
    "\n",
    "\n",
    "    print('####Change to colors###############################################')\n",
    "        \n",
    "    e=rf.predict_proba(X_test)\n",
    "    print(e)\n",
    "    return X_test,Y_test_pred,Y_test,e\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(datax, y_test, y_test_pred,e): \n",
    "        \n",
    "    n=datax.shape[1]\n",
    "    s=datax.shape[0]    \n",
    "    data = np.zeros((s, n), dtype = int)\n",
    "    \n",
    "    r = np.zeros(n, dtype = int) \n",
    "    \n",
    "    for i in range(n):\n",
    "        if int(y_test.iloc[i])==1 :\n",
    "            r[i]=1\n",
    "        else :\n",
    "            r[i]= -1  \n",
    "    \n",
    "    r2 = np.zeros(n, dtype = int) \n",
    "    for i in range(n):\n",
    "        if int(y_test_pred[i])==1 :\n",
    "            r2[i]=1\n",
    "        else :\n",
    "            r2[i]= -1          \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        for i in range(n):\n",
    "                data[j][i]= datax.iloc[j,i]\n",
    "                if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r[i]==1:\n",
    "                         acc1=acc1+1 \n",
    "\n",
    "        print(\"ACTUAL----------total ,accepted, aceeptance rate:\")             \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    print(\"data DP\")\n",
    "    print(DP)\n",
    "    \n",
    "    ar=[]\n",
    "    \n",
    "    for j in range(s):\n",
    "        print(\"sensitive attribute \",(j+1)) \n",
    "        a=0\n",
    "        b=0\n",
    "        acc1=0\n",
    "        acc2=0\n",
    "        prec=0\n",
    "        reca=0\n",
    "        accur=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "        TP=0\n",
    "        TN=0\n",
    "        for i in range(n):\n",
    "             if data[j][i]== 1 :\n",
    "                    a=a+1\n",
    "                    if r2[i]==1:\n",
    "                        acc1=acc1+1 \n",
    "                        if r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        else:\n",
    "                             FP=FP+1                \n",
    "                    else:\n",
    "                        if r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        else:\n",
    "                            TN=TN+1    \n",
    "        \n",
    "        print(\"prec reca accuracy for each sens\") \n",
    "        prec= float(TP/(TP+FP))\n",
    "        reca= float(TP/(TP+FN))\n",
    "        accur= float((TP+TN)/a)\n",
    "        print(prec,reca,accur)\n",
    "        \n",
    "        print(\"Random Forest---------total , accepted, aceeptance rate:\")             \n",
    "        \n",
    "        a1=float(acc1/a)\n",
    "        print(a)\n",
    "        \n",
    "        print(acc1)\n",
    "        print(a1)\n",
    "        ar.append(a1)\n",
    "#COMMENT   : \"ar\" #############use this predicted RandomForet accptance rate(config) hardcoded or\n",
    "#can pass it through LPCA as lpca() \n",
    "#above ar: Random forest acceptance rate is use as beta_initial/beta (named as beata_initial in paper)\n",
    "######################################RF acceptance rate as beta_initial #############   \n",
    "    beta_initial=ar\n",
    "\n",
    "########################################################################################    \n",
    " \n",
    "    \n",
    "    maxi= max(ar)\n",
    "    mini= min(ar)\n",
    "    DP=float(maxi-mini)\n",
    "    \n",
    "    print(\"data acceptance rates\")\n",
    "    print(ar)\n",
    "    \n",
    "    print(\"data DP\")\n",
    "    print(DP) \n",
    "    \n",
    "    print(\"Random Forest accuracy--------------------------\")\n",
    "    prec=0\n",
    "    reca=0\n",
    "    accur=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    for i in range(n):\n",
    "            if r2[i]==1:\n",
    "                acc1=acc1+1 \n",
    "                if r[i]==1:\n",
    "                    TP=TP+1\n",
    "                else:\n",
    "                     FP=FP+1                \n",
    "            else:\n",
    "                if r[i]==1:\n",
    "                     FN=FN+1\n",
    "                else:\n",
    "                     TN=TN+1    \n",
    "\n",
    "        \n",
    "    prec= float(TP/(TP+FP))\n",
    "    reca= float(TP/(TP+FN))\n",
    "    accur= float((TP+TN)/n)\n",
    "    print(prec,reca,accur)\n",
    "    \n",
    "    \n",
    "    \n",
    "    fi= np.zeros(n,dtype=int) \n",
    "#(beta_converge, alpha and epsilon are the parameters of LPCA ) \n",
    "# alpha=1 initial predicted(beta configs) by classifier (RF), at alpha=0 Least DDP config at beta_converge\n",
    "\n",
    "#Example for beta_converge where at all cases alpha=0\n",
    "\n",
    "\n",
    "    alpha=[0]\n",
    "    epsilon=[.005,.01]\n",
    "    fi= np.zeros(n,dtype=int) \n",
    "    new=0  \n",
    "    \n",
    "    beta_converge=[.20,.24,.30,.40,.47,.50,.57,.59,.60]\n",
    "    \n",
    "\n",
    "    gamma=np.zeros((9,6),dtype=float)\n",
    "    for i in range(6):\n",
    "        for j in range(9):\n",
    "            gamma[j][i]=beta_converge[j]\n",
    "    print(gamma)\n",
    "    \n",
    "    t=0\n",
    "    a=0\n",
    "    #for t in range(gamma.shape[0]):\n",
    "    #for t in range(28):\n",
    "    for eps in epsilon:\n",
    "        for new in range(9):\n",
    "        \n",
    "            for a in alpha:\n",
    "                k=0\n",
    "                u1,u2=min_sum_lpca(data,beta_initial,eps,e,gamma[new],a)\n",
    "                #######################Disp_impact#######################  \n",
    "                print(\"gamma-epsilon-delta\",gamma[new],eps)\n",
    "                accu_all=[]\n",
    "                DP_all=[]\n",
    "                precision_all=[]\n",
    "                recall_all=[]\n",
    "                ar_all=[]\n",
    "                acceptance_rate=np.zeros((7,28),dtype=float)\n",
    "                count=0\n",
    "                print(\"<--------------------------------------->\")\n",
    "                print(\"iteration t\",k)\n",
    "                k=k+1\n",
    "\n",
    "\n",
    "                for i in range(n):\n",
    "                     fi[i] = u1[i]\n",
    "\n",
    "\n",
    "                for j in range(s):\n",
    "                    print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "                    TP=0\n",
    "                    FP=0\n",
    "                    FN=0\n",
    "                    TN=0\n",
    "                    precision=0\n",
    "                    recall=0\n",
    "                    for i in range(n):\n",
    "                         if data[j][i]== 1 :                        \n",
    "                            if fi[i]==1 and r[i]==1:\n",
    "                                TP=TP+1\n",
    "                            if fi[i]==1 and r[i]==-1:\n",
    "                                FP=FP+1 \n",
    "                            if fi[i]==-1 and r[i]==1:\n",
    "                                FN=FN+1\n",
    "                            if fi[i]==-1 and r[i]==-1:\n",
    "                                TN=TN+1    \n",
    "                    if TP+FP !=0:\n",
    "                        precision=float(TP/(TP+FP))\n",
    "                    #print(\"precision\",precision)\n",
    "                    if TP+FN !=0:    \n",
    "                        recall=float(TP/(TP+FN))\n",
    "                    #print(\"recall\",recall)\n",
    "\n",
    "                    precision_all.append(precision)\n",
    "                    recall_all.append(recall)\n",
    "                    #print(\"TP,FP,TN,FN\")\n",
    "                    #print(TP,FP,TN,FN)\n",
    "\n",
    "                    a=0\n",
    "                    b=0\n",
    "                    acc1=0\n",
    "                    acc2=0\n",
    "                    for i in range(n):\n",
    "                            if data[j][i]== 1 :\n",
    "                                a=a+1\n",
    "                                if fi[i]==1:\n",
    "                                     acc1=acc1+1 \n",
    "\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                    a1=float(acc1/a)\n",
    "\n",
    "\n",
    "\n",
    "        #                         print(a)\n",
    "        #                         print(acc1)\n",
    "        #                         print(a1)\n",
    "                    ar_all.append(a1)\n",
    "\n",
    "                count = count+1\n",
    "                maxi=max(ar_all)\n",
    "                mini= min(ar_all)\n",
    "                DP=float(maxi-mini)\n",
    "                print(\"individual acceptance rates\")\n",
    "                print(ar_all)\n",
    "                print(\"individual precision\")\n",
    "                print(precision_all)\n",
    "                print(\"individual recall\")\n",
    "                print(recall_all)\n",
    "                print(\"DP all\")\n",
    "                print(DP)\n",
    "                f_acc=0\n",
    "                for i in range(n):\n",
    "                     if fi[i] == r[i]:\n",
    "                            f_acc=f_acc+1\n",
    "                f_acc_l=float((f_acc*100)/n) \n",
    "\n",
    "        #######################################################################33   \n",
    "\n",
    "        #                         print(\"sensitive attribute \",(j+1)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                TP=0\n",
    "                FP=0\n",
    "                FN=0\n",
    "                TN=0\n",
    "                precision=0\n",
    "                recall=0\n",
    "                accu=0\n",
    "                for i in range(n):\n",
    "                        if fi[i]==1 and r[i]==1:\n",
    "                            TP=TP+1\n",
    "                        if fi[i]==1 and r[i]==-1:\n",
    "                            FP=FP+1 \n",
    "                        if fi[i]==-1 and r[i]==1:\n",
    "                            FN=FN+1\n",
    "                        if fi[i]==-1 and r[i]==-1:\n",
    "                            TN=TN+1    \n",
    "\n",
    "                if TP+FP!=0:\n",
    "                    precision=float(TP/(TP+FP))\n",
    "                print(\"precision all\",precision)\n",
    "                if TP+FN!=0:\n",
    "                    recall=float(TP/(TP+FN))\n",
    "\n",
    "\n",
    "                print(\"recall all\",recall)\n",
    "                accu=float((TP+TN)/(TP+FN+TN+FP))\n",
    "\n",
    "\n",
    "                print(\"accuracy all\",accu)\n",
    "\n",
    "\n",
    "\n",
    "                print(\"TP,FP,TN,FN\")\n",
    "                print(TP,FP,TN,FN)\n",
    "        #                         print(\"total ,fair accepted, aceeptance rate:\")             \n",
    "                a1=float(acc1/a)\n",
    "\n",
    "\n",
    "    print(\"<--------------------------------------->\")\n",
    "    alpha_weight=np.arange(0,1.05,.05)        \n",
    "    return accu_all,DP_all,acceptance_rate,alpha_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################LPCA######CODE##### same # beta setting ########\n",
    "               ### fixing alpha to 0 and (varying beta_avg) #########\n",
    "    # In beta_avg config all sub groups have same acceptance rate \n",
    "\n",
    "#data1 is sensitive group in binary matrix sets \n",
    "import time\n",
    "import pulp as p \n",
    "def min_sum_lpca(data1,beta_initial,eps,e,beta,alpha):\n",
    "#here beta_avg=beta[0], same beta is used for ranked weighting\n",
    "    import pulp as p \n",
    "    import math\n",
    "    m=data1.shape[0]\n",
    "    n=data1.shape[1]\n",
    "    print('dimension of data')\n",
    "    print(m,n)\n",
    "    \n",
    "    ################ sorted result\n",
    "    h1=[]\n",
    "    h2=[]\n",
    "    h3=[]\n",
    "    h4=[]\n",
    "    key1=[]\n",
    "    key2=[]\n",
    "    key3=[]\n",
    "    key4=[]\n",
    "    cost=np.zeros(n,dtype=int)\n",
    "    data2=np.zeros((m,n),dtype=int)\n",
    "    for i in range(n):\n",
    "        if data1[0][i]==1:            \n",
    "\n",
    "            h1.append(e[i][1])\n",
    "            key1.append(i)\n",
    "        elif data1[1][i]==1:\n",
    "            h2.append(e[i][1])\n",
    "            key2.append(i)\n",
    "            \n",
    "        if data1[2][i]==1:\n",
    "            h3.append(e[i][1])\n",
    "            key3.append(i)\n",
    "            \n",
    "        elif data1[3][i]==1:\n",
    "            h4.append(e[i][1])\n",
    "            key4.append(i)    \n",
    "\n",
    "#     print(hc)\n",
    "#     print(key1)\n",
    "    \n",
    "    for i in range(1,len(h1)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h1[j-1]<h1[j]:\n",
    "                index=j\n",
    "                var=h1[j]\n",
    "                h1[j]=h1[j-1]\n",
    "                h1[j-1]=var\n",
    "\n",
    "                var2=key1[j]\n",
    "                key1[j]=key1[j-1]\n",
    "                key1[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "            \n",
    "\n",
    "    for i in range(1,len(h2)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h2[j-1]<h2[j]:\n",
    "                index=j\n",
    "                var=h2[j]\n",
    "                h2[j]=h2[j-1]\n",
    "                h2[j-1]=var\n",
    "\n",
    "                var2=key2[j]\n",
    "                key2[j]=key2[j-1]\n",
    "                key2[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "    for i in range(1,len(h3)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h3[j-1]<h3[j]:\n",
    "                index=j\n",
    "                var=h3[j]\n",
    "                h3[j]=h3[j-1]\n",
    "                h3[j-1]=var\n",
    "\n",
    "                var2=key3[j]\n",
    "                key3[j]=key3[j-1]\n",
    "                key3[j-1]=var2\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    for i in range(1,len(h4)):\n",
    "        for j in range(i,0,-1):\n",
    "            var=0\n",
    "            var2=0\n",
    "            if h4[j-1]<h4[j]:\n",
    "                index=j\n",
    "                var=h4[j]\n",
    "                h4[j]=h4[j-1]\n",
    "                h4[j-1]=var\n",
    "\n",
    "                var2=key4[j]\n",
    "                key4[j]=key4[j-1]\n",
    "                key4[j-1]=var2\n",
    "            else:\n",
    "                break            \n",
    "    \n",
    "    #####alpha2 is just another weight influencing parameter for now its neutral with [\"ones\"] vector\n",
    "    alpha2=[1,1,1,1]\n",
    "    '''  \n",
    "   \n",
    "    \n",
    "    '''\n",
    "    \n",
    "#####Setup2#################LPCA with ranked sensitive groups with weight equalization    \n",
    "    for j in range(len(key1)):    \n",
    "        data2[0][key1[j]]=(j+1)*((beta[1]*len(key2))/(beta[0]*len(key1)))*alpha2[0]\n",
    "    for j in range(len(key2)):\n",
    "        data2[1][key2[j]]=(j+1)*alpha2[1]\n",
    "    for j in range(len(key3)):\n",
    "        data2[2][key3[j]]=(j+1)*alpha2[2]\n",
    "                         \n",
    "        \n",
    "    for j in range(len(key4)):                         \n",
    "        data2[3][key4[j]]=(j+1)*((beta[2]*len(key3))/(beta[3]*len(key4)))*alpha2[3]\n",
    "   \n",
    "       \n",
    " #sum up the weighted subgroup rank in cost        \n",
    "    for j in range(n):\n",
    "        summ=0\n",
    "        for i in range(m):\n",
    "       \n",
    "            summ=summ+data2[i][j] \n",
    "        cost[j]=summ\n",
    "        \n",
    "        \n",
    "    ################\n",
    "    \n",
    "    \n",
    "    Lp_prob = p.LpProblem('Problem', p.LpMinimize)  \n",
    "    ###############################Optimization fuction for LPCA###################\n",
    "# beta_avg(convergence point to achieve least DP at alpha=0   \n",
    "# beta_initial (acceptance rate) config obtaind for each group from random forest prediction)   \n",
    "\n",
    "\n",
    "    X=np.zeros(n+m+1,dtype=p.LpVariable)\n",
    "    Y=np.zeros(m,dtype=p.LpVariable)\n",
    "    \n",
    "    sizes=np.zeros(m,dtype=int)\n",
    "  \n",
    "    max_size=0\n",
    "    for i in range(m):\n",
    "        count=0\n",
    "        for j in range(n):\n",
    "            if data1[i][j]==1:\n",
    "                count=count+1 \n",
    "        if count>max_size:\n",
    "            max_size=count\n",
    "        sizes[i]=count\n",
    "    print(sizes)        \n",
    "    \n",
    " ################ As all beta[i]'s are same beta_avg=beta[0] #################   \n",
    "    \n",
    "    beta_avg=beta[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    select_sizes=np.zeros(m,dtype=int)\n",
    "   \n",
    "    size_final=np.zeros(m,dtype=int)\n",
    "\n",
    "    for i in range(m):\n",
    "        var1 = str(n+100+i)\n",
    "        Y[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Continuous')\n",
    "    \n",
    "    for i in range(n):\n",
    "        var1=str(i)       \n",
    "        X[i]=p.LpVariable(var1,lowBound=0,upBound=1,cat='Integer')\n",
    "   \n",
    "    X[n]=p.LpVariable(str(n),lowBound=0,upBound=1,cat='Continuous')  \n",
    "#minimize Objective#\n",
    "    Lp_prob+= p.lpSum([(X[j])*cost[j] for j in range(n)])\n",
    "    \n",
    "\n",
    "    for i in range(2*m):\n",
    "        if i<m:\n",
    "\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) >= (Y[i]-eps)*sizes[i]\n",
    "            Lp_prob += p.lpSum([(X[j])*(data1[i][j]) for j in range(n)]) <= (Y[i]+eps)*sizes[i]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "# for minimum ddp at alpa=0 setup (like paper) change (1- alpha) to (alpha) & (alpha) to (1-alpha)    \n",
    "    for i in range(m):\n",
    "            if beta_initial[i] >= beta_avg:\n",
    "\n",
    "                Lp_prob += Y[i] >= (1-alpha)*beta_initial[i] +alpha*beta_avg\n",
    "                Lp_prob += Y[i] <= (1-alpha)*beta_initial[i] +alpha*beta_avg\n",
    "               \n",
    "            else:\n",
    "                Lp_prob += Y[i] >= (1-alpha)*beta_initial[i] + alpha*beta_avg\n",
    "                Lp_prob += Y[i] <= beta_avg   \n",
    "    '''      \n",
    "\n",
    "\n",
    "    for i in range(m):\n",
    "        Lp_prob += Y[i] >= (alpha)*beta_initial[i] +(1-alpha)*beta_avg\n",
    "        Lp_prob += Y[i] <= (alpha)*beta_initial[i] +(1-alpha)*beta_avg\n",
    "                         \n",
    "   \n",
    "    #####################################\n",
    "    status = Lp_prob.solve()   # Solver \n",
    "    print(p.LpStatus[status]) \n",
    "    print(\"objective is:\")        \n",
    "    print(p.value(Lp_prob.objective))\n",
    "    print(\"discripency is:\") \n",
    "    print(p.value(X[n]))\n",
    "    x=np.zeros(n,dtype=float)\n",
    "\n",
    "   # The solution status \n",
    "    Synth1={}\n",
    "    Synth2={}\n",
    "    # # Printing the final solution \n",
    "    for i in range(n):\n",
    "        if(p.value(X[i])==1):\n",
    "            Synth1[i]=1 \n",
    "            Synth2[i]=-1\n",
    "#             if(data1[2][i]==1):\n",
    "#                 print(\"no\")\n",
    "        else:\n",
    "            Synth1[i]=-1\n",
    "            Synth2[i]=1\n",
    "    Synthu1=Synth1  \n",
    "    Synthu2=Synth2  \n",
    "    \n",
    "              \n",
    "    return Synthu1,Synthu2   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "# import pulp as p \n",
    "# from random import *\n",
    "data= pd.read_csv('data/propublica/compass.csv', skipinitialspace=True)\n",
    "# data = data1[data1[\"race\"].isin([\"african-american\", \"caucasian\"])]\n",
    "\n",
    "print(data['African_American'].value_counts())\n",
    "print(data['Female'].value_counts())\n",
    "# print(data.shape[0],data.shape[1])\n",
    "data=data.drop(columns=['id'])\n",
    "# print(data.head())\n",
    "# Age_Above_FourtyFive,Age_Below_TwentyFive, African_American,Female,  Two_yr_Recidivism  \n",
    "\n",
    "\n",
    "\n",
    "data_c = data.drop(columns=[ 'Two_yr_Recidivism' ])\n",
    "# print(sens)\n",
    "print(data_c.head())\n",
    "r=data[['Two_yr_Recidivism']]\n",
    "\n",
    "X_test,Y_test_pred,Y_test,e = Propublica_rf(data_c , r)\n",
    "\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "# Y_test_pred.reset_index()\n",
    "\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(X_test)\n",
    "print(Y_test_pred)\n",
    "print(Y_test)\n",
    "sens=X_test[['African_American','Female']]\n",
    "print(sens)\n",
    "p=sens.shape[0]\n",
    "\n",
    "\n",
    "sens1 = pd.get_dummies(sens, columns=['African_American','Female'], prefix =['african_american','female'])\n",
    "sensitive = sens1.T\n",
    "print(sensitive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################################Run and observe the results at varius parameters\n",
    "# Run for getting the Final output\n",
    "\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )\n",
    "\n",
    "# Observe group wise acc_rate,precision, recall, DP\n",
    "#eps=.005 ensures DDP<.01  , configuration within [beta_avg+eps ,beta_avg-eps] \n",
    "#see the the outputs at alpha =0 beta_avg varies\n",
    "#SEE Optimal group wise acceptance rate/config (individual beta)\n",
    "\n",
    "#optimal before iterations tells the valid configuration on parameters.\n",
    "#undefined ,means invalid configuration on given parameters\n",
    "       #if undefined increase eps from .005 to .01 \n",
    "#gamma is beta_avg in vector form    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec=[[0.6475409836065574, 0.7925531914893617, 0.7560975609756098, 0.65625],\n",
    "[0.6190476190476191, 0.7797356828193832, 0.7407407407407407, 0.6233766233766234],\n",
    "[0.5891891891891892, 0.7640845070422535, 0.7184986595174263, 0.6041666666666666],\n",
    "[0.5465587044534413, 0.7368421052631579, 0.687374749498998, 0.5625],\n",
    "[0.5154639175257731, 0.680089485458613, 0.6422487223168655, 0.5099337748344371],\n",
    "[0.5080906148867314, 0.6617647058823529, 0.6282051282051282, 0.4968944099378882],\n",
    "[0.49008498583569404, 0.6445672191528545, 0.601123595505618, 0.5163043478260869],\n",
    "[0.48633879781420764, 0.6423487544483986, 0.6002710027100271, 0.5052631578947369],\n",
    "[0.48655913978494625, 0.6416083916083916, 0.5992010652463382, 0.5077720207253886]]\n",
    "\n",
    "\n",
    "rec=[[0.3319327731092437, 0.30785123966942146, 0.3105175292153589, 0.34146341463414637],\n",
    "[0.38235294117647056, 0.365702479338843, 0.3672787979966611, 0.3902439024390244],\n",
    "[0.4579831932773109, 0.44834710743801653, 0.44741235392320533, 0.4715447154471545],\n",
    "[0.5672268907563025, 0.5785123966942148, 0.5726210350584308, 0.5853658536585366],\n",
    "[0.6302521008403361, 0.628099173553719, 0.6293823038397329, 0.6260162601626016],\n",
    "[0.6596638655462185, 0.6508264462809917, 0.654424040066778, 0.6504065040650406],\n",
    "[0.726890756302521, 0.7231404958677686, 0.7145242070116862, 0.7723577235772358],\n",
    "[0.7478991596638656, 0.7458677685950413, 0.7395659432387313, 0.7804878048780488],\n",
    "[0.7605042016806722, 0.7582644628099173, 0.7512520868113522, 0.7967479674796748]    ]\n",
    "accu=[ 0.6363,0.6464,0.6597,0.6723,0.6515,0.6445,0.6388,0.6388,0.6401]\n",
    "\n",
    "\n",
    "beta=[.20,.24,.30,.40,.47,.50,.57,.59,.60]\n",
    "\n",
    "\n",
    "beta_p=[]\n",
    "beta_r=[]\n",
    "beta_check=[0.26282051282051283, 0.58125, 0.5182539682539683, 0.21296296296296297]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#P R P R P R R \n",
    "#0.7251,0.8101 0.5470,0.7329 0.6311,0.7389 0.5308,0.6172 0.6764,0.4776  0.4666,0.5333  0.2666,0.7333\n",
    "#      \n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "#p=[]\n",
    "#r=[0,2,4,1,3,5,6]\n",
    "\n",
    "dp_list=[]\n",
    "sizes=[624,960,1260,324]\n",
    "for i in range(9):\n",
    "    weight_prec=0\n",
    "    weight_p=0\n",
    "    weight_rec=0\n",
    "    weight_r=0\n",
    "    cnt1=0\n",
    "    cnt2=0\n",
    "    for j in range(4):\n",
    "        #print(j)\n",
    "              \n",
    "        \n",
    "        if beta[i] <=beta_check[j]:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            cnt1=1\n",
    "        else:  \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "            cnt2=1\n",
    "    if cnt1==1:\n",
    "        wp=weight_prec/weight_p\n",
    "        weighted_precision.append(wp)\n",
    "        beta_p.append(beta[i])\n",
    "\n",
    "    if cnt2==1: \n",
    "        wr=weight_rec/weight_r\n",
    "        weighted_recall.append(wr) \n",
    "        beta_r.append(beta[i])\n",
    "            \n",
    "   \n",
    "    \n",
    "    \n",
    "len1=(len(weighted_precision)) \n",
    "len2=(len(weighted_recall)) \n",
    "    \n",
    "print(weighted_precision, weighted_recall,beta_p,beta_r,len1,len2)\n",
    "'''\n",
    "[0.9520125938773091, 0.9403591343418822, 0.8608764055619338],0.8561212064210523, 0.8128034718440105, 0.782912341254576, 0.7358009995069338\n",
    "0.4610404591572201, 0.5551300667433751, 0.5983458776930215, 0.6580824258162613,0.6476895465902177, 0.7070185074558867, 0.754950250161771\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "np.set_printoptions(precision=4)  # For compact display.\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "\n",
    "'''\n",
    "y1=savgol_filter(a, 6, 2)\n",
    "y2=savgol_filter(b, 6, 2)\n",
    "y3=savgol_filter(c, 6, 2)\n",
    "y4=savgol_filter(d, 6, 2)\n",
    "y5=savgol_filter(e, 6, 2)\n",
    "y6=savgol_filter(f, 6, 2)\n",
    "y7=savgol_filter(g, 6, 2)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "weighted_precision=[0.7355506832163975, 0.7272029898472769, 0.7382114584470812, 0.7087660384780942, 0.6586122955673509, 0.6427173780114956, 0.6445672191528545] \n",
    "weighted_recall=[0.3902439024390244, 0.46261814389232075, 0.5734262831406103, 0.6288044084568065, 0.6564999571918918, 0.7265055413073017, 0.7473021687950698, 0.7598524232929225] \n",
    "beta1=[.20,.24,.30,.40,.47,.50,.57]\n",
    "beta2=[.24,.30,.40,.47,.50,.57,.59,.60]\n",
    "accu=[ 0.6363,0.6464,0.6597,0.6723,0.6515,0.6445,0.6388,0.6388,0.6401]\n",
    "'''\n",
    "weighted_precision=[0.9520125938773091, 0.9403591343418822, 0.8608764055619338,0.8561212064210523, 0.8128034718440105, 0.782912341254576, 0.7358009995069338]\n",
    "weighted_recall=[0.4610404591572201, 0.5551300667433751, 0.5983458776930215, 0.6580824258162613,0.6476895465902177, 0.7070185074558867, 0.754950250161771]\n",
    "beta1=[.02,.025,.05,.1,.14,.16,.2]\n",
    "beta2=[.1,.14,.16,.2,.25,.3,.35]\n",
    "\n",
    "accu=[ 0.7774,0.7813,0.7954,0.8157,0.82442,0.8245,0.8216,0.8146,0.7954,0.7699]\n",
    "'''\n",
    "ax.plot(beta1,weighted_precision,label='Weighted Precision',color='blue',marker='^',linestyle='--')  \n",
    "ax.plot(beta2,weighted_recall,label='Weighted Recall',color='cyan',marker='^',linestyle='--')\n",
    "ax.plot(beta,accu,label=' Accuracy',color='red',marker='^',linestyle='--')\n",
    "#ax.vlines(y=[.1992], ymin=[0], ymax=[1], colors='purple', linestyles='--', lw=2, label='PRedict avg. acc.')\n",
    "#plt.axvline(.1992, color='green', linestyle='--')\n",
    "#plt.axvline(.10, color='orange', linestyle='--')\n",
    "#plt.axvline(.20, color='orange', linestyle='--')\n",
    "plt.axvline(0.2628, color='orange', linestyle='--')\n",
    "plt.axvline(0.5812, color='orange', linestyle='--')\n",
    "plt.axvline(0.5182, color='orange', linestyle='--')\n",
    "plt.axvline(0.2129, color='orange', linestyle='--')\n",
    "\n",
    "plt.title('')\n",
    "ax.set_xlabel('Acceptance Rate')\n",
    "ax.set_ylabel('Performance Metrics') \n",
    "# ax.set_ylabel('% in +ve class (Acceptance Rate)') \n",
    "\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -0.30), shadow=True, ncol=10)\n",
    "plt.show() \n",
    "fig.savefig('a2.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf(.005)\n",
    "accu,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )\n",
    "# beta=.4\n",
    "# .0009, .6723\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec=[[0.6289308176100629, 0.6578947368421053, 0.6703910614525139, 0.5603448275862069],\n",
    "[0.5586854460093896, 0.7121951219512195, 0.68389662027833, 0.5583333333333333],\n",
    "[0.6023391812865497, 0.7916666666666666, 0.7323943661971831, 0.5949367088607594],\n",
    "[0.5966850828729282, 0.76171875, 0.711864406779661, 0.6144578313253012]]\n",
    "\n",
    "rec=[[0.42016806722689076, 0.6714876033057852, 0.6010016694490818, 0.5284552845528455],\n",
    "[0.5, 0.6033057851239669, 0.5742904841402338, 0.5447154471544715],\n",
    "[0.4327731092436975, 0.3140495867768595, 0.34724540901502504, 0.3821138211382114],\n",
    "[0.453781512605042, 0.40289256198347106, 0.42070116861435725, 0.4146341463414634]]\n",
    "\n",
    "accu=[0.6685,0.6698,0.6369,0.6508]\n",
    "\n",
    "acc_rate=[[0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111],\n",
    "            [0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568],\n",
    "            [0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357],            \n",
    "            [0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924]]\n",
    "\n",
    "\n",
    "\n",
    "#0.5756,0.6218  0.6674,0.6788  0.6188,0.6451  0.6016,0.6097\n",
    "#R P P R\n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "p=[1,2,3]\n",
    "r=[0]\n",
    "print(np.transpose(acc_rate))\n",
    "weight_prec=0\n",
    "weight_p=0\n",
    "weight_rec=0\n",
    "weight_r=0\n",
    "sizes=[624,960,1260,324]\n",
    "dp_list=[]\n",
    "\n",
    "for i in range(4):\n",
    "    weight_prec=0\n",
    "    weight_rec=0\n",
    "    weight_p=0\n",
    "    weight_r=0\n",
    "    acc_list=[]\n",
    "    for j in range(4):\n",
    "        #print(j)\n",
    "        if j in p:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            #print(j)\n",
    "        if j in r:    \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "        #print(acc_rate[i][j])    \n",
    "        acc_list.append(acc_rate[i][j])\n",
    "    #print(acc_list)\n",
    "    dp=max(acc_list)-min(acc_list)   \n",
    "    dp_list.append(dp)     \n",
    "    wp=weight_prec/weight_p\n",
    "    wr=weight_rec/weight_r\n",
    "    weighted_precision.append(wp)\n",
    "    weighted_recall.append(wr)\n",
    "print(weighted_precision, weighted_recall,accu,dp_list)\n",
    "[0.6516601450222167, 0.6785837494590671, 0.7372548722796135, 0.7182718120643752] \n",
    "[0.4201680672268907, 0.5, 0.43277310924369755, 0.45378151260504207] \n",
    "[0.6685, 0.6698, 0.6369, 0.6508]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec=[ [0.5925925925925926, 0.6546184738955824, 0.6611418047882136, 0.5384615384615384],\n",
    "[0.41203703703703703, 0.6207729468599034, 0.5795677799607073, 0.4214876033057851],\n",
    "[0.3735632183908046, 0.5357142857142857, 0.4827586206896552, 0.375],\n",
    "[0.391304347826087, 0.6192307692307693, 0.5555555555555556, 0.39285714285714285]]\n",
    "\n",
    "\n",
    "rec=[[0.40336134453781514, 0.6735537190082644, 0.5993322203672788, 0.5121951219512195],\n",
    "[0.3739495798319328, 0.53099173553719, 0.49248747913188645, 0.4146341463414634],\n",
    "[0.27310924369747897, 0.21694214876033058, 0.2337228714524207, 0.24390243902439024],\n",
    " [0.3025210084033613, 0.33264462809917356, 0.333889816360601, 0.2682926829268293]]\n",
    "acc_rate=[[0.25961538461538464, 0.51875, 0.430952380952381, 0.3611111111111111],\n",
    "            [0.34615384615384615, 0.43125, 0.403968253968254, 0.3734567901234568],\n",
    "            [0.27884615384615385, 0.20416666666666666, 0.23015873015873015, 0.24691358024691357],            \n",
    "            [0.2948717948717949, 0.2708333333333333, 0.2857142857142857, 0.25925925925925924]]\n",
    "\n",
    "\n",
    "\n",
    "#0.5756,0.6218  0.6674,0.6788  0.6188,0.6451  0.6016,0.6097\n",
    "#R P P R\n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "p=[1,2]\n",
    "r=[0,3]\n",
    "print(np.transpose(acc_rate))\n",
    "weight_prec=0\n",
    "weight_p=0\n",
    "weight_rec=0\n",
    "weight_r=0\n",
    "sizes=[624,960,1260,324]\n",
    "dp_list=[]\n",
    "\n",
    "for i in range(4):\n",
    "    weight_prec=0\n",
    "    weight_rec=0\n",
    "    weight_p=0\n",
    "    weight_r=0\n",
    "    acc_list=[]\n",
    "    for j in range(4):\n",
    "        #print(j)\n",
    "        if j in p:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            #print(j)\n",
    "        if j in r:    \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "        #print(acc_rate[i][j])    \n",
    "        acc_list.append(acc_rate[i][j])\n",
    "    #print(acc_list)\n",
    "    dp=max(acc_list)-min(acc_list)   \n",
    "    dp_list.append(dp)     \n",
    "    wp=weight_prec/weight_p\n",
    "    wr=weight_rec/weight_r\n",
    "    weighted_precision.append(wp)\n",
    "    weighted_recall.append(wr)\n",
    "print(weighted_precision, weighted_recall,accu,dp_list)\n",
    "[0.6511413187220398, 0.6605956605956607, 0.6638695246121394, 0.6651638829949126]\n",
    "[0.5235535399710981, 0.5651163213046874, 0.5817103006685849, 0.5927988978857984] \n",
    "[0.6597, 0.6603, 0.6616, 0.6628]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from random import *\n",
    "from subprocess import check_output\n",
    "def Propublica_rf(X,Y):\n",
    "    #Split data into training and test datasets (training will be based on 70% of data)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0,shuffle=True) \n",
    "    #test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\n",
    "    print('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "    \n",
    "    #Scaling data\n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    #sc = StandardScaler(with_mean=False)\n",
    "    \n",
    "    \n",
    "    #sc.fit(X_train)\n",
    "    #X_train_std = sc.transform(X_train)\n",
    "    #X_test_std = sc.transform(X_test)\n",
    "\n",
    "    #X_train_std and X_test_std are the scaled datasets to be used in algorithms\n",
    "\n",
    "    #Applying SVC (Support Vector Classification)\n",
    "#     from sklearn.svm import SVC\n",
    "#     svm = SVC(kernel='rbf', random_state=0, gamma=.1, C=10.0,probability=True)\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=64, random_state=0)\n",
    "    \n",
    "    print(Y_train.dtypes)\n",
    "    Y_train=Y_train.astype('int')\n",
    "    print(Y_train.dtypes)\n",
    "    \n",
    "    print(Y_test.dtypes)\n",
    "    Y_test=Y_test.astype('int')\n",
    "    print(Y_test.dtypes)\n",
    "    \n",
    "    \n",
    "    rf.fit(X_train, Y_train)\n",
    "    print('The accuracy of the SVM classifier on training data is {:.2f}'.format(rf.score(X_train, Y_train)))\n",
    "    print('The accuracy of the SVM classifier on test data is {:.2f}'.format(rf.score(X_test, Y_test)))\n",
    "    print('####Train prediction Label###############################################')\n",
    "    Y_train_pred=rf.predict(X_train)\n",
    "    #print(y_1)\n",
    "    Y_test_pred=rf.predict(X_test)\n",
    "\n",
    "    print('####Actual Train Label###############################################')\n",
    "\n",
    "\n",
    "    print('####Change to colors###############################################')\n",
    "        \n",
    "    e=rf.predict_proba(X_test)\n",
    "    print(e)\n",
    "    return X_test,Y_test_pred,Y_test,e\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,Y_test_pred,Y_test,e = Propublica_rf(data_c , r)\n",
    "\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "# Y_test_pred.reset_index()\n",
    "\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(X_test)\n",
    "print(Y_test_pred)\n",
    "print(Y_test)\n",
    "sens=X_test[['African_American','Female']]\n",
    "print(sens)\n",
    "p=sens.shape[0]\n",
    "\n",
    "\n",
    "sens1 = pd.get_dummies(sens, columns=['African_American','Female'], prefix =['african_american','female'])\n",
    "sensitive = sens1.T\n",
    "print(sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prec=[[0.5625, 0.6680327868852459, 0.6442953020134228, 0.5862068965517241],\n",
    "[0.55, 0.6774193548387096, 0.6520979020979021, 0.556390977443609],\n",
    "[0.5411764705882353, 0.6826086956521739, 0.648881239242685, 0.5597014925373134],\n",
    "[0.5408560311284046, 0.6804347826086956, 0.6493955094991365, 0.5507246376811594]]\n",
    "rec=[[0.5294117647058824, 0.6735537190082644, 0.6410684474123539, 0.5528455284552846],\n",
    "[0.5546218487394958, 0.6508264462809917, 0.6227045075125208, 0.6016260162601627],\n",
    "[0.5798319327731093, 0.6487603305785123, 0.6293823038397329, 0.6097560975609756],\n",
    "[0.5840336134453782, 0.6466942148760331, 0.6277128547579299, 0.6178861788617886]]\n",
    "\n",
    "accu=[0.6654040404040404,0.663510101010101,0.663510101010101,0.6622474747474747]\n",
    "acc_rate1=[[0.3605,0.3862,.4102, 0.4134],\n",
    "[0.5093,0.4854,0.4802,0.4802],\n",
    "[0.4738 ,0.4547,0.4619,0.4603],\n",
    "[.3611,.4135,.4166,.4290]]\n",
    "acc_rate=np.transpose(acc_rate1)\n",
    "\n",
    "accu=[0.6805,.6761,.6666, 0.6628,.6542, 0.6452]\n",
    "#0.5756,0.6218  0.6674,0.6788  0.6188,0.6451  0.6016,0.6097\n",
    "#R P P R\n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "p=[1,2]\n",
    "r=[0,3]\n",
    "print(np.transpose(acc_rate))\n",
    "weight_prec=0\n",
    "weight_p=0\n",
    "weight_rec=0\n",
    "weight_r=0\n",
    "sizes=[624,960,1260,324]\n",
    "dp_list=[]\n",
    "\n",
    "for i in range(4):\n",
    "    weight_prec=0\n",
    "    weight_rec=0\n",
    "    weight_p=0\n",
    "    weight_r=0\n",
    "    acc_list=[]\n",
    "    for j in range(4):\n",
    "        #print(j)\n",
    "        if j in p:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            #print(j)\n",
    "        if j in r:    \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "        #print(acc_rate[i][j])    \n",
    "        acc_list.append(acc_rate[i][j])\n",
    "    #print(acc_list)\n",
    "    dp=max(acc_list)-min(acc_list)   \n",
    "    dp_list.append(dp)     \n",
    "    wp=weight_prec/weight_p\n",
    "    wr=weight_rec/weight_r\n",
    "    weighted_precision.append(wp)\n",
    "    weighted_recall.append(wr)\n",
    "print(weighted_precision, weighted_recall,accu,dp_list)\n",
    "[0.6545601603363733, 0.6630477194993324, 0.6634660852575991, 0.6628178978708377]\n",
    "[0.5374207725696021, 0.5706865642212428, 0.5900591789664308, 0.5956034775750375] \n",
    "[0.6805, 0.6761, 0.6666, 0.6628, 0.6542, 0.6452]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from random import *\n",
    "from subprocess import check_output\n",
    "def Propublica_lr(X,Y):\n",
    "    #Split data into training and test datasets (training will be based on 70% of data)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0,shuffle=True) \n",
    "    #test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\n",
    "    print('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "    #Scaling data\n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    #sc = StandardScaler(with_mean=False)\n",
    "    \n",
    "    \n",
    "    #sc.fit(X_train)\n",
    "    #X_train_std = sc.transform(X_train)\n",
    "    #X_test_std = sc.transform(X_test)\n",
    "\n",
    "    #X_train_std and X_test_std are the scaled datasets to be used in algorithms\n",
    "\n",
    "    #Applying SVC (Support Vector Classification)\n",
    "#     from sklearn.svm import SVC\n",
    "#     svm = SVC(kernel='rbf', random_state=0, gamma=.1, C=10.0,probability=True)\n",
    "    \n",
    "    lr = LogisticRegression(random_state=0)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    print(Y_train.dtypes)\n",
    "    Y_train=Y_train.astype('int')\n",
    "    print(Y_train.dtypes)\n",
    "    \n",
    "    print(Y_test.dtypes)\n",
    "    Y_test=Y_test.astype('int')\n",
    "    print(Y_test.dtypes)\n",
    "    \n",
    "    \n",
    "    lr.fit(X_train, Y_train)\n",
    "    print('The accuracy of the SVM classifier on training data is {:.2f}'.format(lr.score(X_train, Y_train)))\n",
    "    print('The accuracy of the SVM classifier on test data is {:.2f}'.format(lr.score(X_test, Y_test)))\n",
    "    print('####Train prediction Label###############################################')\n",
    "    Y_train_pred=lr.predict(X_train)\n",
    "    #print(y_1)\n",
    "    Y_test_pred=lr.predict(X_test)\n",
    "\n",
    "    print('####Actual Train Label###############################################')\n",
    "\n",
    "\n",
    "    print('####Change to colors###############################################')\n",
    "        \n",
    "    e=lr.predict_proba(X_test)\n",
    "    print(e)\n",
    "    return X_test,Y_test_pred,Y_test,e\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,Y_test_pred,Y_test,e = Propublica_lr(data_c , r)\n",
    "\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "# Y_test_pred.reset_index()\n",
    "\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(X_test)\n",
    "print(Y_test_pred)\n",
    "print(Y_test)\n",
    "sens=X_test[['African_American','Female']]\n",
    "print(sens)\n",
    "p=sens.shape[0]\n",
    "\n",
    "\n",
    "sens1 = pd.get_dummies(sens, columns=['African_American','Female'], prefix =['african_american','female'])\n",
    "sensitive = sens1.T\n",
    "print(sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec=[[0.5580357142857143, 0.6680327868852459, 0.6409395973154363, 0.5948275862068966],\n",
    "[0.5458333333333333, 0.6752688172043011, 0.6451048951048951, 0.5714285714285714],\n",
    "[0.5254901960784314, 0.6739130434782609, 0.6333907056798623, 0.5671641791044776],\n",
    "[0.5214007782101168, 0.6717391304347826, 0.6338514680483592, 0.5507246376811594]]\n",
    "\n",
    "\n",
    "rec=[[0.5252100840336135, 0.6735537190082644, 0.6377295492487479, 0.5609756097560976],\n",
    "[0.5504201680672269, 0.6487603305785123, 0.6160267111853088, 0.6178861788617886],\n",
    "[0.5630252100840336, 0.640495867768595, 0.6143572621035058, 0.6178861788617886],\n",
    "[0.5630252100840336, 0.6384297520661157, 0.6126878130217028, 0.6178861788617886]]\n",
    "\n",
    "accu=[0.6641414141414141, 0.6609848484848485,  0.6534090909090909, 0.6508838383838383]\n",
    "\n",
    "acc_rate1=[[0.3605,0.3862,.4102, 0.4134],\n",
    "[0.5093,0.4854,0.4802,0.4802],\n",
    "[0.4738 ,0.4547,0.4619,0.4603],\n",
    "[.3611,.4135,.4166,.4290]]\n",
    "acc_rate=np.transpose(acc_rate1)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#0.5756,0.6218  0.6674,0.6788  0.6188,0.6451  0.6016,0.6097\n",
    "#R P P R\n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "p=[1,2]\n",
    "r=[0,3]\n",
    "print(np.transpose(acc_rate))\n",
    "weight_prec=0\n",
    "weight_p=0\n",
    "weight_rec=0\n",
    "weight_r=0\n",
    "sizes=[624,960,1260,324]\n",
    "dp_list=[]\n",
    "\n",
    "for i in range(4):\n",
    "    weight_prec=0\n",
    "    weight_rec=0\n",
    "    weight_p=0\n",
    "    weight_r=0\n",
    "    acc_list=[]\n",
    "    for j in range(4):\n",
    "        #print(j)\n",
    "        if j in p:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            #print(j)\n",
    "        if j in r:    \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "        #print(acc_rate[i][j])    \n",
    "        acc_list.append(acc_rate[i][j])\n",
    "    #print(acc_list)\n",
    "    dp=max(acc_list)-min(acc_list)   \n",
    "    dp_list.append(dp)     \n",
    "    wp=weight_prec/weight_p\n",
    "    wr=weight_rec/weight_r\n",
    "    weighted_precision.append(wp)\n",
    "    weighted_recall.append(wr)\n",
    "print(weighted_precision, weighted_recall,accu,dp_list)\n",
    "[0.6526555711834621, 0.6581487533100436, 0.6509138787818726, 0.650235322053299] \n",
    "[0.5374337447235764, 0.5734781717565075, 0.5817751614384562, 0.5817751614384562] \n",
    "[0.6641414141414141, 0.6609848484848485, 0.6534090909090909, 0.6508838383838383]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from random import *\n",
    "from subprocess import check_output\n",
    "def Propublica_nn(X,Y):\n",
    "    #Split data into training and test datasets (training will be based on 70% of data)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0,shuffle=True) \n",
    "    #test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\n",
    "    print('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "    #Scaling data\n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    #sc = StandardScaler(with_mean=False)\n",
    "    \n",
    "    \n",
    "    #sc.fit(X_train)\n",
    "    #X_train_std = sc.transform(X_train)\n",
    "    #X_test_std = sc.transform(X_test)\n",
    "\n",
    "    #X_train_std and X_test_std are the scaled datasets to be used in algorithms\n",
    "\n",
    "    #Applying SVC (Support Vector Classification)\n",
    "#     from sklearn.svm import SVC\n",
    "#     svm = SVC(kernel='rbf', random_state=0, gamma=.1, C=10.0,probability=True)\n",
    "    nn = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                        hidden_layer_sizes=(60, 2), random_state=0)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    print(Y_train.dtypes)\n",
    "    Y_train=Y_train.astype('int')\n",
    "    print(Y_train.dtypes)\n",
    "    \n",
    "    print(Y_test.dtypes)\n",
    "    Y_test=Y_test.astype('int')\n",
    "    print(Y_test.dtypes)\n",
    "    \n",
    "    \n",
    "    nn.fit(X_train, Y_train)\n",
    "    print('The accuracy of the SVM classifier on training data is {:.2f}'.format(nn.score(X_train, Y_train)))\n",
    "    print('The accuracy of the SVM classifier on test data is {:.2f}'.format(nn.score(X_test, Y_test)))\n",
    "    print('####Train prediction Label###############################################')\n",
    "    Y_train_pred=nn.predict(X_train)\n",
    "    #print(y_1)\n",
    "    Y_test_pred=nn.predict(X_test)\n",
    "\n",
    "    print('####Actual Train Label###############################################')\n",
    "\n",
    "\n",
    "    print('####Change to colors###############################################')\n",
    "        \n",
    "    e=nn.predict_proba(X_test)\n",
    "    print(e)\n",
    "    return X_test,Y_test_pred,Y_test,e\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn\n",
    "X_test,Y_test_pred,Y_test,e = Propublica_nn(data_c , r)\n",
    "\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "# Y_test_pred.reset_index()\n",
    "\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(X_test)\n",
    "print(Y_test_pred)\n",
    "print(Y_test)\n",
    "sens=X_test[['African_American','Female']]\n",
    "print(sens)\n",
    "p=sens.shape[0]\n",
    "\n",
    "\n",
    "sens1 = pd.get_dummies(sens, columns=['African_American','Female'], prefix =['african_american','female'])\n",
    "sensitive = sens1.T\n",
    "print(sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec=[[0.53125, 0.6762295081967213, 0.6375838926174496, 0.5948275862068966],\n",
    "[0.5166666666666667, 0.6774193548387096, 0.6363636363636364, 0.5639097744360902],\n",
    "[0.5098039215686274, 0.6804347826086956, 0.6316695352839932, 0.5671641791044776],\n",
    "[0.5058365758754864, 0.6826086956521739, 0.6321243523316062, 0.5652173913043478]]\n",
    "\n",
    "\n",
    "rec=[[0.5, 0.6818181818181818, 0.6343906510851419, 0.5609756097560976],\n",
    "[0.5210084033613446, 0.6508264462809917, 0.6076794657762938, 0.6097560975609756],\n",
    "[0.5462184873949579, 0.6466942148760331, 0.6126878130217028, 0.6178861788617886],\n",
    "[0.5462184873949579, 0.6487603305785123, 0.6110183639398998, 0.6341463414634146]]\n",
    "\n",
    "\n",
    "accu=[0.6616161616161617, 0.6534090909090909, 0.6521464646464646, 0.6521464646464646]\n",
    "\n",
    "\n",
    "\n",
    "acc_rate1=[[0.3605,0.3862,.4102, 0.4134],\n",
    "[0.5093,0.4854,0.4802,0.4802],\n",
    "[0.4738 ,0.4547,0.4619,0.4603],\n",
    "[.3611,.4135,.4166,.4290]]\n",
    "acc_rate=np.transpose(acc_rate1)\n",
    "\n",
    "\n",
    "#0.5756,0.6218  0.6674,0.6788  0.6188,0.6451  0.6016,0.6097\n",
    "#R P P R\n",
    "weighted_precision=[]\n",
    "weighted_recall=[]\n",
    "p=[1,2]\n",
    "r=[0,3]\n",
    "print(np.transpose(acc_rate))\n",
    "weight_prec=0\n",
    "weight_p=0\n",
    "weight_rec=0\n",
    "weight_r=0\n",
    "sizes=[624,960,1260,324]\n",
    "dp_list=[]\n",
    "\n",
    "for i in range(4):\n",
    "    weight_prec=0\n",
    "    weight_rec=0\n",
    "    weight_p=0\n",
    "    weight_r=0\n",
    "    acc_list=[]\n",
    "    for j in range(4):\n",
    "        #print(j)\n",
    "        if j in p:\n",
    "            weight_prec=weight_prec+sizes[j]*prec[i][j]\n",
    "            weight_p=weight_p+sizes[j]\n",
    "            #print(j)\n",
    "        if j in r:    \n",
    "            weight_rec=weight_rec+sizes[j]*rec[i][j]\n",
    "            weight_r=weight_r+sizes[j]\n",
    "        #print(acc_rate[i][j])    \n",
    "        acc_list.append(acc_rate[i][j])\n",
    "    #print(acc_list)\n",
    "    dp=max(acc_list)-min(acc_list)   \n",
    "    dp_list.append(dp)     \n",
    "    wp=weight_prec/weight_p\n",
    "    wr=weight_rec/weight_r\n",
    "    weighted_precision.append(wp)\n",
    "    weighted_recall.append(wr)\n",
    "print(weighted_precision, weighted_recall,accu,dp_list)\n",
    "\n",
    "[0.6542955101652428, 0.6541174605690734, 0.6527572098027834, 0.6539554197134733] \n",
    "[0.5208397653596789, 0.5513398937840033, 0.5707125085291913, 0.5762697792917723]\n",
    "[0.6616161616161617, 0.6534090909090909, 0.6521464646464646, 0.6521464646464646] \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LP5 same beta\n",
    "accu,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )\n",
    "\n",
    "precision=[[0.5458715596330275, 0.7053571428571429, 0.6689342403628118, 0.5398230088495575],\n",
    "[0.5358649789029536, 0.6840659340659341, 0.6485355648535565, 0.5365853658536586],\n",
    "[0.5381526104417671, 0.6796875, 0.6468253968253969, 0.5348837209302325],\n",
    "[0.5214285714285715, 0.6828703703703703, 0.6472663139329806, 0.5103448275862069],\n",
    "[0.5083612040133779, 0.6695652173913044, 0.6307947019867549, 0.5096774193548387],\n",
    "[0.49691358024691357, 0.6613226452905812, 0.6213740458015267, 0.5],\n",
    "[0.4851190476190476, 0.6583011583011583, 0.6132352941176471, 0.5]]\n",
    "recall=[[0.5, 0.4896694214876033, 0.49248747913188645, 0.4959349593495935],\n",
    "[0.5336134453781513, 0.5144628099173554, 0.5175292153589316, 0.5365853658536586],\n",
    "[0.5630252100840336, 0.5392561983471075, 0.5442404006677797, 0.5609756097560976],\n",
    "[0.6134453781512605, 0.609504132231405, 0.6126878130217028, 0.6016260162601627],\n",
    "[0.6386554621848739, 0.6363636363636364, 0.6360601001669449, 0.6422764227642277],\n",
    "[0.6764705882352942, 0.6818181818181818, 0.679465776293823, 0.6829268292682927],\n",
    "[0.6848739495798319, 0.7045454545454546, 0.6961602671118531, 0.7073170731707317]]\n",
    "accu=[0.6439, 0.6395, 0.6433, 0.6515, 0.6458, 0.6445, 0.6414]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LP5 same beta final one\n",
    "accu,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )\n",
    "\n",
    "\n",
    "precision=[[0.6532258064516129, 0.7604166666666666, 0.7341269841269841, 0.65625],\n",
    "[0.6241610738255033, 0.7391304347826086, 0.7052980132450332, 0.6493506493506493],\n",
    "[0.5828877005347594, 0.7152777777777778, 0.6798941798941799, 0.5979381443298969],\n",
    "[0.5381526104417671, 0.6796875, 0.6468253968253969, 0.5348837209302325],\n",
    "[0.515358361774744, 0.6718403547671841, 0.6351351351351351, 0.5131578947368421],\n",
    "[0.5, 0.6625, 0.6238095238095238, 0.5],\n",
    "[0.4647887323943662, 0.6526508226691042, 0.6002785515320335, 0.4945652173913043]]\n",
    "\n",
    "\n",
    "recall=[[0.3403361344537815, 0.30165289256198347, 0.3088480801335559, 0.34146341463414637],\n",
    "[0.3907563025210084, 0.3512396694214876, 0.3555926544240401, 0.4065040650406504],\n",
    "[0.4579831932773109, 0.4256198347107438, 0.4290484140233723, 0.4715447154471545],\n",
    "[0.5630252100840336, 0.5392561983471075, 0.5442404006677797, 0.5609756097560976],\n",
    "[0.634453781512605, 0.6260330578512396, 0.6277128547579299, 0.6341463414634146],\n",
    "[0.6554621848739496, 0.6570247933884298, 0.656093489148581, 0.6585365853658537],\n",
    "[0.6932773109243697, 0.737603305785124, 0.7195325542570952, 0.7398373983739838]]\n",
    "\n",
    "\n",
    "accu=[0.6313, 0.6369, 0.6420, 0.6433, 0.6477, 0.6426 ,0.6338]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#padala\n",
    "accu,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )\n",
    "'''\n",
    "prec=[[0.5344827586206896, 0.6548507462686567, 0.6449511400651465, 0.512987012987013],\n",
    "[0.5328185328185329, 0.6567460317460317, 0.6465661641541038, 0.5],\n",
    "[0.5307692307692308, 0.671875, 0.6528028933092225, 0.5032258064516129],\n",
    "[0.48328267477203646, 0.6613545816733067, 0.6150375939849624, 0.4939759036144578]]\n",
    "\n",
    "rec=[[0.5210084033613446, 0.7252066115702479, 0.66110183639399, 0.6422764227642277],\n",
    "[0.5798319327731093, 0.6838842975206612, 0.6444073455759599, 0.6747967479674797],\n",
    "[0.5798319327731093, 0.621900826446281, 0.6026711185308848, 0.6341463414634146],\n",
    "[0.6680672268907563, 0.6859504132231405, 0.6828046744574291, 0.6666666666666666]]\n",
    "acc=[0.6590,0.6546,0.6515,0.6395]\n",
    "Dp= [0.18653,0.1099,0.0617,0.0154]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LP-5 results with beta balance \n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )\n",
    "'''\n",
    "prec=[[0.6335403726708074, 0.6666666666666666, 0.6556291390728477, 0.6842105263157895],\n",
    "[0.5989304812834224, 0.6713147410358565, 0.6555183946488294, 0.6263736263736264],\n",
    "[0.5666666666666667, 0.6707818930041153, 0.6480541455160744, 0.5904761904761905],\n",
    "[0.5404255319148936, 0.6801705756929638, 0.6547008547008547, 0.5294117647058824],\n",
    "[0.5135135135135135, 0.6843267108167771, 0.6528497409326425, 0.48872180451127817],\n",
    "[0.49295774647887325, 0.6880733944954128, 0.6573426573426573, 0.43243243243243246]]\n",
    "\n",
    "rec=[[0.42857142857142855, 0.7148760330578512, 0.66110183639399, 0.42276422764227645],\n",
    "[0.47058823529411764, 0.6962809917355371, 0.654424040066778, 0.4634146341463415],\n",
    "[0.5, 0.6735537190082644, 0.6393989983305509, 0.5040650406504065],\n",
    "[0.5336134453781513, 0.6590909090909091, 0.6393989983305509, 0.5121951219512195],\n",
    "[0.5588235294117647, 0.640495867768595, 0.6310517529215359, 0.5284552845528455],\n",
    "[0.5882352941176471, 0.6198347107438017, 0.6277128547579299, 0.5203252032520326]]\n",
    "\n",
    "accu=[0.6805,.6761,.6666, 0.6628,.6542, 0.6452]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overlapping\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#LP-5 results with beta balance \n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LP-5 results with beta balance \n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#basic2 bilal final\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic2 agarwal final\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic2 bilal final\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#basic2 bilal final 33------------\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic2 bilal final 44------------\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#basic2 bilal\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic2 bilal final--last1\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic2 bilal final--last2\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#basic2 agarwal final--last1\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic2 agarwal final--last2\n",
    "accu_all,DP_all,acceptance_rate,alpha_weight = main(sensitive, Y_test, Y_test_pred,e )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
